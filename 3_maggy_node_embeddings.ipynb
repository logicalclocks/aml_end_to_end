{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>45</td><td>application_1610641989196_0292</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://resourcemanager.service.consul:8088/proxy/application_1610641989196_0292/\">Link</a></td><td><a target=\"_blank\" href=\"http://jimeuwest-worker-1.internal.cloudapp.net:8042/node/containerlogs/container_e02_1610641989196_0292_01_000001/amlsim__davit000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "<pyspark.sql.session.SparkSession object at 0x7fe54d084ad0>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maggy import Searchspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve nodes and edges training datasets from hsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_td = fs.get_training_dataset(\"node_td\", 1)\n",
    "edge_td = fs.get_training_dataset(\"edges_td\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert training dataset in to pandas daframe\n",
    "We are going to use StellarGraph library to compute node embeddings. StellarGraph supports loading data via Pandas DataFrames, NumPy arrays, Neo4j and NetworkX graphs. \n",
    "\n",
    "---\n",
    "**NOTE**:\n",
    "\n",
    "Loading large scale dataset in to StellarGraph for training can not be handled with above mentioned fameworks. It will require loading data using frameworks such as `tf.data`. \n",
    "\n",
    "If your training datasets measure from couple of GB to 100s of GBs or even TBs contact us at Logical Clocks and we will help you to setup distributed training pipelines. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fg as pandas\n",
    "node_pdf = node_td.read().toPandas()\n",
    "edge_pdf = edge_td.read().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The searchspace can be instantiated with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter added: walk_number\n",
      "Hyperparameter added: walk_length\n",
      "Hyperparameter added: emb_size"
     ]
    }
   ],
   "source": [
    "sp = Searchspace(walk_number=('DISCRETE', [30, 40]), walk_length=('INTEGER', [5, 7]) , emb_size=('DISCRETE', [128, 256]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define hopsworks experiments wrapper function and put all the training logic there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_computer(walk_number, walk_length, emb_size):\n",
    "    \n",
    "    import os\n",
    "    import sys\n",
    "    import uuid\n",
    "    import random    \n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    import pydoop.hdfs as pydoop\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    from hops import hdfs\n",
    "    from hops import pandas_helper as pandas\n",
    "    from hops import model as hops_model\n",
    "    from hops import tensorboard\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn import preprocessing, feature_extraction, model_selection\n",
    "    from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.manifold import TSNE\n",
    "\n",
    "    import stellargraph as sg\n",
    "    from stellargraph import StellarGraph\n",
    "    from stellargraph import StellarDiGraph\n",
    "    from stellargraph.data import BiasedRandomWalk\n",
    "    from stellargraph.data import UnsupervisedSampler\n",
    "    from stellargraph.mapper import Node2VecLinkGenerator, Node2VecNodeGenerator\n",
    "    from stellargraph.layer import Node2Vec, link_classification\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras  \n",
    "        \n",
    "    ###########\n",
    "    batch_size = 512\n",
    "    epochs = 10\n",
    "    num_samples = [20, 20]\n",
    "    layer_sizes = [100, 100]\n",
    "    learning_rate = 1e-2\n",
    "\n",
    "    node_data = pd.DataFrame(node_pdf[['type']], index=node_pdf['id'])\n",
    "    ###########\n",
    "        \n",
    "    print('Defining StellarDiGraph')\n",
    "    G =StellarDiGraph(node_data,\n",
    "                      edges=edge_pdf, \n",
    "                      edge_type_column=\"tx_type\")\n",
    "\n",
    "\n",
    "    nodes = list(G.nodes())\n",
    "\n",
    "    walker = BiasedRandomWalk(\n",
    "        G,\n",
    "        n=walk_number,\n",
    "        length=walk_length,\n",
    "        p=0.5,  # defines probability, 1/p, of returning to source node\n",
    "        q=2.0,  # defines probability, 1/q, for moving to a node away from the source node\n",
    "    )\n",
    "    unsupervised_samples = UnsupervisedSampler(G, nodes=list(G.nodes()), walker=walker)\n",
    "    generator = Node2VecLinkGenerator(G, batch_size)\n",
    "    node2vec = Node2Vec(emb_size, generator=generator)\n",
    "    \n",
    "    x_inp, x_out = node2vec.in_out_tensors()\n",
    "    prediction = link_classification(\n",
    "        output_dim=1, output_act=\"sigmoid\", edge_embedding_method=\"dot\"\n",
    "    )(x_out)\n",
    "\n",
    "    print('Defining the model')\n",
    "    model = keras.Model(inputs=x_inp, outputs=prediction)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "        loss=keras.losses.binary_crossentropy,\n",
    "        metrics=[keras.metrics.binary_accuracy],\n",
    "    )\n",
    "        \n",
    "    # Save the weights using the `checkpoint_path` format\n",
    "    \n",
    "    print('Training the model')\n",
    "    history = model.fit(\n",
    "        generator.flow(unsupervised_samples),\n",
    "        epochs=epochs,\n",
    "        verbose=0,\n",
    "        use_multiprocessing=False,\n",
    "        workers=4,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    binary_accuracy = history.history['binary_accuracy'][-1]\n",
    "        \n",
    "    return binary_accuracy    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use above experiments wrapper function to conduct maggy hyperparameter search experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Can't reach Maggy server. No progress information and logs available. Job continues running anyway.\n"
     ]
    }
   ],
   "source": [
    "from maggy import experiment\n",
    "result = experiment.lagom(embeddings_computer, \n",
    "                           searchspace=sp, \n",
    "                           optimizer='randomsearch', \n",
    "                           direction='max',\n",
    "                           num_trials=20, \n",
    "                           name='EMBEDDINGS',\n",
    "                           hb_interval=5, \n",
    "                           es_interval=5,\n",
    "                           es_min=5\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from hops import hdfs\n",
    "EMBEDDINGS_HYPERPARAMS_FILE = 'embeddings_best_hp.json'\n",
    "hdfs.dump(json.dumps(result['best_hp']), \"Resources/\" + EMBEDDINGS_HYPERPARAMS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
