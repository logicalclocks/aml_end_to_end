{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model achitecture and hyperparameter tuning \n",
    "No we are ready to define embeddings model architecture and perform hyperparameter tuning using maggy.\n",
    "![Training Dataset](./images/maggy_hp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use StellarGraph library to compute node embeddings. StellarGraph supports loading data via Pandas DataFrames, NumPy arrays, Neo4j and NetworkX graphs. \n",
    "\n",
    "---\n",
    "**NOTE**:\n",
    "\n",
    "Loading large scale dataset in to StellarGraph for training can not be handled with above mentioned fameworks. It will require loading data using frameworks such as `tf.data`. \n",
    "\n",
    "If your training datasets measure from couple of GB to 100s of GBs or even TBs contact us at Logical Clocks and we will help you to setup distributed training pipelines. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required libraries. Such as StellarGraph. \n",
    "##### To do this 1st navigate to python environment:\n",
    "![Incremental Feature Engineering](./images/navigate_to_python.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### And then install library by name and version\n",
    "![Incremental Feature Engineering](./images/install_lib_by_name.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameter searchspace for maggy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>12</td><td>application_1631881319280_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://resourcemanager.service.consul:8089/proxy/application_1631881319280_0004/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://ip-10-0-0-216.us-west-2.compute.internal:8044/node/containerlogs/container_e03_1631881319280_0004_01_000001/amlsim__meb10180\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "Hyperparameter added: walk_number\n",
      "Hyperparameter added: walk_length\n",
      "Hyperparameter added: emb_size"
     ]
    }
   ],
   "source": [
    "from maggy import Searchspace\n",
    "sp = Searchspace(walk_number=('DISCRETE', [2, 3]), walk_length=('INTEGER', [2, 3]) , emb_size=('DISCRETE', [16, 32]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define hopsworks experiments wrapper function and put all the training logic there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_computer(walk_number, walk_length, emb_size):\n",
    "    \n",
    "    import os\n",
    "    import sys\n",
    "    import uuid\n",
    "    import random    \n",
    "    \n",
    "    # pandas and numpy\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # hops utility libraries\n",
    "    from hops import hdfs\n",
    "    from hops import pandas_helper as pandas\n",
    "    from hops import model as hops_model\n",
    "    from hops import tensorboard\n",
    "    \n",
    "    # tensorlfow\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras  \n",
    "    \n",
    "    # stellargraph library for graph neural networks\n",
    "    import stellargraph as sg\n",
    "    from stellargraph import StellarGraph\n",
    "    from stellargraph import StellarDiGraph\n",
    "    from stellargraph.data import BiasedRandomWalk\n",
    "    from stellargraph.data import UnsupervisedSampler\n",
    "    from stellargraph.mapper import Node2VecLinkGenerator, Node2VecNodeGenerator\n",
    "    from stellargraph.layer import Node2Vec, link_classification\n",
    "\n",
    "    ## Connect to hsfs and retrieve datasets for training and evaluation \n",
    "    import hsfs\n",
    "    # Create a connection\n",
    "    connection = hsfs.connection(engine = \"training\")\n",
    "    # Get the feature store handle for the project's feature store\n",
    "    fs = connection.get_feature_store()\n",
    "\n",
    "    # Get nodes and edges training datasets metadata objects from hsfs\n",
    "    node_td = fs.get_training_dataset(\"node_td\", 1)\n",
    "    edge_td = fs.get_training_dataset(\"edges_td\", 1)\n",
    "    \n",
    "    # Get fg as pandas dataframe\n",
    "    node_pdf = node_td.read()\n",
    "    edge_pdf = edge_td.read()\n",
    "\n",
    "    # define static hyperparameters\n",
    "    batch_size = 512\n",
    "    epochs = 10\n",
    "    num_samples = [20, 20]\n",
    "    layer_sizes = [100, 100]\n",
    "    learning_rate = 1e-2\n",
    "\n",
    "\n",
    "    # construct Stellargraph object for training\n",
    "    node_data = pd.DataFrame(node_pdf[['type']], index=node_pdf['id'])\n",
    "        \n",
    "    print('Defining StellarDiGraph')\n",
    "    G =StellarDiGraph(node_data,\n",
    "                      edges=edge_pdf, \n",
    "                      edge_type_column=\"tx_type\")\n",
    "\n",
    "\n",
    "    nodes = list(G.nodes())\n",
    "\n",
    "    walker = BiasedRandomWalk(\n",
    "        G,\n",
    "        n=walk_number,\n",
    "        length=walk_length,\n",
    "        p=0.5,  # defines probability, 1/p, of returning to source node\n",
    "        q=2.0,  # defines probability, 1/q, for moving to a node away from the source node\n",
    "    )\n",
    "    unsupervised_samples = UnsupervisedSampler(G, nodes=list(G.nodes()), walker=walker)\n",
    "    generator = Node2VecLinkGenerator(G, batch_size)\n",
    "    node2vec = Node2Vec(emb_size, generator=generator)\n",
    "    \n",
    "    x_inp, x_out = node2vec.in_out_tensors()\n",
    "    prediction = link_classification(\n",
    "        output_dim=1, output_act=\"sigmoid\", edge_embedding_method=\"dot\"\n",
    "    )(x_out)\n",
    "\n",
    "    # define and train keras model\n",
    "    print('Defining the model')\n",
    "    model = keras.Model(inputs=x_inp, outputs=prediction)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "        loss=keras.losses.binary_crossentropy,\n",
    "        metrics=[keras.metrics.binary_accuracy],\n",
    "    )\n",
    "        \n",
    "    # Save the weights using the `checkpoint_path` format\n",
    "    \n",
    "    print('Training the model')\n",
    "    history = model.fit(\n",
    "        generator.flow(unsupervised_samples),\n",
    "        epochs=epochs,\n",
    "        verbose=0,\n",
    "        use_multiprocessing=False,\n",
    "        workers=4,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    binary_accuracy = history.history['binary_accuracy'][-1]\n",
    "        \n",
    "    return binary_accuracy    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use above experiments wrapper function to conduct maggy hyperparameter search experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bde0958f5824549adb66e9ef0bd95cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Maggy experiment', max=2.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Connected. Call `.close()` to terminate connection gracefully.\n",
      "0: Defining StellarDiGraph\n",
      "0: link_classification: using 'dot' method to combine node embeddings into edge embeddings\n",
      "0: \n",
      "0: Defining the model\n",
      "0: Training the model\n",
      "0: \n",
      "0: \n",
      "0: Connected. Call `.close()` to terminate connection gracefully.\n",
      "0: Defining StellarDiGraph\n",
      "0: link_classification: using 'dot' method to combine node embeddings into edge embeddings\n",
      "0: Defining the model\n",
      "0: Training the model\n",
      "Started Maggy Experiment: EMBEDDINGS, application_1631881319280_0004, run 1\n",
      "\n",
      "------ RandomSearch Results ------ direction(max) \n",
      "BEST combination {\"walk_number\": 3, \"walk_length\": 3, \"emb_size\": 32} -- metric 0.6784741878509521\n",
      "WORST combination {\"walk_number\": 2, \"walk_length\": 3, \"emb_size\": 16} -- metric 0.6134476661682129\n",
      "AVERAGE metric -- 0.6459609270095825\n",
      "EARLY STOPPED Trials -- 0\n",
      "Total job time 0 hours, 1 minutes, 57 seconds\n",
      "\n",
      "Finished Experiment\n"
     ]
    }
   ],
   "source": [
    "from maggy import experiment\n",
    "result = experiment.lagom(embeddings_computer, \n",
    "                           searchspace=sp, \n",
    "                           optimizer='randomsearch', \n",
    "                           direction='max',\n",
    "                           num_trials=2, \n",
    "                           name='EMBEDDINGS',\n",
    "                           hb_interval=5, \n",
    "                           es_interval=5,\n",
    "                           es_min=5\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output best hyperparameter as a json file, so we can use it next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from hops import hdfs\n",
    "EMBEDDINGS_HYPERPARAMS_FILE = 'embeddings_best_hp.json'\n",
    "hdfs.dump(json.dumps(result['best_hp']), \"Resources/\" + EMBEDDINGS_HYPERPARAMS_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing experiments\n",
    "Experiments service provides a unified view of all the experiments run using the `experiment` module.\n",
    "<br>\n",
    "As demonstrated in the gif it provides general information about the experiment and the resulting metric. Experiments can be visualized meanwhile or after training in a TensorBoard.\n",
    "<br>\n",
    "<br>\n",
    "![Image7-Monitor.png](./images/experiments.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}