{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor transactions using anomaly detection model.     \n",
    "---\n",
    "**NOTE**: \n",
    "\n",
    "In real life scenarios financial transaction are dynamically evolving graphs. Performing anomaly detection inference on graph embeddings in live Transaction Monitoring Systems will require to update the graph and node representations after new transactions arrive. Recomputing entire graph for every newly arrived transaction will lead to unaxeptable delayes and even monitoring system failure. This problem  will be more sever if large amount of updates happen in a short time window.\n",
    "\n",
    "Contact us at Logical Clocks and we will help you to setup end to end graph based deep anomaly detection for live Transaction Monitoring Systems. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image7-Monitor.png](./images/model_registry.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Model Repository for best anomaly detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hsml\n",
    "\n",
    "conn = hsml.connection()\n",
    "mr = conn.get_model_registry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME=\"ganAml\"\n",
    "EVALUATION_METRIC=\"loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = mr.get_best_model(MODEL_NAME, EVALUATION_METRIC, \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: ganAml\n",
      "Model version: 2\n",
      "{'loss': '-0.03453196585178375'}\n"
     ]
    }
   ],
   "source": [
    "print('Model name: ' + best_model.name)\n",
    "print('Model version: ' + str(best_model.version))\n",
    "print(best_model.training_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the deployment\n",
    "Here, we fetch the model we want from the model registry and define a configuration for the deployment. For the configuration, we need to specify the serving type (default or KFserving) and in this case, since we use default serving and an sklearn model, we need to give the location of the prediction script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hsml\n",
    "\n",
    "conn = hsml.connection()\n",
    "mr = conn.get_model_registry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model name from notebook 4, where we registered the model\n",
    "model = mr.get_model(best_model.name, version=best_model.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment started, explore it at https://hopsworks0.logicalclocks.com/p/119/deployments/2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Deployment(name: 'ganTfServing')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create serving instance\n",
    "SERVING_NAME = \"ganTfServing\"\n",
    "\n",
    "# Give it any name you want\n",
    "model.deploy(name=SERVING_NAME, model_server=\"TENSORFLOW_SERVING\", serving_tool=\"DEFAULT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch model server object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = conn.get_model_serving()\n",
    "deployment = ms.get_deployment('ganTfServing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Model Serving for active deployments\n",
    "![Image7-Monitor.png](./images/deployment.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictorState(status: 'Stopped')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get serving status\n",
    "deployment.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2550de1b4dde45f4b740dbcda4bcb22f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start deployment\n",
    "deployment.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Retrieve serving vectors and send prediction requests to the served model using Hopsworks REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view_non_sar = fs.get_feature_view('non_sar_transactions_view', 1)\n",
    "feature_view_sar = fs.get_feature_view('sar_transactions_view', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve test feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VersionWarning: No training dataset version was provided to initialise serving. Defaulting to version 1.\n",
      "SADeprecationWarning: The LegacyRow.items() method is deprecated and will be removed in a future release.  Use the Row._mapping attribute, i.e., 'row._mapping.items()'. (deprecated since: 1.4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0.13333333333333333,\n",
       " 0.10375241959933768,\n",
       " 0.41951605906870604,\n",
       " 0.5523306125312814,\n",
       " 0.3333333333333333,\n",
       " 0.5159525808594092,\n",
       " 0.7698600424453773,\n",
       " 0.23520631848583132,\n",
       " [0.9999812245368958,\n",
       "  0.9999898076057434,\n",
       "  0.999980628490448,\n",
       "  0.9999653100967407,\n",
       "  0.9999864101409912,\n",
       "  0.9999986290931702,\n",
       "  1.0,\n",
       "  0.9999991059303284,\n",
       "  0.9999988675117493,\n",
       "  0.9999955892562866,\n",
       "  0.9999983310699463,\n",
       "  0.9999964833259583,\n",
       "  1.0,\n",
       "  0.9999929666519165,\n",
       "  0.9999990463256836,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9999840259552002,\n",
       "  0.999994158744812,\n",
       "  0.9999998807907104,\n",
       "  0.9999914765357971,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.9999879002571106,\n",
       "  0.9999469518661499,\n",
       "  0.9999988079071045,\n",
       "  0.999983549118042,\n",
       "  0.9999848008155823,\n",
       "  0.9999995231628418,\n",
       "  0.9999985098838806,\n",
       "  0.9999852776527405,\n",
       "  0.9999995231628418]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_view_non_sar.get_feature_vector({'id': \"0016359b\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_score = [\"0016359b\", \n",
    "                \"001dcc27\", \n",
    "                \"0054a022\", \n",
    "                \"00d6b609\", \n",
    "                \"00e14860\", \n",
    "                \"00e39a1b\", \n",
    "                \"014ed5cb\", \n",
    "                \"01ce3306\", \n",
    "                \"01fa19ae\", \n",
    "                \"01fa1d01\", \n",
    "                \"036dce03\", \n",
    "                \"03e09be4\", \n",
    "                \"04b23f4b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anomaly score for node_id  0016359b  :  0.790985763\n",
      " anomaly score for node_id  001dcc27  :  0.760188341\n",
      " anomaly score for node_id  0054a022  :  0.778505445\n",
      " anomaly score for node_id  00d6b609  :  0.78152281\n",
      " anomaly score for node_id  00e14860  :  0.795529366\n",
      " anomaly score for node_id  00e39a1b  :  0.753337741\n",
      " anomaly score for node_id  014ed5cb  :  0.760115087\n",
      " anomaly score for node_id  01ce3306  :  0.757739365\n",
      " anomaly score for node_id  01fa19ae  :  0.766529322\n",
      " anomaly score for node_id  01fa1d01  :  0.768219531\n",
      " anomaly score for node_id  036dce03  :  0.766904116\n",
      " anomaly score for node_id  03e09be4  :  0.76641053\n",
      " anomaly score for node_id  04b23f4b  :  0.777334511\n"
     ]
    }
   ],
   "source": [
    "def flat2gen(alist):\n",
    "    for item in alist:\n",
    "        if isinstance(item, list):\n",
    "            for subitem in item: yield subitem\n",
    "        else:\n",
    "            yield item\n",
    "\n",
    "def model_server(input):\n",
    "    data = {\"inputs\": [input]}\n",
    "    return deployment.predict(data)\n",
    "\n",
    "for node_id in ids_to_score:\n",
    "    serving_vector = np.array(list(flat2gen(feature_view_non_sar.get_feature_vector({'id': node_id})))).reshape(1,41).tolist()\n",
    "    print(\" anomaly score for node_id \", node_id, \" : \",   model_server(serving_vector)[\"outputs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}