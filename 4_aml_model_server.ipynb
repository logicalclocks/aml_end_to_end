{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor transactions using anomaly detection model.     \n",
    "---\n",
    "**NOTE**: \n",
    "\n",
    "In real life scenarios financial transaction are dynamically evolving graphs. Performing anomaly detection inference on graph embeddings in live Transaction Monitoring Systems will require to update the graph and node representations after new transactions arrive. Recomputing entire graph for every newly arrived transaction will lead to unaxeptable delayes and even monitoring system failure. This problem  will be more sever if large amount of updates happen in a short time window.\n",
    "\n",
    "Contact us at Logical Clocks and we will help you to setup end to end graph based deep anomaly detection for live Transaction Monitoring Systems. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image7-Monitor.png](./images/model_registry.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Model Repository for best anomaly detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hsml\n",
    "\n",
    "conn = hsml.connection()\n",
    "mr = conn.get_model_registry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME=\"ganAml\"\n",
    "EVALUATION_METRIC=\"loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = mr.get_best_model(MODEL_NAME, EVALUATION_METRIC, \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: ganAml\n",
      "Model version: 1\n",
      "{'loss': '0.768'}\n"
     ]
    }
   ],
   "source": [
    "print('Model name: ' + best_model.name)\n",
    "print('Model version: ' + str(best_model.version))\n",
    "print(best_model.training_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the deployment\n",
    "Here, we fetch the model we want from the model registry and define a configuration for the deployment. For the configuration, we need to specify the serving type (default or KFserving) and in this case, since we use default serving and an sklearn model, we need to give the location of the prediction script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hsml\n",
    "\n",
    "conn = hsml.connection()\n",
    "mr = conn.get_model_registry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model name from notebook 4, where we registered the model\n",
    "model = mr.get_model(best_model.name, version=best_model.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment started, explore it at https://hopsworks0.logicalclocks.com/p/119/deployments/1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Deployment(name: 'ganTfServing')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create serving instance\n",
    "SERVING_NAME = \"ganTfServing\"\n",
    "\n",
    "# Give it any name you want\n",
    "model.deploy(name=SERVING_NAME, model_server=\"TENSORFLOW_SERVING\", serving_tool=\"DEFAULT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch model server object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = conn.get_model_serving()\n",
    "deployment = ms.get_deployment('ganTfServing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Model Serving for active deployments\n",
    "![Image7-Monitor.png](./images/deployment.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictorState(status: 'Stopped')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get serving status\n",
    "deployment.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae231438d98c4961ae76ebd2950b01a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start deployment\n",
    "deployment.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve serving vectors and send prediction requests to the served model using Hopsworks REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view_non_sar = fs.get_feature_view('non_sar_transactions_view', 1)\n",
    "feature_view_sar = fs.get_feature_view('sar_transactions_view', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve test feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VersionWarning: No training dataset version was provided to initialise serving. Defaulting to version 1.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0.13333333333333333,\n",
       " 0.10145738951559842,\n",
       " 0.424836226818694,\n",
       " 0.5523306125312814,\n",
       " 0.3333333333333333,\n",
       " 0.4982684096194535,\n",
       " 0.7698600424453773,\n",
       " 0.217727110291117,\n",
       " [51.6157112121582,\n",
       "  46.86443328857422,\n",
       "  47.00802230834961,\n",
       "  47.32992935180664,\n",
       "  46.863372802734375,\n",
       "  42.14142608642578,\n",
       "  34.83536148071289,\n",
       "  -0.0,\n",
       "  38.38047409057617,\n",
       "  36.33411407470703,\n",
       "  52.19700241088867,\n",
       "  48.818546295166016,\n",
       "  47.00347900390625,\n",
       "  41.64397430419922,\n",
       "  43.1258544921875,\n",
       "  46.23503875732422,\n",
       "  48.12347412109375,\n",
       "  42.12324142456055,\n",
       "  49.05451202392578,\n",
       "  -0.0,\n",
       "  44.95339584350586,\n",
       "  44.95967483520508,\n",
       "  -0.0,\n",
       "  47.70062255859375,\n",
       "  -0.0,\n",
       "  45.151161193847656,\n",
       "  40.933101654052734,\n",
       "  50.38645553588867,\n",
       "  54.1074104309082,\n",
       "  44.93682861328125,\n",
       "  37.081214904785156,\n",
       "  46.127140045166016]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_view_non_sar.get_feature_vector({'id': \"0016359b\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_score = [\"0016359b\", \n",
    "                \"001dcc27\", \n",
    "                \"0054a022\", \n",
    "                \"00d6b609\", \n",
    "                \"00e14860\", \n",
    "                \"00e39a1b\", \n",
    "                \"014ed5cb\", \n",
    "                \"01ce3306\", \n",
    "                \"01fa19ae\", \n",
    "                \"01fa1d01\", \n",
    "                \"036dce03\", \n",
    "                \"03e09be4\", \n",
    "                \"04b23f4b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'outputs': nan}\n",
      "{'outputs': nan}\n",
      "{'outputs': nan}\n",
      "{'outputs': nan}\n",
      "{'outputs': nan}\n",
      "{'outputs': nan}\n",
      "{'outputs': nan}\n",
      "{'outputs': nan}\n",
      "{'outputs': nan}\n",
      "{'outputs': nan}\n",
      "{'outputs': nan}\n",
      "{'outputs': nan}\n",
      "{'outputs': nan}\n"
     ]
    }
   ],
   "source": [
    "def flat2gen(alist):\n",
    "    for item in alist:\n",
    "        if isinstance(item, list):\n",
    "            for subitem in item: yield subitem\n",
    "        else:\n",
    "            yield item\n",
    "\n",
    "def model_server(input):\n",
    "    data = {\"inputs\": [input]}\n",
    "    return deployment.predict(data)\n",
    "\n",
    "for node_id in ids_to_score:\n",
    "    serving_vector = np.array(list(flat2gen(feature_view_non_sar.get_feature_vector({'id': node_id})))).reshape(1,41).tolist()\n",
    "    print(model_server(serving_vector))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}