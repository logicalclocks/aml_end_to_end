{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train adversarial anomaly detection model\n",
    "In the previous notebook we performed hyperparamer tuning for adversarial anomaly detection model. Now we are ready to train the model based on the best hyper parameters and export to model repository.\n",
    "![Training Dataset](./images/experiment_td.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>9</td><td>application_1627907706936_0010</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://resourcemanager.service.consul:8089/proxy/application_1627907706936_0010/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://ip-10-0-0-108.eu-north-1.compute.internal:8044/node/containerlogs/container_e01_1627907706936_0010_01_000001/amlsim__meb10180\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "<pyspark.sql.session.SparkSession object at 0x7f185ad703d0>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to hsfs and retrieve datasets for training and evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import experiment\n",
    "from hops import hdfs\n",
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "emb_best_hyperparams_path = \"Resources/embeddings_best_hp.json\"\n",
    "emb_best_hyperparams = json.loads(hdfs.load(emb_best_hyperparams_path))\n",
    "emb_args_dict = {}\n",
    "for key in emb_best_hyperparams.keys():\n",
    "    emb_args_dict[key] = [emb_best_hyperparams[key]]\n",
    "\n",
    "    \n",
    "best_hyperparams_path = \"Resources/gan_best_hp.json\"\n",
    "best_hyperparams = json.loads(hdfs.load(best_hyperparams_path))\n",
    "args_dict = {}\n",
    "for key in best_hyperparams.keys():\n",
    "    args_dict[key] = [best_hyperparams[key]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define hopsworks experiments wrapper function and put all the training logic there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_wrapper():\n",
    "    \n",
    "    import os\n",
    "    import sys\n",
    "    import uuid\n",
    "    import random    \n",
    "    \n",
    "    import tensorflow as tf\n",
    "    from adversarialaml import keras_utils\n",
    "    from adversarialaml.gan_enc_ano import GanAnomalyDetector,  GanAnomalyMonitor \n",
    "    from hops import tensorboard\n",
    "    from hops import model as hops_model\n",
    "    \n",
    "    ## Connect to hsfs and retrieve datasets for training and evaluation \n",
    "    import hsfs\n",
    "    # Create a connection\n",
    "    connection = hsfs.connection(engine = \"training\")\n",
    "    # Get the feature store handle for the project's feature store\n",
    "    fs = connection.get_feature_store()\n",
    "\n",
    "    ben_td = fs.get_training_dataset(\"gan_non_sar_training_df\", 1)\n",
    "    eval_td = fs.get_training_dataset(\"gan_eval_df\", 1)    \n",
    "        \n",
    "    latent_dim = args_dict['latent_dim'][0]\n",
    "    discriminator_n_layers = args_dict['discriminator_n_layers'][0]\n",
    "    discriminator_batch_norm = args_dict['discriminator_batch_norm'][0]\n",
    "    discriminator_dropout_rate = args_dict['discriminator_dropout_rate'][0]\n",
    "    discriminator_learning_rate = args_dict['discriminator_learning_rate'][0]\n",
    "    discriminator_extra_steps = args_dict['discriminator_extra_steps'][0]\n",
    "\n",
    "    generator_start_n_units = args_dict['generator_start_n_units'][0]\n",
    "    generator_n_layers = args_dict['generator_n_layers'][0]\n",
    "    generator_activation_fn = args_dict['generator_activation_fn'][0]\n",
    "    generator_batch_norm = args_dict['generator_batch_norm'][0]\n",
    "    generator_dropout_rate = args_dict['generator_dropout_rate'][0] \n",
    "    generator_learning_rate = args_dict['generator_learning_rate'][0]\n",
    "\n",
    "    encoder_start_n_units = args_dict['encoder_start_n_units'][0] \n",
    "    encoder_n_layers = args_dict['encoder_n_layers'][0]\n",
    "    encoder_activation_fn = args_dict['encoder_activation_fn'][0]\n",
    "    encoder_batch_norm = args_dict['encoder_batch_norm'][0]\n",
    "    encoder_dropout_rate = args_dict['encoder_dropout_rate'][0] \n",
    "    encoder_learning_rate = args_dict['encoder_learning_rate'][0]\n",
    "    \n",
    "    discriminator_activation_fn = args_dict['discriminator_activation_fn'][0]\n",
    "\n",
    "    discriminator_middle_layer_activation_fn = args_dict['discriminator_middle_layer_activation_fn'][0]    \n",
    "    generator_middle_layer_activation_fn = args_dict['generator_middle_layer_activation_fn'][0]  \n",
    "    encoder_middle_layer_activation_fn = args_dict['encoder_middle_layer_activation_fn'][0]  \n",
    "    \n",
    "    int_to_act_fn = {\n",
    "        1: 'linear',        \n",
    "        2: 'relu',\n",
    "        3: 'leaky_relu',\n",
    "        4: 'selu',\n",
    "        5: 'tanh'\n",
    "    }    \n",
    "\n",
    "    # Define distribution strategy\n",
    "    strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "    \n",
    "    EPOCHS = 2\n",
    "    # Define per device batch size\n",
    "    batch_size_per_replica = 32\n",
    "    # Define global batch size\n",
    "    BATCH_SIZE = batch_size_per_replica * strategy.num_replicas_in_sync\n",
    "    TOTAL_SAMPLES = 6366\n",
    "    STEPS_PER_EPOCH=TOTAL_SAMPLES//BATCH_SIZE\n",
    "\n",
    "    train_input = ben_td.tf_data(target_name='is_sar', is_training=True)\n",
    "    train_input_processed = train_input.tf_record_dataset(process=True, batch_size=BATCH_SIZE, num_epochs=EPOCHS)\n",
    "    train_input_processed  = train_input_processed.with_options(options)\n",
    "\n",
    "    eval_input = eval_td.tf_data(target_name='is_sar', is_training=True)\n",
    "    eval_input_processed = eval_input.tf_record_dataset(process=True, batch_size=1, num_epochs=EPOCHS)    \n",
    "    eval_input_processed  = eval_input_processed.with_options(options)\n",
    "        \n",
    "    discriminator_activation_fn=int_to_act_fn[discriminator_activation_fn]\n",
    "    discriminator_middle_layer_activation_fn=int_to_act_fn[discriminator_middle_layer_activation_fn]\n",
    "    \n",
    "    if discriminator_dropout_rate > 0.0:\n",
    "        discriminator_batch_dropout = True\n",
    "    else:\n",
    "        discriminator_batch_dropout = False\n",
    "    \n",
    "\n",
    "    if discriminator_dropout_rate > 0.0:\n",
    "        generator_batch_dropout=True\n",
    "    else:\n",
    "        generator_batch_dropout=False\n",
    "\n",
    "    if encoder_dropout_rate > 0.0:\n",
    "        encoder_batch_dropout=True\n",
    "    else:\n",
    "        encoder_batch_dropout=False   \n",
    "\n",
    "\n",
    "    if discriminator_batch_norm==0:\n",
    "        discriminator_batch_norm = False\n",
    "    else:\n",
    "        discriminator_batch_norm = True\n",
    "\n",
    "    generator_activation_fn=int_to_act_fn[generator_activation_fn]\n",
    "    generator_middle_layer_activation_fn=int_to_act_fn[generator_middle_layer_activation_fn]\n",
    "\n",
    "    if generator_batch_norm==0:\n",
    "        generator_batch_norm = False\n",
    "    else:\n",
    "        generator_batch_norm = True\n",
    "\n",
    "    encoder_activation_fn=int_to_act_fn[encoder_activation_fn]\n",
    "    encoder_middle_layer_activation_fn=int_to_act_fn[encoder_middle_layer_activation_fn]\n",
    "                \n",
    "    if encoder_batch_norm==0:\n",
    "        encoder_batch_norm=False\n",
    "    else:\n",
    "        encoder_batch_norm=True        \n",
    "        \n",
    "    discriminator_double_neurons=False\n",
    "    discriminator_bottleneck_neurons=True\n",
    "    generator_double_neurons=True\n",
    "    generator_bottleneck_neurons=False\n",
    "    \n",
    "    # construct model under distribution strategy scope\n",
    "    with strategy.scope():    \n",
    "        # Instantiate the GanAnomalyDetector model.\n",
    "        gan_anomaly_detector = GanAnomalyDetector(\n",
    "                    input_dim=emb_args_dict['emb_size'][0],\n",
    "                    latent_dim=latent_dim,\n",
    "\n",
    "                    discriminator_start_n_units=emb_args_dict['emb_size'][0],\n",
    "                    discriminator_n_layers=discriminator_n_layers,\n",
    "                    discriminator_activation_fn=discriminator_activation_fn,\n",
    "                    discriminator_middle_layer_activation_fn=discriminator_middle_layer_activation_fn,\n",
    "                    discriminator_double_neurons=discriminator_double_neurons,\n",
    "                    discriminator_bottleneck_neurons=discriminator_bottleneck_neurons,\n",
    "                    discriminator_batch_norm=discriminator_batch_norm,\n",
    "                    discriminator_batch_dropout=discriminator_batch_dropout,\n",
    "                    discriminator_dropout_rate=discriminator_dropout_rate,\n",
    "                    discriminator_learning_rate=discriminator_learning_rate,\n",
    "                    discriminator_extra_steps=discriminator_extra_steps,\n",
    "\n",
    "                    generator_start_n_units=generator_start_n_units,\n",
    "                    generator_n_layers=generator_n_layers,\n",
    "                    generator_activation_fn=generator_activation_fn,\n",
    "                    generator_middle_layer_activation_fn=generator_middle_layer_activation_fn,        \n",
    "                    generator_double_neurons=generator_double_neurons,\n",
    "                    generator_bottleneck_neurons=generator_bottleneck_neurons,\n",
    "                    generator_batch_norm=generator_batch_norm,\n",
    "                    generator_batch_dropout=generator_batch_dropout,\n",
    "                    generator_dropout_rate=generator_dropout_rate,\n",
    "                    generator_learning_rate=generator_learning_rate,\n",
    "\n",
    "                    encoder_start_n_units=encoder_start_n_units,\n",
    "                    encoder_n_layers=encoder_n_layers,\n",
    "                    encoder_activation_fn=encoder_activation_fn,\n",
    "                    encoder_middle_layer_activation_fn=encoder_middle_layer_activation_fn,        \n",
    "                    encoder_batch_norm=encoder_batch_norm,\n",
    "                    encoder_batch_dropout=encoder_batch_dropout,\n",
    "                    encoder_dropout_rate=encoder_dropout_rate,\n",
    "                    encoder_learning_rate=encoder_learning_rate,\n",
    "        )\n",
    "    \n",
    "\n",
    "        # Compile the WGAN model.\n",
    "        gan_anomaly_detector.compile()\n",
    "\n",
    "    callbacks = [\n",
    "        GanAnomalyMonitor(batch_size=1, latent_dim=latent_dim, input_dim=emb_args_dict['emb_size'][0]),\n",
    "        keras_utils.TimeHistory(BATCH_SIZE,2, logdir=tensorboard.logdir()),\n",
    "        tf.keras.callbacks.TensorBoard(log_dir=tensorboard.logdir()),\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=tensorboard.logdir()),\n",
    "    ]\n",
    "    \n",
    "    # Start training the model.\n",
    "    history = gan_anomaly_detector.fit(train_input_processed, \n",
    "                                       callbacks=[callbacks], \n",
    "                                       epochs=EPOCHS, \n",
    "                                       steps_per_epoch= STEPS_PER_EPOCH)\n",
    "\n",
    "    metrics={'loss': history.history[\"g_loss\"][0]} \n",
    "\n",
    "    # save to the model registry\n",
    "    export_path = os.getcwd() + '/model-' + str(uuid.uuid4())\n",
    "    print('Exporting trained model to: {}'.format(export_path))\n",
    "    \n",
    "    call = gan_anomaly_detector.serve_function.get_concrete_function(tf.TensorSpec([None,None], tf.float32))\n",
    "    tf.saved_model.save(gan_anomaly_detector, export_path, signatures=call)\n",
    "\n",
    "    print('Done exporting!')\n",
    "        \n",
    "    hops_model.export(export_path, 'ganAml', metrics=metrics)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use above experiments wrapper function to conduct hops training experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment \n",
      "\n",
      "('hdfs://rpc.namenode.service.consul:8020/Projects/amlsim/Experiments/application_1627907706936_0010_1', {'loss': -0.569298267364502, 'log': 'Experiments/application_1627907706936_0010_1/output.log'})"
     ]
    }
   ],
   "source": [
    "from hops import experiment\n",
    "    \n",
    "experiment.launch(experiment_wrapper,  name='train_gan_aml', metric_key='loss', local_logdir=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing experiments\n",
    "Experiments service provides a unified view of all the experiments run using the `experiment` module.\n",
    "<br>\n",
    "As demonstrated in the gif it provides general information about the experiment and the resulting metric. Experiments can be visualized meanwhile or after training in a TensorBoard.\n",
    "<br>\n",
    "<br>\n",
    "![Image7-Monitor.png](./images/experiments.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
