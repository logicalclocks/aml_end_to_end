{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create feature groupsÂ¶\n",
    "In this notebook we are going to read raw datasets, perform feature engineering and write to feature stores as feature groups. \n",
    "\n",
    "![Feature Stores](./images/online_offline_fs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "First we will import the multiple libraries we need for feature engineering and writing into the Feature Store.\n",
    "\n",
    "**The process will then be:**\n",
    "1. Define the feature engineering utiliy functions\n",
    "2. Load transactions datasets \n",
    "3. Load alert transactions datasets \n",
    "4. Load party datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for feature engineering\n",
    "# common libaries for hashing and date time conversions\n",
    "\n",
    "import hashlib\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark functions for feature engineering \n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hops hdfs utility library for reading and writing files from HopsFs\n",
    "\n",
    "from hops import hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define feature engineering utility functions\n",
    "Creating users defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_2_code(input_str):\n",
    "    x = input_str.split(\"-\")[0]\n",
    "    if (x == \"CASH_IN\"):\n",
    "        node_type = 0\n",
    "    elif (x == \"CASH_OUT\"):\n",
    "        node_type = 1\n",
    "    elif (x == \"DEBIT\"):\n",
    "        node_type = 2\n",
    "    elif (x == \"PAYMENT\"):\n",
    "        node_type = 3\n",
    "    elif (x == \"TRANSFER\"):\n",
    "        node_type = 4\n",
    "    elif (x == \"DEPOSIT\"):\n",
    "        node_type = 4        \n",
    "    else:\n",
    "        node_type = 99\n",
    "    return node_type\n",
    "\n",
    "def party_2_code(x):\n",
    "    if (x == \"Organization\"):\n",
    "        party_type = 0\n",
    "    elif (x == \"Individual\"):\n",
    "        party_type = 1\n",
    "    else:    \n",
    "        party_type = 99\n",
    "    return party_type\n",
    "\n",
    "def timestamp_2_time(x):\n",
    "    dt_obj = datetime.strptime(str(x), '%Y-%m-%d %H:%M:%S')\n",
    "    return dt_obj.strftime(\"%b-%d\") \n",
    "\n",
    "action_2_code_udf = F.udf(action_2_code)\n",
    "party_2_code_udf = F.udf(party_2_code)\n",
    "timestamp_2_time_udf = F.udf(timestamp_2_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load transactions datasets as spark dataframe and perform feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = spark.read\\\n",
    "             .option(\"inferSchema\", \"true\")\\\n",
    "             .option(\"header\", \"true\")\\\n",
    "             .format(\"csv\")\\\n",
    "             .load(\"hdfs:///Projects/{}/Resources/transactions.csv\".format(hdfs.project_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------------+-------+-------+--------+\n",
      "|  source|  target|tran_timestamp|tran_id|tx_type|base_amt|\n",
      "+--------+--------+--------------+-------+-------+--------+\n",
      "|3aa9646b|1e46e726|        Jan-01|    496|      4|  858.77|\n",
      "|49203bc3|a74d1101|        Jan-01|   1342|      4|  386.86|\n",
      "|616d4505|99af2455|        Jan-02|   1580|      4|  616.43|\n",
      "|39be1ea2|e7ec7bdb|        Jan-02|   2866|      4|  146.44|\n",
      "|e2e0d938|afc399a9|        Jan-03|   3997|      4|  439.09|\n",
      "|75c9a805|d7a317f6|        Jan-04|   5518|      4|   361.0|\n",
      "|c14f4989|733a496b|        Jan-06|   7340|      4|  768.98|\n",
      "|576eb672|aa49b0eb|        Jan-07|   9376|      4|   943.4|\n",
      "|847a9cf6|b070a6bb|        Jan-08|  10362|      4|   668.3|\n",
      "|12a388ff|586377aa|        Jan-08|  10817|      4|  139.84|\n",
      "|b36f9c84|1b467848|        Jan-08|  11317|      4|  499.47|\n",
      "|362e42e0|385afb8b|        Jan-09|  11748|      4|  357.96|\n",
      "|572014da|acd60eca|        Jan-10|  13285|      4|   630.9|\n",
      "|5ff2d9a7|31976e38|        Jan-11|  14832|      4|  685.07|\n",
      "|24bf603c|fcf3bbf3|        Jan-11|  15619|      4|  964.81|\n",
      "|9a118f8d|ca0967a6|        Jan-12|  16574|      4|  919.76|\n",
      "|65b8a85f|bc0de3c7|        Jan-14|  18944|      4|  302.26|\n",
      "|d7a317f6|31db9495|        Jan-14|  19204|      4|  637.18|\n",
      "|95aac0c4|3a63a8fc|        Jan-14|  19530|      4|  630.43|\n",
      "|5411dc34|f388cc0f|        Jan-15|  20382|      4|  374.73|\n",
      "+--------+--------+--------------+-------+-------+--------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "transactions_df = transactions_df.withColumn('tx_type', action_2_code_udf(F.col('tx_type')))\\\n",
    "                                 .withColumn('tran_timestamp', timestamp_2_time_udf(F.col('tran_timestamp')).cast(StringType()))\\\n",
    "                                 .withColumnRenamed(\"src\",\"source\")\\\n",
    "                                 .withColumnRenamed(\"dst\",\"target\")\\\n",
    "                                 .select(\"source\",\"target\",\"tran_timestamp\",\"tran_id\",\"tx_type\",\"base_amt\")\n",
    "transactions_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load alert transactions datasets as spark dataframe and perform feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+------+-------+\n",
      "|alert_id|    alert_type|is_sar|tran_id|\n",
      "+--------+--------------+------+-------+\n",
      "|      47|gather_scatter|  true|  11873|\n",
      "|      47|gather_scatter|  true|  11874|\n",
      "|      47|gather_scatter|  true|  11875|\n",
      "|      47|gather_scatter|  true|  13151|\n",
      "|      47|gather_scatter|  true|  23148|\n",
      "|      17|scatter_gather|  true|  23779|\n",
      "|      17|scatter_gather|  true|  23780|\n",
      "|      17|scatter_gather|  true|  26441|\n",
      "|      17|scatter_gather|  true|  26442|\n",
      "|      47|gather_scatter|  true|  28329|\n",
      "|      47|gather_scatter|  true|  31581|\n",
      "|      47|gather_scatter|  true|  34310|\n",
      "|      17|scatter_gather|  true|  34433|\n",
      "|      58|gather_scatter|  true|  36131|\n",
      "|      17|scatter_gather|  true|  36563|\n",
      "|      17|scatter_gather|  true|  41430|\n",
      "|      17|scatter_gather|  true|  42363|\n",
      "|      58|gather_scatter|  true|  42511|\n",
      "|      58|gather_scatter|  true|  44370|\n",
      "|      58|gather_scatter|  true|  46176|\n",
      "+--------+--------------+------+-------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "alert_transactions = spark.read\\\n",
    "             .option(\"inferSchema\", \"true\")\\\n",
    "             .option(\"header\", \"true\")\\\n",
    "             .format(\"csv\")\\\n",
    "             .load(\"hdfs:///Projects/{}/Resources/alert_transactions.csv\".format(hdfs.project_name()))\n",
    "alert_transactions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+------+-------+\n",
      "|alert_id|    alert_type|is_sar|tran_id|\n",
      "+--------+--------------+------+-------+\n",
      "|      47|gather_scatter|     1|  11873|\n",
      "|      47|gather_scatter|     1|  11874|\n",
      "|      47|gather_scatter|     1|  11875|\n",
      "|      47|gather_scatter|     1|  13151|\n",
      "|      47|gather_scatter|     1|  23148|\n",
      "|      17|scatter_gather|     1|  23779|\n",
      "|      17|scatter_gather|     1|  23780|\n",
      "|      17|scatter_gather|     1|  26441|\n",
      "|      17|scatter_gather|     1|  26442|\n",
      "|      47|gather_scatter|     1|  28329|\n",
      "|      47|gather_scatter|     1|  31581|\n",
      "|      47|gather_scatter|     1|  34310|\n",
      "|      17|scatter_gather|     1|  34433|\n",
      "|      58|gather_scatter|     1|  36131|\n",
      "|      17|scatter_gather|     1|  36563|\n",
      "|      17|scatter_gather|     1|  41430|\n",
      "|      17|scatter_gather|     1|  42363|\n",
      "|      58|gather_scatter|     1|  42511|\n",
      "|      58|gather_scatter|     1|  44370|\n",
      "|      58|gather_scatter|     1|  46176|\n",
      "+--------+--------------+------+-------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "alert_transactions = alert_transactions.select(\"alert_id\",\"alert_type\",\"is_sar\",\"tran_id\").withColumn(\"is_sar\",F.when(F.col(\"is_sar\") == \"true\", 1).otherwise(0))\n",
    "alert_transactions.orderBy(\"tran_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Load party datasets as spark dataframe and ingest into hsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "| partyId|   partyType|\n",
      "+--------+------------+\n",
      "|5628bd6c|Organization|\n",
      "|a1fcba39|Organization|\n",
      "|f56c9501|  Individual|\n",
      "|9969afdd|Organization|\n",
      "|b356eeae|  Individual|\n",
      "|3406706a|Organization|\n",
      "|26c56102|Organization|\n",
      "|e386ebf7|  Individual|\n",
      "|8c094b0d|  Individual|\n",
      "|939235aa|  Individual|\n",
      "|de6bf2a5|Organization|\n",
      "|33a8ff5b|Organization|\n",
      "|a32807a1|  Individual|\n",
      "|2906ef08|Organization|\n",
      "|c2a01b8d|  Individual|\n",
      "|5a99160f|  Individual|\n",
      "|8b9017b8|Organization|\n",
      "|fcf3bbf3|  Individual|\n",
      "|5132aa4d|Organization|\n",
      "|68b90958|  Individual|\n",
      "+--------+------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "party = spark.read\\\n",
    "             .option(\"inferSchema\", \"true\")\\\n",
    "             .option(\"header\", \"true\")\\\n",
    "             .format(\"csv\")\\\n",
    "             .load(\"hdfs:///Projects/{}/Resources/party.csv\".format(hdfs.project_name()))\n",
    "party.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+\n",
      "|      id|type|\n",
      "+--------+----+\n",
      "|5628bd6c|   0|\n",
      "|a1fcba39|   0|\n",
      "|f56c9501|   1|\n",
      "|9969afdd|   0|\n",
      "|b356eeae|   1|\n",
      "|3406706a|   0|\n",
      "|26c56102|   0|\n",
      "|e386ebf7|   1|\n",
      "|8c094b0d|   1|\n",
      "|939235aa|   1|\n",
      "|de6bf2a5|   0|\n",
      "|33a8ff5b|   0|\n",
      "|a32807a1|   1|\n",
      "|2906ef08|   0|\n",
      "|c2a01b8d|   1|\n",
      "|5a99160f|   1|\n",
      "|8b9017b8|   0|\n",
      "|fcf3bbf3|   1|\n",
      "|5132aa4d|   0|\n",
      "|68b90958|   1|\n",
      "+--------+----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "party=party.withColumn('partyType', party_2_code_udf(F.col('partyType'))).toDF(\"id\",\"type\")\n",
    "party.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register feature groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Instantiate a connection and get the `project` feature store handler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "from hsfs.rule import Rule\n",
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Validation\n",
    "Before we define [feature groups](https://docs.hopsworks.ai/latest/generated/feature_group/) lets define [validation rules](https://docs.hopsworks.ai/latest/generated/feature_validation/) for features. We do expect some of the features to comply with certain *rules* or *expectations*. For example: a transacted amount must be a positive value. In the case of a transacted amount arriving as a negative value we can decide whether to stop it from `write` into a feature group and throw an error or allow it to be written but provide a warning. In the next section we will create feature store `expectations`, attach them to feature groups, and apply them to dataframes being appended to said feature group.\n",
    "\n",
    "#### Data validation rules supported in Hopsworks\n",
    "Hopsworks comes shipped with a set of data validation rules. These rules are **immutable**, uniquely identified by **name** and are available across all feature stores. These rules are used to create feature store expectations which can then be attached to individual feature groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all rule definitions available in Hopsworks\n",
    "rules = connection.get_rules()\n",
    "[print(rule.to_dict()) for rule in rules]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'IS_POSITIVE', 'predicate': 'VALUE', 'acceptedType': 'Boolean', 'featureType': None, 'description': 'Assert on a feature containing non negative values.'}"
     ]
    }
   ],
   "source": [
    "# Get a rule definition by name\n",
    "is_positive = connection.get_rule(\"IS_POSITIVE\")\n",
    "print(is_positive.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create Expectations based on Hopsworks rules\n",
    "Expectations are created at the feature store level. Multiple expectations can be created per feature store.\n",
    "An expectation consist of one or multiple rules and can refer to one or multiple features. An expectation can be utilized by attaching it to a feature group, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expectation.rules[0].to_dict(){'name': 'IS_POSITIVE', 'level': 'ERROR', 'min': None, 'max': None, 'value': True, 'pattern': None, 'acceptedType': None, 'legalValues': None}\n",
      "ExpectationsApi.expectation.to_dict(){'name': 'amount', 'description': 'validate amount correctness', 'features': ['base_amt'], 'rules': [<hsfs.rule.Rule object at 0x7ffa1d46eb50>]}\n",
      "ExpectationsApi.expectation.rules[0].to_dict(){'name': 'IS_POSITIVE', 'level': 'ERROR', 'min': None, 'max': None, 'value': True, 'pattern': None, 'acceptedType': None, 'legalValues': None}\n",
      "ExpectationsApi.expectation.payload{\"name\": \"amount\", \"description\": \"validate amount correctness\", \"features\": [\"base_amt\"], \"rules\": [{\"name\": \"IS_POSITIVE\", \"level\": \"ERROR\", \"min\": null, \"max\": null, \"value\": true, \"pattern\": null, \"acceptedType\": null, \"legalValues\": null}]}"
     ]
    }
   ],
   "source": [
    "expectation_amount = fs.create_expectation(\"amount\",\n",
    "                                           features=[\"base_amt\"], \n",
    "                                           description=\"validate amount correctness\",\n",
    "                                           rules=[Rule(name=\"IS_POSITIVE\", level=\"ERROR\", value=True)])\n",
    "expectation_amount.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Start the creation of feature groups. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "> ####  a. Create transactions feature group metadata and save it in to hsfs\n",
    "We are going to create time travel enabled feature groups. For this hopsworks uses Apache Hudi. By default, Hudi tends to over-partition input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommended shuffle parallelism for hoodie.[insert|upsert|bulkinsert].shuffle.parallelism is atleast input_data_size/500MB\n",
    "extra_hudi_options = {\n",
    "    \"hoodie.bulkinsert.shuffle.parallelism\":\"1\", \n",
    "    \"hoodie.insert.shuffle.parallelism\":\"1\", \n",
    "    \"hoodie.upsert.shuffle.parallelism\":\"1\",\n",
    "    \"hoodie.parquet.compression.ratio\":\"0.5\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_fg = fs.create_feature_group(name=\"transactions_fg\",\n",
    "                                       version=1,\n",
    "                                       primary_key=[\"tran_id\"],\n",
    "                                       partition_key=[\"tran_timestamp\"],   \n",
    "                                       description=\"transactions features\",\n",
    "                                       time_travel_format=\"HUDI\",  \n",
    "                                       online_enabled=True,  \n",
    "                                       validation_type=\"STRICT\",\n",
    "                                       expectations= [expectation_amount],                                          \n",
    "                                       statistics_config={\"enabled\": True, \"histograms\": True, \"correlations\": True})\n",
    "\n",
    "transactions_fg.save(transactions_df, extra_hudi_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### b. Create alert transactions feature group and save it in to hsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_transactions_fg = fs.create_feature_group(name=\"alert_transactions_fg\",\n",
    "                                       version=1,\n",
    "                                       primary_key=[\"tran_id\"],\n",
    "                                       partition_key=[\"alert_type\"],         \n",
    "                                       description=\"alert transactions\",\n",
    "                                       time_travel_format=\"HUDI\",     \n",
    "                                       online_enabled=True,                                                \n",
    "                                       statistics_config={\"enabled\": True, \"histograms\": True, \"correlations\": False})\n",
    "alert_transactions_fg.save(alert_transactions, extra_hudi_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### c. Create party feature group and save it in to hsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_fg = fs.create_feature_group(name=\"party_fg\",\n",
    "                                       version=1,\n",
    "                                       primary_key=[\"id\"],\n",
    "                                       description=\"party fg\",\n",
    "                                       time_travel_format=\"HUDI\",\n",
    "                                       online_enabled=True,\n",
    "                                       statistics_config={\"enabled\": True, \"histograms\": True, \"correlations\": True})\n",
    "party_fg.save(party, extra_hudi_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature groups exploration from the user interface\n",
    "\n",
    "##### Hopsworks provides user interface that enables exploration and discovery of available Feature Groups and related features. Bellow screenshot demonstrates how one can preview the list of available features in `transactions_fg` and get basic information such as identify feature types and which one are used as primary and partition keys.    \n",
    "\n",
    "![Incremental Feature Engineering](./images/feature_list.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can also preview the data itself. This is similar to `.head()` method   \n",
    "\n",
    "![Incremental Feature Engineering](./images/data_preview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One of the important steps of feature group exploration is the discovery distribution of ist features and whether they are correlated or not. Since we enabled statistics to be computed during the feature group creation we can easily preview descriptive statistics.  \n",
    "\n",
    "![Incremental Feature Engineering](./images/statistics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can also disover additional properties. Here we can see if there are expection attached to this feature group or not. Please note that we attached expectation using python API but it is also possible too attach and manage feature group expectations from the UI itself.\n",
    "\n",
    "![Incremental Feature Engineering](./images/expecation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hopsworks UI also provide access to feature group activity timeline metadata.  \n",
    "![Incremental Feature Engineering](./images/activity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**NOTE**:\n",
    "\n",
    "All of the above UI functionality is also available via hsfs API. For more details please refer to the [Hopsworks Feature Store documentation.](https://docs.hopsworks.ai/latest/generated/feature_store/)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python 3 (ipykernel)"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    },
    {
     "diff": [
      {
       "key": "version",
       "op": "add",
       "value": "3.8.5"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    },
    {
     "diff": [
      {
       "key": "version",
       "op": "add",
       "value": "3.7.9"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
