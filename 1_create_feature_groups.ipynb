{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>2</td><td>application_1629134411192_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://resourcemanager.service.consul:8089/proxy/application_1629134411192_0004/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://ip-10-0-0-75.eu-north-1.compute.internal:8044/node/containerlogs/container_e01_1629134411192_0004_01_000001/amlsim__meb10180\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "<pyspark.sql.session.SparkSession object at 0x7ffa7c7a8c50>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for feature engineering\n",
    "# common libaries for hashing and date time conversions\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "# pyspark functions for feature engineering \n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType, StringType\n",
    "# Hops hdfs utility library for reading and writing files from FopsFs  \n",
    "from hops import hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define feature engineering utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_2_code(input_str):\n",
    "    x = input_str.split(\"-\")[0]\n",
    "    if (x == \"CASH_IN\"):\n",
    "        node_type = 0\n",
    "    elif (x == \"CASH_OUT\"):\n",
    "        node_type = 1\n",
    "    elif (x == \"DEBIT\"):\n",
    "        node_type = 2\n",
    "    elif (x == \"PAYMENT\"):\n",
    "        node_type = 3\n",
    "    elif (x == \"TRANSFER\"):\n",
    "        node_type = 4\n",
    "    elif (x == \"DEPOSIT\"):\n",
    "        node_type = 4        \n",
    "    else:\n",
    "        node_type = 99\n",
    "    return node_type\n",
    "\n",
    "def party_2_code(x):\n",
    "    if (x == \"Organization\"):\n",
    "        party_type = 0\n",
    "    elif (x == \"Individual\"):\n",
    "        party_type = 1\n",
    "    else:    \n",
    "        party_type = 99\n",
    "    return party_type\n",
    "\n",
    "def timestamp_2_time(x):\n",
    "    dt_obj = datetime.strptime(str(x), '%Y-%m-%d %H:%M:%S')\n",
    "    return dt_obj.strftime(\"%b-%d\") \n",
    "\n",
    "action_2_code_udf = F.udf(action_2_code)\n",
    "party_2_code_udf = F.udf(party_2_code)\n",
    "timestamp_2_time_udf = F.udf(timestamp_2_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load transactions datasets as spark dataframe and perform feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = spark.read\\\n",
    "             .option(\"inferSchema\", \"true\")\\\n",
    "             .option(\"header\", \"true\")\\\n",
    "             .format(\"csv\")\\\n",
    "             .load(\"hdfs:///Projects/{}/Resources/transactions.csv\".format(hdfs.project_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+--------+-------------------+--------+--------+\n",
      "|tran_id|         tx_type|base_amt|     tran_timestamp|     src|     dst|\n",
      "+-------+----------------+--------+-------------------+--------+--------+\n",
      "|    496| TRANSFER-FanOut|  858.77|2020-01-01 00:00:00|3aa9646b|1e46e726|\n",
      "|   1342| TRANSFER-Mutual|  386.86|2020-01-01 00:00:00|49203bc3|a74d1101|\n",
      "|   1580| TRANSFER-FanOut|  616.43|2020-01-02 00:00:00|616d4505|99af2455|\n",
      "|   2866| TRANSFER-FanOut|  146.44|2020-01-02 00:00:00|39be1ea2|e7ec7bdb|\n",
      "|   3997| TRANSFER-Mutual|  439.09|2020-01-03 00:00:00|e2e0d938|afc399a9|\n",
      "|   5518| TRANSFER-FanOut|   361.0|2020-01-04 00:00:00|75c9a805|d7a317f6|\n",
      "|   7340| TRANSFER-Mutual|  768.98|2020-01-06 00:00:00|c14f4989|733a496b|\n",
      "|   9376| TRANSFER-FanOut|   943.4|2020-01-07 00:00:00|576eb672|aa49b0eb|\n",
      "|  10362|TRANSFER-Forward|   668.3|2020-01-08 00:00:00|847a9cf6|b070a6bb|\n",
      "|  10817| TRANSFER-FanOut|  139.84|2020-01-08 00:00:00|12a388ff|586377aa|\n",
      "|  11317| TRANSFER-Mutual|  499.47|2020-01-08 00:00:00|b36f9c84|1b467848|\n",
      "|  11748|TRANSFER-Forward|  357.96|2020-01-09 00:00:00|362e42e0|385afb8b|\n",
      "|  13285|TRANSFER-Forward|   630.9|2020-01-10 00:00:00|572014da|acd60eca|\n",
      "|  14832| TRANSFER-Mutual|  685.07|2020-01-11 00:00:00|5ff2d9a7|31976e38|\n",
      "|  15619| TRANSFER-FanOut|  964.81|2020-01-11 00:00:00|24bf603c|fcf3bbf3|\n",
      "|  16574|TRANSFER-Forward|  919.76|2020-01-12 00:00:00|9a118f8d|ca0967a6|\n",
      "|  18944| TRANSFER-FanOut|  302.26|2020-01-14 00:00:00|65b8a85f|bc0de3c7|\n",
      "|  19204|TRANSFER-Forward|  637.18|2020-01-14 00:00:00|d7a317f6|31db9495|\n",
      "|  19530| TRANSFER-Mutual|  630.43|2020-01-14 00:00:00|95aac0c4|3a63a8fc|\n",
      "|  20382| TRANSFER-Mutual|  374.73|2020-01-15 00:00:00|5411dc34|f388cc0f|\n",
      "+-------+----------------+--------+-------------------+--------+--------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "transactions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------------+-------+-------+--------+\n",
      "|  source|  target|tran_timestamp|tran_id|tx_type|base_amt|\n",
      "+--------+--------+--------------+-------+-------+--------+\n",
      "|3aa9646b|1e46e726|        Jan-01|    496|      4|  858.77|\n",
      "|49203bc3|a74d1101|        Jan-01|   1342|      4|  386.86|\n",
      "|616d4505|99af2455|        Jan-02|   1580|      4|  616.43|\n",
      "|39be1ea2|e7ec7bdb|        Jan-02|   2866|      4|  146.44|\n",
      "|e2e0d938|afc399a9|        Jan-03|   3997|      4|  439.09|\n",
      "|75c9a805|d7a317f6|        Jan-04|   5518|      4|   361.0|\n",
      "|c14f4989|733a496b|        Jan-06|   7340|      4|  768.98|\n",
      "|576eb672|aa49b0eb|        Jan-07|   9376|      4|   943.4|\n",
      "|847a9cf6|b070a6bb|        Jan-08|  10362|      4|   668.3|\n",
      "|12a388ff|586377aa|        Jan-08|  10817|      4|  139.84|\n",
      "|b36f9c84|1b467848|        Jan-08|  11317|      4|  499.47|\n",
      "|362e42e0|385afb8b|        Jan-09|  11748|      4|  357.96|\n",
      "|572014da|acd60eca|        Jan-10|  13285|      4|   630.9|\n",
      "|5ff2d9a7|31976e38|        Jan-11|  14832|      4|  685.07|\n",
      "|24bf603c|fcf3bbf3|        Jan-11|  15619|      4|  964.81|\n",
      "|9a118f8d|ca0967a6|        Jan-12|  16574|      4|  919.76|\n",
      "|65b8a85f|bc0de3c7|        Jan-14|  18944|      4|  302.26|\n",
      "|d7a317f6|31db9495|        Jan-14|  19204|      4|  637.18|\n",
      "|95aac0c4|3a63a8fc|        Jan-14|  19530|      4|  630.43|\n",
      "|5411dc34|f388cc0f|        Jan-15|  20382|      4|  374.73|\n",
      "+--------+--------+--------------+-------+-------+--------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "transactions_df = transactions_df.withColumn('tx_type', action_2_code_udf(F.col('tx_type')))\\\n",
    "                                 .withColumn('tran_timestamp', timestamp_2_time_udf(F.col('tran_timestamp')).cast(StringType()))\\\n",
    "                                 .withColumnRenamed(\"src\",\"source\")\\\n",
    "                                 .withColumnRenamed(\"dst\",\"target\")\\\n",
    "                                 .select(\"source\",\"target\",\"tran_timestamp\",\"tran_id\",\"tx_type\",\"base_amt\")\n",
    "transactions_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load alert transactions datasets as spark dataframe and perform feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+------+-------+\n",
      "|alert_id|    alert_type|is_sar|tran_id|\n",
      "+--------+--------------+------+-------+\n",
      "|      47|gather_scatter|  true|  11873|\n",
      "|      47|gather_scatter|  true|  11874|\n",
      "|      47|gather_scatter|  true|  11875|\n",
      "|      47|gather_scatter|  true|  13151|\n",
      "|      47|gather_scatter|  true|  23148|\n",
      "|      17|scatter_gather|  true|  23779|\n",
      "|      17|scatter_gather|  true|  23780|\n",
      "|      17|scatter_gather|  true|  26441|\n",
      "|      17|scatter_gather|  true|  26442|\n",
      "|      47|gather_scatter|  true|  28329|\n",
      "|      47|gather_scatter|  true|  31581|\n",
      "|      47|gather_scatter|  true|  34310|\n",
      "|      17|scatter_gather|  true|  34433|\n",
      "|      58|gather_scatter|  true|  36131|\n",
      "|      17|scatter_gather|  true|  36563|\n",
      "|      17|scatter_gather|  true|  41430|\n",
      "|      17|scatter_gather|  true|  42363|\n",
      "|      58|gather_scatter|  true|  42511|\n",
      "|      58|gather_scatter|  true|  44370|\n",
      "|      58|gather_scatter|  true|  46176|\n",
      "+--------+--------------+------+-------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "alert_transactions = spark.read\\\n",
    "             .option(\"inferSchema\", \"true\")\\\n",
    "             .option(\"header\", \"true\")\\\n",
    "             .format(\"csv\")\\\n",
    "             .load(\"hdfs:///Projects/{}/Resources/alert_transactions.csv\".format(hdfs.project_name()))\n",
    "alert_transactions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+------+-------+\n",
      "|alert_id|    alert_type|is_sar|tran_id|\n",
      "+--------+--------------+------+-------+\n",
      "|      47|gather_scatter|     1|  11873|\n",
      "|      47|gather_scatter|     1|  11874|\n",
      "|      47|gather_scatter|     1|  11875|\n",
      "|      47|gather_scatter|     1|  13151|\n",
      "|      47|gather_scatter|     1|  23148|\n",
      "|      17|scatter_gather|     1|  23779|\n",
      "|      17|scatter_gather|     1|  23780|\n",
      "|      17|scatter_gather|     1|  26441|\n",
      "|      17|scatter_gather|     1|  26442|\n",
      "|      47|gather_scatter|     1|  28329|\n",
      "|      47|gather_scatter|     1|  31581|\n",
      "|      47|gather_scatter|     1|  34310|\n",
      "|      17|scatter_gather|     1|  34433|\n",
      "|      58|gather_scatter|     1|  36131|\n",
      "|      17|scatter_gather|     1|  36563|\n",
      "|      17|scatter_gather|     1|  41430|\n",
      "|      17|scatter_gather|     1|  42363|\n",
      "|      58|gather_scatter|     1|  42511|\n",
      "|      58|gather_scatter|     1|  44370|\n",
      "|      58|gather_scatter|     1|  46176|\n",
      "+--------+--------------+------+-------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "alert_transactions = alert_transactions.select(\"alert_id\",\"alert_type\",\"is_sar\",\"tran_id\").withColumn(\"is_sar\",F.when(F.col(\"is_sar\") == \"true\", 1).otherwise(0))\n",
    "alert_transactions.orderBy(\"tran_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load party datasets as spark dataframe and ingest into hsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "| partyId|   partyType|\n",
      "+--------+------------+\n",
      "|5628bd6c|Organization|\n",
      "|a1fcba39|Organization|\n",
      "|f56c9501|  Individual|\n",
      "|9969afdd|Organization|\n",
      "|b356eeae|  Individual|\n",
      "|3406706a|Organization|\n",
      "|26c56102|Organization|\n",
      "|e386ebf7|  Individual|\n",
      "|8c094b0d|  Individual|\n",
      "|939235aa|  Individual|\n",
      "|de6bf2a5|Organization|\n",
      "|33a8ff5b|Organization|\n",
      "|a32807a1|  Individual|\n",
      "|2906ef08|Organization|\n",
      "|c2a01b8d|  Individual|\n",
      "|5a99160f|  Individual|\n",
      "|8b9017b8|Organization|\n",
      "|fcf3bbf3|  Individual|\n",
      "|5132aa4d|Organization|\n",
      "|68b90958|  Individual|\n",
      "+--------+------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "party = spark.read\\\n",
    "             .option(\"inferSchema\", \"true\")\\\n",
    "             .option(\"header\", \"true\")\\\n",
    "             .format(\"csv\")\\\n",
    "             .load(\"hdfs:///Projects/{}/Resources/party.csv\".format(hdfs.project_name()))\n",
    "party.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+\n",
      "|      id|type|\n",
      "+--------+----+\n",
      "|5628bd6c|   0|\n",
      "|a1fcba39|   0|\n",
      "|f56c9501|   1|\n",
      "|9969afdd|   0|\n",
      "|b356eeae|   1|\n",
      "|3406706a|   0|\n",
      "|26c56102|   0|\n",
      "|e386ebf7|   1|\n",
      "|8c094b0d|   1|\n",
      "|939235aa|   1|\n",
      "|de6bf2a5|   0|\n",
      "|33a8ff5b|   0|\n",
      "|a32807a1|   1|\n",
      "|2906ef08|   0|\n",
      "|c2a01b8d|   1|\n",
      "|5a99160f|   1|\n",
      "|8b9017b8|   0|\n",
      "|fcf3bbf3|   1|\n",
      "|5132aa4d|   0|\n",
      "|68b90958|   1|\n",
      "+--------+----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "party=party.withColumn('partyType', party_2_code_udf(F.col('partyType'))).toDF(\"id\",\"type\")\n",
    "party.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register feature groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate a connection and get the project feature store handler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "from hsfs.rule import Rule\n",
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Validation\n",
    "\n",
    "Before we define feature groups lets define validation rules for features. We expect that some of the feature must comply certain rules or expectations. For example transacted amount must be a positive value. In case transacted amount arrives as negative value we can decide weather to stop it from write to feature group and throw an error or allow write but give us a warning. In the next section we will create feature store expectations, attach them to feature groups, and apply them to dataframes being appended to the feature group.\n",
    "\n",
    "#### Discover data validation rules supported in Hopsworks\n",
    "Hopsworks comes shipped with a set of data validation rules. These rules are **immutable**, uniquely identified by **name** and are available across all feature stores. These rules are used to create feature store expectations which can then be attached to feature groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all rule definitions available in Hopsworks\n",
    "rules = connection.get_rules()\n",
    "[print(rule.to_dict()) for rule in rules]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'IS_POSITIVE', 'predicate': 'VALUE', 'acceptedType': 'Boolean', 'featureType': None, 'description': 'Assert on a feature containing non negative values.'}"
     ]
    }
   ],
   "source": [
    "# Get a rule definition by name\n",
    "is_positive = connection.get_rule(\"IS_POSITIVE\")\n",
    "print(is_positive.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Expectations based on Hopsworks rules\n",
    "Expectations are created at the feature store level. Multiple expectations can be created per feature store.\n",
    "An expectation is comprised from one or multiple rules and can refer to one or multiple features. An expectation can be utilized by attaching it to a feature group, as shown in the next sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expectation.rules[0].to_dict(){'name': 'IS_POSITIVE', 'level': 'ERROR', 'min': None, 'max': None, 'value': True, 'pattern': None, 'acceptedType': None, 'legalValues': None}\n",
      "ExpectationsApi.expectation.to_dict(){'name': 'amount', 'description': 'validate amount correctness', 'features': ['base_amt'], 'rules': [<hsfs.rule.Rule object at 0x7ffa1d46eb50>]}\n",
      "ExpectationsApi.expectation.rules[0].to_dict(){'name': 'IS_POSITIVE', 'level': 'ERROR', 'min': None, 'max': None, 'value': True, 'pattern': None, 'acceptedType': None, 'legalValues': None}\n",
      "ExpectationsApi.expectation.payload{\"name\": \"amount\", \"description\": \"validate amount correctness\", \"features\": [\"base_amt\"], \"rules\": [{\"name\": \"IS_POSITIVE\", \"level\": \"ERROR\", \"min\": null, \"max\": null, \"value\": true, \"pattern\": null, \"acceptedType\": null, \"legalValues\": null}]}"
     ]
    }
   ],
   "source": [
    "expectation_amount = fs.create_expectation(\"amount\",\n",
    "                                           features=[\"base_amt\"], \n",
    "                                           description=\"validate amount correctness\",\n",
    "                                           rules=[Rule(name=\"IS_POSITIVE\", level=\"ERROR\", value=True)])\n",
    "expectation_amount.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we are ready to start creating feature groups. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create transactions feature group metadata and save it in to hsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to create time travel enabled feature groups. For this hopsworks uses Apache Hudi. By default, Hudi tends to over-partition input. \n",
    "# Recommended shuffle parallelism for hoodie.[insert|upsert|bulkinsert].shuffle.parallelism is atleast input_data_size/500MB\n",
    "extra_hudi_options = {\n",
    "    \"hoodie.bulkinsert.shuffle.parallelism\":\"1\", \n",
    "    \"hoodie.insert.shuffle.parallelism\":\"1\", \n",
    "    \"hoodie.upsert.shuffle.parallelism\":\"1\",\n",
    "    \"hoodie.parquet.compression.ratio\":\"0.5\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_fg = fs.create_feature_group(name=\"transactions_fg\",\n",
    "                                       version=1,\n",
    "                                       primary_key=[\"tran_id\"],\n",
    "                                       partition_key=[\"tran_timestamp\"],   \n",
    "                                       description=\"transactions features\",\n",
    "                                       time_travel_format=\"HUDI\",  \n",
    "                                       online_enabled=True,  \n",
    "                                       validation_type=\"STRICT\",\n",
    "                                       expectations= [expectation_amount],                                          \n",
    "                                       statistics_config={\"enabled\": True, \"histograms\": True, \"correlations\": True})\n",
    "\n",
    "transactions_fg.save(transactions_df, extra_hudi_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create alert transactions feature group and save it in to hsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_transactions_fg = fs.create_feature_group(name=\"alert_transactions_fg\",\n",
    "                                       version=1,\n",
    "                                       primary_key=[\"tran_id\"],\n",
    "                                       partition_key=[\"alert_type\"],         \n",
    "                                       description=\"alert transactions\",\n",
    "                                       time_travel_format=\"HUDI\",     \n",
    "                                       online_enabled=True,                                                \n",
    "                                       statistics_config={\"enabled\": True, \"histograms\": True, \"correlations\": False})\n",
    "alert_transactions_fg.save(alert_transactions, extra_hudi_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create party feature group and save it in to hsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_fg = fs.create_feature_group(name=\"party_fg\",\n",
    "                                       version=1,\n",
    "                                       primary_key=[\"id\"],\n",
    "                                       description=\"party fg\",\n",
    "                                       time_travel_format=\"HUDI\",\n",
    "                                       online_enabled=True,\n",
    "                                       statistics_config={\"enabled\": True, \"histograms\": True, \"correlations\": True})\n",
    "party_fg.save(party, extra_hudi_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature groups exploration from the user interface\n",
    "\n",
    "##### Hopsworks provides user interface that enables to discover and explore avaibale Feature Groups and related features. Bellow screen shot demonstrates how one can preview list of available features in `transactions_fg` and get basic information such us identify feature types and which one are used as primary and partition keys.    \n",
    "\n",
    "![Incremental Feature Engineering](./images/feature_list.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can also preview the data itself. This is similar to `.head()` method   \n",
    "\n",
    "![Incremental Feature Engineering](./images/data_preview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One of the important steps of feature group exploration is discover distribution of ist features and weather they are correlated or not. Since we enabled statistics to be computed durring feature group creation we can easily preview descriptive statitsics.  \n",
    "\n",
    "![Incremental Feature Engineering](./images/statistics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can also disover additional properties. Such as is there any expection attached to this feature group or not. Please not that we attached expectation using python API but it also possible too attach and manage feature group expectations from the UI itself.\n",
    "\n",
    "![Incremental Feature Engineering](./images/expecation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hopsworks UI also give access to feature group activity timeline metadata.  \n",
    "![Incremental Feature Engineering](./images/activity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**NOTE**:\n",
    "\n",
    "All of the above UI functionality is also availavel via hsfs API. For more detail please refer to our documentation.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
