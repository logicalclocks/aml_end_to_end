{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training dataset for anomaly detection model\n",
    "In this notebook We are going to create training dataset from node embeddings feature group and register to Hopsworks Feature Store. \n",
    "![Training Dataset](./images/create_training_dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a connection to hsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>6</td><td>application_1651410992845_0007</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://hopsworks0.logicalclocks.com:8089/proxy/application_1651410992845_0007/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://hopsworks0.logicalclocks.com:8044/node/containerlogs/container_1651410992845_0007_01_000001/aml_demo__meb10000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "from hops import hdfs\n",
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve alert nodes feature group from hsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_nodes_fg = fs.get_feature_group(\"alert_nodes_fg\", 1)\n",
    "node_embeddings_fg = fs.get_feature_group(\"node_embeddings_fg\", 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training datasets for anomaly detection \n",
    "###### In the next notebook we are going to train [gan for anomaly detection](https://arxiv.org/pdf/1905.11034.pdf). Durring training step  we will provide only features of accounts that have never been reported for money laundering behaviour.  But we will disclose previously reported accounts to the model only in evaluation step.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_sar_emb_query = node_embeddings_fg.select([\"embedding\"])\\\n",
    "                                      .join(alert_nodes_fg.select([\"is_sar\"])\\\n",
    "                                      .filter(alert_nodes_fg.is_sar == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|           embedding|is_sar|\n",
      "+--------------------+------+\n",
      "|[-0.2894337177276...|     0|\n",
      "|[-0.8168580532073...|     0|\n",
      "|[0.89537668228149...|     0|\n",
      "|[0.55149841308593...|     0|\n",
      "|[0.28041338920593...|     0|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "non_sar_emb_query.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6531"
     ]
    }
   ],
   "source": [
    "non_sar_emb_query.read().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserWarning: Training dataset splits were defined but no `train_split` (the name of the split that is going to be used for training) was provided. Setting this property to `train`. The statistics of this split will be used for transformation functions."
     ]
    }
   ],
   "source": [
    "non_sar_td = fs.create_training_dataset(name=\"gan_non_sar_training_df\",\n",
    "                                       version=1,\n",
    "                                       data_format=\"tfrecord\",\n",
    "                                       label=[\"is_sar\"], \n",
    "                                       statistics_config={\"enabled\": False, \"histograms\": False, \"correlations\": False, \"exact_uniqueness\": False}, \n",
    "                                       splits={'train': 0.8, 'test': 0.2},\n",
    "                                       coalesce=True,\n",
    "                                       description=\"non sar dataset for gan training\")\n",
    "non_sar_td.save(non_sar_emb_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For testing and evaluation we will include known SAR nodes to measure anomaly score  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_sar_td = fs.get_training_dataset(\"gan_non_sar_training_df\", 1)\n",
    "non_sar_test_df = non_sar_td.read(split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sar_emb_query = node_embeddings_fg.select([\"embedding\"])\\\n",
    "                                  .join(alert_nodes_fg.select([\"is_sar\"])\\\n",
    "                                  .filter(alert_nodes_fg.is_sar == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|           embedding|is_sar|\n",
      "+--------------------+------+\n",
      "|[-0.9998507499694...|     0|\n",
      "|[-0.9986557960510...|     0|\n",
      "|[-0.9984421730041...|     0|\n",
      "|[-0.9970214366912...|     0|\n",
      "|[-0.9947502613067...|     0|\n",
      "|[-0.9934816360473...|     0|\n",
      "|[-0.9908211231231...|     0|\n",
      "|[-0.9882340431213...|     0|\n",
      "|[-0.9830579757690...|     0|\n",
      "|[-0.9823658466339...|     0|\n",
      "|[-0.9815602302551...|     0|\n",
      "|[-0.9814951419830...|     0|\n",
      "|[-0.9812114238739...|     0|\n",
      "|[-0.9809970855712...|     0|\n",
      "|[-0.9808425903320...|     0|\n",
      "|[-0.9780859947204...|     0|\n",
      "|[-0.9746136665344...|     0|\n",
      "|[-0.9715352058410...|     0|\n",
      "|[-0.9711296558380...|     0|\n",
      "|[-0.9682199954986...|     0|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "sar_df = sar_emb_query.read()\n",
    "sar_df = sar_df.select(*non_sar_test_df.columns)\n",
    "eval_df = non_sar_test_df.union(sar_df)\n",
    "eval_df.cache()\n",
    "eval_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1267"
     ]
    }
   ],
   "source": [
    "non_sar_test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "816"
     ]
    }
   ],
   "source": [
    "sar_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2083"
     ]
    }
   ],
   "source": [
    "eval_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_eval_ds = fs.create_training_dataset(name=\"gan_eval_df\",\n",
    "                                       version=1,\n",
    "                                       data_format=\"tfrecord\",\n",
    "                                       label=[\"is_sar\"], \n",
    "                                       statistics_config={\"enabled\": False, \"histograms\": False, \"correlations\": False, \"exact_uniqueness\": False}, \n",
    "                                       coalesce = True,\n",
    "                                       description=\"evaluation dataset for gan training\")\n",
    "gan_eval_ds.save(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training dataset provenance\n",
    "![Training dataset provenance](./images/provenance_td.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}