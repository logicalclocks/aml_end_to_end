{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>5</td><td>application_1613335206545_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://resourcemanager.service.consul:8088/proxy/application_1613335206545_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://amlsim-worker-1.internal.cloudapp.net:8042/node/containerlogs/container_e04_1613335206545_0003_01_000001/amlsim__meb10179\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "<pyspark.sql.session.SparkSession object at 0x7f4173022f90>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a connection to hsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve nodes and edges training datasets from hsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_td = fs.get_training_dataset(\"node_td\", 1)\n",
    "edge_td = fs.get_training_dataset(\"edges_td\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert training dataset in to pandas daframe\n",
    "We are going to use StellarGraph library to compute node embeddings. StellarGraph supports loading data via Pandas DataFrames, NumPy arrays, Neo4j and NetworkX graphs. \n",
    "\n",
    "---\n",
    "**NOTE**:\n",
    "\n",
    "Loading large scale dataset in to StellarGraph for training can not be handled with above mentioned fameworks. It will require loading data using frameworks such as `tf.data`. \n",
    "\n",
    "If your training datasets measure from couple of GB to 100s of GBs or even TBs contact us at Logical Clocks and we will help you to setup distributed training pipelines. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fg as pandas\n",
    "node_pdf = node_td.read().toPandas()\n",
    "edge_pdf = edge_td.read().drop(\"tran_timestamp\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define hopsworks experiments wrapper function and put all the training logic there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_computer(walk_number, walk_length, emb_size):\n",
    "    \n",
    "    import os\n",
    "    import sys\n",
    "    import uuid\n",
    "    import random    \n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    import pydoop.hdfs as pydoop\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    from hops import hdfs\n",
    "    from hops import pandas_helper as pandas\n",
    "    from hops import model as hops_model\n",
    "    from hops import tensorboard\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn import preprocessing, feature_extraction, model_selection\n",
    "    from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.manifold import TSNE\n",
    "\n",
    "    import stellargraph as sg\n",
    "    from stellargraph import StellarGraph\n",
    "    from stellargraph import StellarDiGraph\n",
    "    from stellargraph.data import BiasedRandomWalk\n",
    "    from stellargraph.data import UnsupervisedSampler\n",
    "    from stellargraph.mapper import Node2VecLinkGenerator, Node2VecNodeGenerator\n",
    "    from stellargraph.layer import Node2Vec, link_classification\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras  \n",
    "        \n",
    "    ###########\n",
    "    batch_size = 64\n",
    "    epochs = 10\n",
    "    num_samples = [20, 20]\n",
    "    layer_sizes = [100, 100]\n",
    "    learning_rate = 1e-2\n",
    "\n",
    "    node_data = pd.DataFrame(node_pdf[['type']], index=node_pdf['id'])\n",
    "    ###########\n",
    "        \n",
    "    print('Defining StellarDiGraph')\n",
    "    G =StellarDiGraph(node_data,\n",
    "                      edges=edge_pdf, \n",
    "                      edge_type_column=\"tx_type\")\n",
    "\n",
    "\n",
    "    nodes = list(G.nodes())\n",
    "\n",
    "    walker = BiasedRandomWalk(\n",
    "        G,\n",
    "        n=walk_number,\n",
    "        length=walk_length,\n",
    "        p=0.5,  # defines probability, 1/p, of returning to source node\n",
    "        q=2.0,  # defines probability, 1/q, for moving to a node away from the source node\n",
    "    )\n",
    "    unsupervised_samples = UnsupervisedSampler(G, nodes=list(G.nodes()), walker=walker)\n",
    "    generator = Node2VecLinkGenerator(G, batch_size)\n",
    "    node2vec = Node2Vec(emb_size, generator=generator)\n",
    "    \n",
    "    x_inp, x_out = node2vec.in_out_tensors()\n",
    "    prediction = link_classification(\n",
    "        output_dim=1, output_act=\"sigmoid\", edge_embedding_method=\"dot\"\n",
    "    )(x_out)\n",
    "\n",
    "    print('Defining the model')\n",
    "    model = keras.Model(inputs=x_inp, outputs=prediction)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "        loss=keras.losses.binary_crossentropy,\n",
    "        metrics=[keras.metrics.binary_accuracy],\n",
    "    )\n",
    "    \n",
    "    # Create a callback that saves the model's weights every 5 epochs\n",
    "    log_dir = tensorboard.logdir()\n",
    "    cp_callbacks = [\n",
    "        tf.keras.callbacks.TensorBoard(log_dir=log_dir),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "        filepath=tensorboard.logdir(), \n",
    "        verbose=1, \n",
    "        save_weights_only=True,\n",
    "        save_freq=5*batch_size\n",
    "    )]\n",
    "    \n",
    "    # Save the weights using the `checkpoint_path` format\n",
    "    \n",
    "    print('Training the model')\n",
    "    history = model.fit(\n",
    "        generator.flow(unsupervised_samples),\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        use_multiprocessing=False,\n",
    "        workers=4,\n",
    "        shuffle=True,\n",
    "        callbacks=cp_callbacks,\n",
    "    )\n",
    "\n",
    "    binary_accuracy = history.history['binary_accuracy'][-1]\n",
    "    metrics={'accuracy': binary_accuracy} \n",
    "    \n",
    "    # save to the model registry\n",
    "    export_path = os.getcwd() + '/model-' + str(uuid.uuid4())\n",
    "    print('Exporting trained model to: {}'.format(export_path))\n",
    "    model.save(export_path)\n",
    "    print('Done exporting!')\n",
    "        \n",
    "    hops_model.export(export_path, 'NodeEmbeddings', metrics=metrics)\n",
    "    \n",
    "    return metrics    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use above experiments wrapper function to conduct hops training experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import experiment\n",
    "from hops import hdfs\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparams_path = \"Resources/embeddings_best_hp.json\"\n",
    "best_hyperparams = json.loads(hdfs.load(best_hyperparams_path))\n",
    "args_dict = {}\n",
    "for key in best_hyperparams.keys():\n",
    "    args_dict[key] = [best_hyperparams[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment \n",
      "\n",
      "('hdfs://rpc.namenode.service.consul:8020/Projects/amlsim/Experiments/application_1613335206545_0003_1', {'accuracy': 0.6966959238052368, 'log': 'Experiments/application_1613335206545_0003_1/walk_number=2&walk_length=3&emb_size=32/output.log'})"
     ]
    }
   ],
   "source": [
    "experiment.launch(embeddings_computer, args_dict, name='graph_embeddings_compute', metric_key='accuracy', local_logdir=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
