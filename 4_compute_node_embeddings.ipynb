{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training \n",
    "In the previous notebook we performed hyperparamer tuning. Now we are ready to train the embeddings model based on the best hyper parameters and export to model repository.\n",
    "![Training Dataset](./images/experiment_td.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here as well we are going to use StellarGraph library to compute node embeddings. StellarGraph supports loading data via Pandas DataFrames, NumPy arrays, Neo4j and NetworkX graphs. \n",
    "\n",
    "---\n",
    "**NOTE**:\n",
    "\n",
    "Loading large scale dataset in to StellarGraph for training can not be handled with above mentioned fameworks. It will require loading data using frameworks such as `tf.data`. \n",
    "\n",
    "If your training datasets measure from couple of GB to 100s of GBs or even TBs contact us at Logical Clocks and we will help you to setup distributed training pipelines. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define hopsworks experiments wrapper function and put all the training logic there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_computer(walk_number, walk_length, emb_size):\n",
    "    \n",
    "    import os\n",
    "    import sys\n",
    "    import uuid\n",
    "    import random    \n",
    "    \n",
    "    # pandas and numpy\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # hops utility libraries\n",
    "    from hops import hdfs\n",
    "    from hops import pandas_helper as pandas\n",
    "    from hops import model as hops_model\n",
    "    from hops import tensorboard\n",
    "\n",
    "    # tensorlfow \n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras  \n",
    "    \n",
    "    # stellargraph library for graph neural networks\n",
    "    import stellargraph as sg\n",
    "    from stellargraph import StellarGraph\n",
    "    from stellargraph import StellarDiGraph\n",
    "    from stellargraph.data import BiasedRandomWalk\n",
    "    from stellargraph.data import UnsupervisedSampler\n",
    "    from stellargraph.mapper import Node2VecLinkGenerator, Node2VecNodeGenerator\n",
    "    from stellargraph.layer import Node2Vec, link_classification\n",
    "\n",
    "    # get training datasets from hsfs\n",
    "    import hsfs\n",
    "    # Create a connection\n",
    "    connection = hsfs.connection()\n",
    "    # Get the feature store handle for the project's feature store\n",
    "    fs = connection.get_feature_store()\n",
    "\n",
    "    # Retrieve nodes and edges training datasets from hsfs\n",
    "    node_td = fs.get_training_dataset(\"node_td\", 1)\n",
    "    edge_td = fs.get_training_dataset(\"edges_td\", 1)\n",
    "\n",
    "    # Get fg as pandas\n",
    "    node_pdf = node_td.read().toPandas()\n",
    "    edge_pdf = edge_td.read().drop(\"tran_timestamp\").toPandas()\n",
    "\n",
    "    # define static hyperparameters\n",
    "    batch_size = 64\n",
    "    epochs = 10\n",
    "    num_samples = [20, 20]\n",
    "    layer_sizes = [100, 100]\n",
    "    learning_rate = 1e-2\n",
    "\n",
    "    # construct Stellargraph object for training\n",
    "    node_data = pd.DataFrame(node_pdf[['type']], index=node_pdf['id'])\n",
    "        \n",
    "    print('Defining StellarDiGraph')\n",
    "    G =StellarDiGraph(node_data,\n",
    "                      edges=edge_pdf, \n",
    "                      edge_type_column=\"tx_type\")\n",
    "\n",
    "\n",
    "    nodes = list(G.nodes())\n",
    "\n",
    "    walker = BiasedRandomWalk(\n",
    "        G,\n",
    "        n=walk_number,\n",
    "        length=walk_length,\n",
    "        p=0.5,  # defines probability, 1/p, of returning to source node\n",
    "        q=2.0,  # defines probability, 1/q, for moving to a node away from the source node\n",
    "    )\n",
    "    unsupervised_samples = UnsupervisedSampler(G, nodes=list(G.nodes()), walker=walker)\n",
    "    generator = Node2VecLinkGenerator(G, batch_size)\n",
    "    node2vec = Node2Vec(emb_size, generator=generator)\n",
    "    \n",
    "    x_inp, x_out = node2vec.in_out_tensors()\n",
    "    prediction = link_classification(\n",
    "        output_dim=1, output_act=\"sigmoid\", edge_embedding_method=\"dot\"\n",
    "    )(x_out)\n",
    "\n",
    "    # define and train keras model\n",
    "    print('Defining the model')\n",
    "    model = keras.Model(inputs=x_inp, outputs=prediction)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "        loss=keras.losses.binary_crossentropy,\n",
    "        metrics=[keras.metrics.binary_accuracy],\n",
    "    )\n",
    "    \n",
    "    # Create a callback that saves the model's weights every 5 epochs\n",
    "    log_dir = tensorboard.logdir()\n",
    "    cp_callbacks = [\n",
    "        tf.keras.callbacks.TensorBoard(log_dir=log_dir),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "        filepath=tensorboard.logdir(), \n",
    "        verbose=1, \n",
    "        save_weights_only=True,\n",
    "        save_freq=5*batch_size\n",
    "    )]\n",
    "    \n",
    "    # Save the weights using the `checkpoint_path` format\n",
    "    \n",
    "    print('Training the model')\n",
    "    history = model.fit(\n",
    "        generator.flow(unsupervised_samples),\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        use_multiprocessing=False,\n",
    "        workers=4,\n",
    "        shuffle=True,\n",
    "        callbacks=cp_callbacks,\n",
    "    )\n",
    "\n",
    "    binary_accuracy = history.history['binary_accuracy'][-1]\n",
    "    metrics={'accuracy': binary_accuracy} \n",
    "    \n",
    "    # save to the model registry\n",
    "    export_path = os.getcwd() + '/model-' + str(uuid.uuid4())\n",
    "    print('Exporting trained model to: {}'.format(export_path))\n",
    "    model.save(export_path)\n",
    "    print('Done exporting!')\n",
    "        \n",
    "    hops_model.export(export_path, 'NodeEmbeddings', metrics=metrics)\n",
    "    \n",
    "    return metrics    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use above experiments wrapper function to conduct hops training experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import experiment\n",
    "from hops import hdfs\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparams_path = \"Resources/embeddings_best_hp.json\"\n",
    "best_hyperparams = json.loads(hdfs.load(best_hyperparams_path))\n",
    "args_dict = {}\n",
    "for key in best_hyperparams.keys():\n",
    "    args_dict[key] = [best_hyperparams[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Experiment \n",
      "\n",
      "('hdfs://rpc.namenode.service.consul:8020/Projects/amlsim/Experiments/application_1627907706936_0004_1', {'accuracy': 0.6620710492134094, 'log': 'Experiments/application_1627907706936_0004_1/walk_number=3&walk_length=3&emb_size=16/output.log'})"
     ]
    }
   ],
   "source": [
    "experiment.launch(embeddings_computer, args_dict, name='graph_embeddings_compute', metric_key='accuracy', local_logdir=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing experiments\n",
    "Experiments service provides a unified view of all the experiments run using the `experiment` module.\n",
    "<br>\n",
    "As demonstrated in the gif it provides general information about the experiment and the resulting metric. Experiments can be visualized meanwhile or after training in a TensorBoard.\n",
    "<br>\n",
    "<br>\n",
    "![Image7-Monitor.png](./images/experiments.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
