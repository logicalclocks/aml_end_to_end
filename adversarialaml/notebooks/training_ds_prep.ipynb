{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>77</td><td>application_1613059669625_0003</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://resourcemanager.service.consul:8088/proxy/application_1613059669625_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://amlsim-worker-5.internal.cloudapp.net:8042/node/containerlogs/container_e12_1613059669625_0003_01_000001/amldemo__meb10179\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@4cae4aad\n"
     ]
    }
   ],
   "source": [
    "val sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.types.{FloatType, LongType}\n",
      "import org.apache.spark.sql.types.{ArrayType, StructField, StructType}\n",
      "import org.apache.spark.sql.functions.explode\n",
      "import org.apache.spark.sql.expressions.Window\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{FloatType,LongType}\n",
    "import org.apache.spark.sql.types.{ArrayType, StructField, StructType}\n",
    "import org.apache.spark.sql.functions.explode\n",
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import com.logicalclocks.hsfs._\n"
     ]
    }
   ],
   "source": [
    "import com.logicalclocks.hsfs._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection: com.logicalclocks.hsfs.HopsworksConnection = com.logicalclocks.hsfs.HopsworksConnection@7c61b14a\n",
      "fs: com.logicalclocks.hsfs.FeatureStore = FeatureStore{id=1092, name='amldemo_featurestore', projectId=1144, featureGroupApi=com.logicalclocks.hsfs.metadata.FeatureGroupApi@5fdf9eac}\n"
     ]
    }
   ],
   "source": [
    "val connection = HopsworksConnection.builder().build();\n",
    "val fs = connection.getFeatureStore();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_all: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [f_2: double, f_4: double ... 364 more fields]\n"
     ]
    }
   ],
   "source": [
    "val df_all = fs.getFeatureGroup(\"simts_features\", 1).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res1: Long = 60000\n"
     ]
    }
   ],
   "source": [
    "df_all.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res2: Long = 1000\n"
     ]
    }
   ],
   "source": [
    "df_all.where($\"target\" === 1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res3: Long = 59000\n"
     ]
    }
   ],
   "source": [
    "df_all.where($\"target\" === 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WrappedArray(f_2, f_4, f_12, f_21, f_23, f_27, f_30, f_32, f_35, f_37, f_38, f_39, f_52, f_53, f_54, f_57, f_63, f_65, f_69, f_82, f_87, f_99, f_106, f_111, f_118, f_123, f_126, f_127, f_131, f_133, f_140, f_142, f_143, f_146, f_152, f_155, f_157, f_163, f_169, f_170, f_174, f_182, f_184, f_186, f_191, f_196, f_201, f_203, f_214, f_215, f_220, f_221, f_222, f_225, f_229, f_234, f_242, f_243, f_244, f_249, f_256, f_258, f_259, f_274, f_275, f_277, f_281, f_282, f_285, f_291, f_297, f_298, f_306, f_308, f_309, f_310, f_314, f_320, f_322, f_325, f_334, f_335, f_351, f_358, target, f_1, f_7, f_10, f_20, f_22, f_24, f_33, f_34, f_41, f_44, f_46, f_47, f_49, f_51, f_58, f_70, f_74, f_76, f_80, f_81, f_83, f_91, f_92, f_93, f_97, f_110, f_114, f_120, f_121, f_128, f_130, f_132, f_135, f_139, f_145, f_147, f_148, f_149, f_151, f_154, f_159, f_161, f_164, f_168, f_172, f_175, f_178, f_179, f_181, f_185, f_188, f_190, f_193, f_194, f_195, f_197, f_206, f_207, f_212, f_213, f_218, f_224, f_226, f_232, f_236, f_238, f_241, f_246, f_248, f_254, f_261, f_268, f_271, f_278, f_279, f_283, f_284, f_288, f_289, f_290, f_293, f_294, f_300, f_307, f_312, f_316, f_317, f_318, f_329, f_332, f_343, f_347, f_348, f_350, f_352, f_355, f_356, f_361, f_362, f_3, f_8, f_11, f_13, f_15, f_16, f_17, f_18, f_25, f_29, f_36, f_40, f_42, f_45, f_48, f_55, f_60, f_64, f_66, f_67, f_68, f_72, f_73, f_79, f_85, f_86, f_88, f_89, f_98, f_102, f_103, f_104, f_105, f_112, f_116, f_119, f_122, f_125, f_137, f_144, f_165, f_166, f_171, f_173, f_176, f_177, f_180, f_183, f_187, f_204, f_205, f_208, f_211, f_216, f_219, f_227, f_228, f_230, f_237, f_239, f_240, f_245, f_247, f_252, f_257, f_260, f_262, f_273, f_276, f_280, f_286, f_295, f_296, f_304, f_305, f_311, f_313, f_315, f_324, f_326, f_327, f_338, f_340, f_341, f_342, f_344, f_346, f_353, f_357, f_363, f_365, f_5, f_6, f_9, f_14, f_19, f_26, f_28, f_31, f_43, f_50, f_56, f_59, f_61, f_62, f_71, f_75, f_77, f_78, f_84, f_90, f_94, f_95, f_96, f_100, f_101, f_107, f_108, f_109, f_113, f_115, f_117, f_124, f_129, f_134, f_136, f_138, f_141, f_150, f_153, f_156, f_158, f_160, f_162, f_167, f_189, f_192, f_198, f_199, f_200, f_202, f_209, f_210, f_217, f_223, f_231, f_233, f_235, f_250, f_251, f_253, f_255, f_263, f_264, f_265, f_266, f_267, f_269, f_270, f_272, f_287, f_292, f_299, f_301, f_302, f_303, f_319, f_321, f_323, f_328, f_330, f_331, f_333, f_336, f_337, f_339, f_345, f_349, f_354, f_359, f_360, f_364)"
     ]
    }
   ],
   "source": [
    "print(df_all.columns.toSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WrappedArray(f_2, f_4, f_12, f_21, f_23, f_27, f_30, f_32, f_35, f_37, f_38, f_39, f_52, f_53, f_54, f_57, f_63, f_65, f_69, f_82, f_87, f_99, f_106, f_111, f_118, f_123, f_126, f_127, f_131, f_133, f_140, f_142, f_143, f_146, f_152, f_155, f_157, f_163, f_169, f_170, f_174, f_182, f_184, f_186, f_191, f_196, f_201, f_203, f_214, f_215, f_220, f_221, f_222, f_225, f_229, f_234, f_242, f_243, f_244, f_249, f_256, f_258, f_259, f_274, f_275, f_277, f_281, f_282, f_285, f_291, f_297, f_298, f_306, f_308, f_309, f_310, f_314, f_320, f_322, f_325, f_334, f_335, f_351, f_358, target, f_1, f_7, f_10, f_20, f_22, f_24, f_33, f_34, f_41, f_44, f_46, f_47, f_49, f_51, f_58, f_70, f_74, f_76, f_80, f_81, f_83, f_91, f_92, f_93, f_97, f_110, f_114, f_120, f_121, f_128, f_130, f_132, f_135, f_139, f_145, f_147, f_148, f_149, f_151, f_154, f_159, f_161, f_164, f_168, f_172, f_175, f_178, f_179, f_181, f_185, f_188, f_190, f_193, f_194, f_195, f_197, f_206, f_207, f_212, f_213, f_218, f_224, f_226, f_232, f_236, f_238, f_241, f_246, f_248, f_254, f_261, f_268, f_271, f_278, f_279, f_283, f_284, f_288, f_289, f_290, f_293, f_294, f_300, f_307, f_312, f_316, f_317, f_318, f_329, f_332, f_343, f_347, f_348, f_350, f_352, f_355, f_356, f_361, f_362, f_3, f_8, f_11, f_13, f_15, f_16, f_17, f_18, f_25, f_29, f_36, f_40, f_42, f_45, f_48, f_55, f_60, f_64, f_66, f_67, f_68, f_72, f_73, f_79, f_85, f_86, f_88, f_89, f_98, f_102, f_103, f_104, f_105, f_112, f_116, f_119, f_122, f_125, f_137, f_144, f_165, f_166, f_171, f_173, f_176, f_177, f_180, f_183, f_187, f_204, f_205, f_208, f_211, f_216, f_219, f_227, f_228, f_230, f_237, f_239, f_240, f_245, f_247, f_252, f_257, f_260, f_262, f_273, f_276, f_280, f_286, f_295, f_296, f_304, f_305, f_311, f_313, f_315, f_324, f_326, f_327, f_338, f_340, f_341, f_342, f_344, f_346, f_353, f_357, f_363, f_365, f_5, f_6, f_9, f_14, f_19, f_26, f_28, f_31, f_43, f_50, f_56, f_59, f_61, f_62, f_71, f_75, f_77, f_78, f_84, f_90, f_94, f_95, f_96, f_100, f_101, f_107, f_108, f_109, f_113, f_115, f_117, f_124, f_129, f_134, f_136, f_138, f_141, f_150, f_153, f_156, f_158, f_160, f_162, f_167, f_189, f_192, f_198, f_199, f_200, f_202, f_209, f_210, f_217, f_223, f_231, f_233, f_235, f_250, f_251, f_253, f_255, f_263, f_264, f_265, f_266, f_267, f_269, f_270, f_272, f_287, f_292, f_299, f_301, f_302, f_303, f_319, f_321, f_323, f_328, f_330, f_331, f_333, f_336, f_337, f_339, f_345, f_349, f_354, f_359, f_360)"
     ]
    }
   ],
   "source": [
    "print(df_all.columns.slice(0,365).map(a => a.toString ).toSeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.feature.{MinMaxScaler, StandardScaler, Normalizer, MaxAbsScaler}\n",
      "import org.apache.spark.ml.Pipeline\n",
      "import org.apache.spark.ml.feature.VectorAssembler\n",
      "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_2abe75f13a82\n",
      "df_all_features: org.apache.spark.sql.DataFrame = [f_2: double, f_4: double ... 365 more fields]\n",
      "mms: org.apache.spark.ml.feature.MinMaxScaler = minMaxScal_0c85f2163fad\n",
      "mms_tr: mms.type = minMaxScal_0c85f2163fad\n",
      "trainingFeaturesPipeline: org.apache.spark.ml.Pipeline = pipeline_3a2a14187f74\n",
      "trainingFeaturesDF: org.apache.spark.sql.DataFrame = [f_2: double, f_4: double ... 366 more fields]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{MinMaxScaler,StandardScaler,Normalizer, MaxAbsScaler}\n",
    "import org.apache.spark.ml.{Pipeline}\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val assembler = new VectorAssembler().setInputCols(df_all.columns.slice(0,365).map(a => a.toString).toArray).setOutputCol(\"features\")\n",
    "\n",
    "val df_all_features = assembler.transform(df_all)\n",
    "\n",
    "// scaler instance with the min(-1) and max(1) \n",
    "val mms = new MinMaxScaler().\n",
    "    setInputCol(\"features\").\n",
    "    setOutputCol(\"mms\").\n",
    "    setMax(1).\n",
    "    setMin(-1)\n",
    "\n",
    "\n",
    "val mms_tr = mms.setInputCol(\"features\").setOutputCol(\"mms\")\n",
    "\n",
    "val trainingFeaturesPipeline = (new Pipeline()\n",
    "  .setStages(Array(mms_tr)))\n",
    "\n",
    "val trainingFeaturesDF = trainingFeaturesPipeline.fit(df_all_features).transform(df_all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.ml.linalg.DenseVector\n",
      "toArr: Any => Array[Double] = <function1>\n",
      "toArrUdf: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,ArrayType(DoubleType,false),None)\n",
      "final_df_all: org.apache.spark.sql.DataFrame = [target: float, mms: array<float>]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.DenseVector\n",
    "val toArr: Any => Array[Double] = _.asInstanceOf[DenseVector].toArray\n",
    "val toArrUdf = udf(toArr)\n",
    "\n",
    "val final_df_all = trainingFeaturesDF.select($\"target\",$\"mms\").\n",
    "             withColumn(\"target\",$\"target\".cast(FloatType)).\n",
    "             withColumn(\"mms\",toArrUdf($\"mms\").cast(ArrayType(FloatType)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "van: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [target: float, mms: array<float>]\n",
      "ben: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [target: float, mms: array<float>]\n"
     ]
    }
   ],
   "source": [
    "val van = final_df_all.where($\"target\"===1)\n",
    "val ben = final_df_all.where($\"target\"===0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benTrainDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [target: float, mms: array<float>]\n",
      "benEvalDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [target: float, mms: array<float>]\n"
     ]
    }
   ],
   "source": [
    "// Now that the data has been prepared, let's split the dataset into a training and test dataframe\n",
    "val Array(benTrainDF, benEvalDF) = final_df_all.randomSplit(Array(0.8, 0.02),seed = 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res16: Long = 58539\n"
     ]
    }
   ],
   "source": [
    "benTrainDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res17: Long = 1461\n"
     ]
    }
   ],
   "source": [
    "benEvalDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvalDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [target: float, mms: array<float>]\n"
     ]
    }
   ],
   "source": [
    "val EvalDF = benEvalDF.union(van)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res18: Long = 1024\n"
     ]
    }
   ],
   "source": [
    "EvalDF.where($\"target\"===1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res19: Long = 1437\n"
     ]
    }
   ],
   "source": [
    "EvalDF.where($\"target\"===0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ben_sim_ts_td_meta: com.logicalclocks.hsfs.TrainingDataset = com.logicalclocks.hsfs.TrainingDataset@198389d3\n"
     ]
    }
   ],
   "source": [
    "val ben_sim_ts_td_meta = (fs.createTrainingDataset()\n",
    "                .name(\"ben_td\")\n",
    "                .description(\"Ben simulated Time Series Training Dataset\")\n",
    "                .dataFormat(DataFormat.TFRECORD)     \n",
    "                .statisticsEnabled(false)          \n",
    "                .version(1)\n",
    "                .build())\n",
    "ben_sim_ts_td_meta.save(benTrainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_sim_ts_td_meta: com.logicalclocks.hsfs.TrainingDataset = com.logicalclocks.hsfs.TrainingDataset@77dd83a6\n"
     ]
    }
   ],
   "source": [
    "val eval_sim_ts_td_meta = (fs.createTrainingDataset()\n",
    "                .name(\"eval_td\")\n",
    "                .description(\"Simulated Time Series Training Dataset for evaluation\")\n",
    "                .dataFormat(DataFormat.TFRECORD)     \n",
    "                .statisticsEnabled(false)          \n",
    "                .version(1)\n",
    "                .build())\n",
    "eval_sim_ts_td_meta.save(EvalDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [target: float, mms: array<float>]\n",
      "test_sim_ts_td_meta: com.logicalclocks.hsfs.TrainingDataset = com.logicalclocks.hsfs.TrainingDataset@1f276e26\n"
     ]
    }
   ],
   "source": [
    "val testDf = EvalDF.where($\"target\"===1)\n",
    "\n",
    "val test_sim_ts_td_meta = (fs.createTrainingDataset()\n",
    "                .name(\"test_td\")\n",
    "                .description(\"Simulated Time Series Training Dataset for testing\")\n",
    "                .dataFormat(DataFormat.TFRECORD)     \n",
    "                .statisticsEnabled(false)          \n",
    "                .version(1)\n",
    "                .build())\n",
    "test_sim_ts_td_meta.save(testDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "scala",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
