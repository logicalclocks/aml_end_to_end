{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>76</td><td>application_1613059669625_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://resourcemanager.service.consul:8088/proxy/application_1613059669625_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://amlsim-worker-1.internal.cloudapp.net:8042/node/containerlogs/container_e12_1613059669625_0002_01_000001/amldemo__meb10179\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "<pyspark.sql.session.SparkSession object at 0x7f8fc2958ed0>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_generator(size=635,\n",
    "                          cycle_period=30.5,\n",
    "                          signal_type='sine',\n",
    "                          salary=1,\n",
    "                          trend=0.1,\n",
    "                          noise=0.1,\n",
    "                          offset=0,\n",
    "                          spike=0):\n",
    "    '''\n",
    "    Synthetic time series generator\n",
    "    Input : \n",
    "    Output: - ts : the generated time series\n",
    "    '''\n",
    "    \n",
    "    #x = np.linspace(-0.5*30.5*21, 0.5*30.5*21, 635)\n",
    "    #phase_random = np.random.uniform(-1,1)\n",
    "    #phi = np.pi\n",
    "    #s1 = np.sin(2*np.pi*(1./30.5)*x + 0  ) + 2\n",
    "    #s2 = np.sin(2*np.pi*(1./30.5)*x +phase_random*phi) + 2    \n",
    "    \n",
    "    # size : length of the time series\n",
    "    # cycle_period : period of the signal (usually 30.5, the month period, in days)\n",
    "    # count_periods : number of periods in the time series\n",
    "    # in size = 635, and cycle_period = 30.5, we have ~ 21 periods (20.8)\n",
    "    count_periods = size / cycle_period\n",
    "    \n",
    "    # 1. The trend making\n",
    "    t = np.linspace(-0.5*cycle_period*count_periods, 0.5*cycle_period*count_periods, size)\n",
    "    t_trend = np.linspace(0, 1, size)\n",
    "    trend = trend*salary*t_trend**2          \n",
    "  \n",
    "    # 2. The seasonality making\n",
    "    phase_random = np.random.uniform(-1,1)\n",
    "    if offset == 1: \n",
    "        phase = np.random.uniform(-1,1)*np.pi\n",
    "    else: \n",
    "        phase = 0           \n",
    "        \n",
    "    if signal_type == 'mixed'   : \n",
    "        choice = np.random.randint(4, size=1)\n",
    "        if choice == 0 : signal_type = 'sine'\n",
    "        if choice == 1 : signal_type = 'sawtooth'\n",
    "        if choice == 2 : signal_type = 'triangle'\n",
    "        if choice == 3 : signal_type = 'square'\n",
    "    if signal_type == 'sine':     ts = 0.25*salary*np.sin(2*np.pi*(1./cycle_period)*t + phase)    \n",
    "    if signal_type == 'sawtooth': ts = -0.25*salary*signal.sawtooth(2*np.pi*(1./cycle_period)*t + phase)\n",
    "    if signal_type == 'triangle': ts = 0.5*salary*np.abs(signal.sawtooth(2*np.pi*(1./cycle_period)*t + phase))-1\n",
    "    if signal_type == 'square':   ts = 0.25*salary*signal.square(2*np.pi*(1./cycle_period)*t + phase)\n",
    "           \n",
    "    # 3. The noise making\n",
    "    noise = np.random.normal(0,noise*salary,size)  \n",
    "            \n",
    "    ts = ts + trend + noise\n",
    "            \n",
    "    # 4. Adding spikes to the time series\n",
    "    if spike > 0: \n",
    "        for spike_i in range(spike):\n",
    "            sign = random.choice([-1,1])\n",
    "            t_spike = np.random.randint(0,455) #size)\n",
    "            ts[t_spike:] = ts[t_spike:] + sign * np.random.normal(3*salary,salary)\n",
    "                \n",
    "    return ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = time_series_generator()\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateOutlierGenerator:\n",
    "    def __init__(self, timestamps):\n",
    "        self.timestamps = timestamps\n",
    "\n",
    "    def add_outliers(self, timeseries):\n",
    "        return NotImplementedError\n",
    "    \n",
    "class MultivariateExtremeOutlierGenerator(MultivariateOutlierGenerator):\n",
    "    def __init__(self, timestamps=None, factor=8):\n",
    "        self.timestamps = [] if timestamps is None else list(sum(timestamps, ()))\n",
    "        self.factor = factor\n",
    "\n",
    "    def get_value(self, current_timestamp, timeseries):\n",
    "        if current_timestamp in self.timestamps:\n",
    "            local_std = timeseries.iloc[max(0, current_timestamp - 10):current_timestamp + 10].std()\n",
    "            return np.random.choice([-1, 1]) * self.factor * local_std\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def add_outliers(self, timeseries):\n",
    "        additional_values = []\n",
    "        for timestamp_index in range(len(timeseries)):\n",
    "            additional_values.append(self.get_value(timestamp_index, timeseries))\n",
    "        return additional_values\n",
    "    \n",
    "\n",
    "class MultivariateShiftOutlierGenerator(MultivariateOutlierGenerator):\n",
    "    def __init__(self, timestamps=None, factor=8):\n",
    "        timestamps = timestamps or []\n",
    "        self.timestamps = timestamps\n",
    "        self.factor = factor\n",
    "\n",
    "    def add_outliers(self, timeseries):\n",
    "        additional_values = np.zeros(timeseries.size)\n",
    "        for start, end in self.timestamps:\n",
    "            local_std = timeseries.iloc[max(0, start - 10):end + 10].std()\n",
    "            additional_values[list(range(start, end))] += np.random.choice([-1, 1]) * self.factor * local_std\n",
    "        return additional_values\n",
    "    \n",
    "    \n",
    "class MultivariateTrendOutlierGenerator(MultivariateOutlierGenerator):\n",
    "    def __init__(self, timestamps=None, factor=8):\n",
    "        self.timestamps = timestamps or []\n",
    "        self.factor = factor / 10  # Adjust for trend\n",
    "\n",
    "    def add_outliers(self, timeseries):\n",
    "        additional_values = np.zeros(timeseries.size)\n",
    "        for start, end in self.timestamps:\n",
    "            slope = np.random.choice([-1, 1]) * self.factor * np.arange(end - start)\n",
    "            additional_values[list(range(start, end))] += slope\n",
    "            additional_values[end:] += slope[-1]\n",
    "        return additional_values\n",
    "\n",
    "class MultivariateVarianceOutlierGenerator(MultivariateOutlierGenerator):\n",
    "    def __init__(self, timestamps=None, factor=8):\n",
    "        self.timestamps = timestamps or []\n",
    "        self.factor = factor\n",
    "\n",
    "    def add_outliers(self, timeseries):\n",
    "        additional_values = np.zeros(timeseries.size)\n",
    "        for start, end in self.timestamps:\n",
    "            difference = np.diff(timeseries[start-1:end]) if start > 0 \\\n",
    "                         else np.insert(np.diff(timeseries[start:end]), 0, 0)\n",
    "            additional_values[list(range(start, end))] += (self.factor - 1) * difference\n",
    "        return additional_values\n",
    "\n",
    "\n",
    "INITIAL_VALUE_MIN = 0\n",
    "INITIAL_VALUE_MAX = 1\n",
    "\n",
    "\n",
    "class MultivariateDataGenerator:\n",
    "    def __init__(self, stream_length, n, k, shift_config=None, behavior=None, behavior_config=None):\n",
    "        \"\"\"Create multivariate time series using outlier generators\n",
    "        :param stream_length: number of values in each time series\n",
    "        :param n: number of time series at all\n",
    "        :param k: number of time series that should correlate. If all should correlate with the first\n",
    "        one, set k=n.\n",
    "        :param shift_config: dictionary from index of the time series to how much it should be displaced in time (>=0)\n",
    "        \"\"\"\n",
    "\n",
    "        if not shift_config:\n",
    "            self.shift_config = {}\n",
    "            self.max_shift = 0\n",
    "        else:\n",
    "            self.shift_config = shift_config\n",
    "            self.max_shift = max(list(self.shift_config.values()))\n",
    "        self.behavior = behavior\n",
    "        self.behavior_config = behavior_config if behavior_config is not None else {}\n",
    "\n",
    "        self.STREAM_LENGTH = stream_length\n",
    "        self.N = n\n",
    "        self.K = k\n",
    "        self.data = pd.DataFrame()\n",
    "        self.outlier_data = pd.DataFrame()\n",
    "\n",
    "        assert self.STREAM_LENGTH > 0, 'stream_length must at least be 1'\n",
    "        assert self.N > 0, 'n must at least be 1'\n",
    "        assert self.K >= 0, 'k must at least be 0'\n",
    "        assert self.K <= self.N, 'k must be less than or equal to n'\n",
    "        assert 0 not in self.shift_config.keys(), 'The origin time series cannot be shifted in time'\n",
    "\n",
    "        if k == 0:  # There is no difference between k=0 and k=1.\n",
    "            self.K = 1\n",
    "\n",
    "    def generate_baseline(self, correlation_min=0.9, correlation_max=0.7, initial_value_min=INITIAL_VALUE_MIN,\n",
    "                          initial_value_max=INITIAL_VALUE_MAX):\n",
    "        \"\"\"\n",
    "        Generate the multivariate data frame\n",
    "        :param correlation_min: how much the k columns should at least correlate with the first one\n",
    "        :param correlation_max: how much the n-k-1 columns should at max correlate with the first one\n",
    "        :param initial_value_min: minimal possible value of the first entry of the time series\n",
    "        :param initial_value_max: maximal possible value of the first entry of the time series\n",
    "        :return: a DataFrame with columns ['timestamp', 'x0', ... 'xn-1'] in which the first column\n",
    "        is the original time series. The following k columns correlate at least with correlation_min\n",
    "        with it and the remaining n-k columns correlate at max correlation_max with it.\n",
    "        \"\"\"\n",
    "\n",
    "        df = self.init_dataframe(self.N)\n",
    "        # Create k correlating time series\n",
    "        df = self.create_correlating_time_series(self.K, correlation_min, df, initial_value_min, initial_value_max)\n",
    "\n",
    "        # Create the remaining n - k time series randomly\n",
    "        df = self.create_not_correlating_time_series(self.K, self.N, correlation_max, df, initial_value_min,\n",
    "                                                     initial_value_max)\n",
    "\n",
    "        # Perform the shifts: currently all time series have n+max_shift elements\n",
    "        # Each one should start at index max_shift - own_shift such that the padded measurements of a time series before\n",
    "        # the origin time series starts descend from a self-correlating distribution\n",
    "        for k, column_name in enumerate(df.columns):\n",
    "            own_shift = 0 if k not in self.shift_config.keys() else self.shift_config[k]\n",
    "            df[column_name] = df[column_name].shift(own_shift)\n",
    "\n",
    "        df.dropna(axis=0, inplace=True)\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        assert not df.isnull().values.any(), 'There is at least one NaN in the generated DataFrame'\n",
    "        self.data = df\n",
    "        return self.data\n",
    "\n",
    "    def init_dataframe(self, number_time_series):\n",
    "        columns = ['timestamp']\n",
    "        for value_column_index in range(number_time_series):\n",
    "            columns.append('x{}'.format(value_column_index))\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "        return df\n",
    "\n",
    "    def create_basic_time_series(self, df, initial_value_min=INITIAL_VALUE_MIN, initial_value_max=INITIAL_VALUE_MAX):\n",
    "        if initial_value_min != initial_value_max:\n",
    "            start = np.random.randint(initial_value_min, initial_value_max)\n",
    "        else:\n",
    "            start = initial_value_min\n",
    "\n",
    "        if self.behavior is not None:\n",
    "            behavior_generator = self.behavior(**self.behavior_config)\n",
    "\n",
    "        # Create basic time series\n",
    "        x = [start]\n",
    "        timestamps = [0]\n",
    "        for i in range(1, self.STREAM_LENGTH + self.max_shift):\n",
    "            timestamps.append(i)\n",
    "            value = x[i - 1] + np.random.normal(0, 1)\n",
    "            if self.behavior is not None:\n",
    "                value += next(behavior_generator)\n",
    "            x.append(value)\n",
    "        df['x0'] = x\n",
    "        df['timestamp'] = timestamps\n",
    "        df.set_index('timestamp', inplace=True)\n",
    "        return df\n",
    "\n",
    "    def create_correlating_time_series(self, number_time_series, correlation_min, df,\n",
    "                                       initial_value_min=INITIAL_VALUE_MIN,\n",
    "                                       initial_value_max=INITIAL_VALUE_MAX):\n",
    "        # First time series\n",
    "        df = self.create_basic_time_series(df=df, initial_value_min=initial_value_min,\n",
    "                                           initial_value_max=initial_value_max)\n",
    "        origin_offset = df.iloc[0, 0]\n",
    "\n",
    "        # number_time_series time series which are correlating\n",
    "        for index_correlating in range(1, number_time_series):\n",
    "            while True:\n",
    "                x = [0]\n",
    "                if initial_value_min != initial_value_max:\n",
    "                    offset = np.random.randint(initial_value_min, initial_value_max)\n",
    "                else:\n",
    "                    offset = initial_value_min\n",
    "                for index_timeseries_length in range(self.STREAM_LENGTH - 1 + self.max_shift):\n",
    "                    # Take 50% of time series 0 and add 50% randomness\n",
    "                    original_value = df.iloc[index_timeseries_length, 0] - origin_offset\n",
    "                    x.append(0.5 * original_value + 0.5 * (np.random.random() - 0.5))\n",
    "                df['x' + str(index_correlating)] = x\n",
    "                df['x' + str(index_correlating)] += offset\n",
    "                if abs(df.corr().iloc[0, index_correlating]) >= correlation_min:\n",
    "                    break\n",
    "            assert (len(df) == self.STREAM_LENGTH + self.max_shift)\n",
    "        return df\n",
    "\n",
    "    def create_not_correlating_time_series(self, k, n, correlation_max, df, initial_value_min=INITIAL_VALUE_MIN,\n",
    "                                           initial_value_max=INITIAL_VALUE_MAX):\n",
    "        for index_not_correlation in range(k, n):\n",
    "            if self.behavior is not None:\n",
    "                behavior_generator = self.behavior(**self.behavior_config)\n",
    "            while True:\n",
    "                if initial_value_min != initial_value_max:\n",
    "                    x = [np.random.randint(initial_value_min, initial_value_max)]\n",
    "                else:\n",
    "                    x = [initial_value_min]\n",
    "                for index_timeseries_length in range(self.STREAM_LENGTH - 1 + self.max_shift):\n",
    "                    value = x[index_timeseries_length] + (np.random.random() - 0.5)\n",
    "                    if self.behavior is not None:\n",
    "                        value += next(behavior_generator)\n",
    "                    x.append(value)\n",
    "                df['x' + str(index_not_correlation)] = x\n",
    "                if abs(df.corr().iloc[0, index_not_correlation]) <= correlation_max:\n",
    "                    break\n",
    "            assert(len(df) == self.STREAM_LENGTH + self.max_shift)\n",
    "        return df\n",
    "\n",
    "    def add_outliers(self, config):\n",
    "        \"\"\"Adds outliers based on the given configuration to the base line\n",
    "         :param config: Configuration file for the outlier addition e.g.\n",
    "         {'extreme': [{'n': 0, 'timestamps': [(3,)]}],\n",
    "          'shift':   [{'n': 3, 'timestamps': [(4,10)]}]}\n",
    "          would add an extreme outlier to time series 0 at timestamp 3 and a base shift\n",
    "          to time series 3 between timestamps 4 and 10\n",
    "         :return:\n",
    "         \"\"\"\n",
    "        OUTLIER_GENERATORS = {'extreme': MultivariateExtremeOutlierGenerator,\n",
    "                              'shift': MultivariateShiftOutlierGenerator,\n",
    "                              'trend': MultivariateTrendOutlierGenerator,\n",
    "                              'variance': MultivariateVarianceOutlierGenerator}\n",
    "\n",
    "        generator_keys = []\n",
    "\n",
    "        # Validate the input\n",
    "        for outlier_key, outlier_generator_config in config.items():\n",
    "            assert outlier_key in OUTLIER_GENERATORS, 'outlier_key must be one of {} but was'.format(OUTLIER_GENERATORS,\n",
    "                                                                                                     outlier_key)\n",
    "            generator_keys.append(outlier_key)\n",
    "            for outlier_timeseries_config in outlier_generator_config:\n",
    "                n, timestamps = outlier_timeseries_config['n'], outlier_timeseries_config['timestamps']\n",
    "                assert n in range(self.N), 'n must be between 0 and {} but was {}'.format(self.N - 1, n)\n",
    "                for timestamp in list(sum(timestamps, ())):\n",
    "                    assert timestamp in range(\n",
    "                        self.STREAM_LENGTH), 'timestamp must be between 0 and {} but was {}'.format(self.STREAM_LENGTH,\n",
    "                                                                                                    timestamp)\n",
    "\n",
    "        df = self.data\n",
    "        if self.data.shape == (0, 0):\n",
    "            raise Exception('You have to first compute a base line by invoking generate_baseline()')\n",
    "        for generator_key in generator_keys:\n",
    "            for outlier_timeseries_config in config[generator_key]:\n",
    "                n, timestamps = outlier_timeseries_config['n'], outlier_timeseries_config['timestamps']\n",
    "                generator_args = dict([(k, v) for k, v in outlier_timeseries_config.items() if k not in ['n', 'timestamps']])\n",
    "                generator = OUTLIER_GENERATORS[generator_key](timestamps=timestamps, **generator_args)\n",
    "                df[df.columns[n]] += generator.add_outliers(self.data[self.data.columns[n]])\n",
    "\n",
    "        assert not df.isnull().values.any(), 'There is at least one NaN in the generated DataFrame'\n",
    "        self.outlier_data = df\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_var_ben_ano_gen(time_steps, sample_size):\n",
    "    baseline = MultivariateDataGenerator(stream_length=time_steps, n=sample_size, k=0)    \n",
    "    baseline.generate_baseline()\n",
    "    # would add an extreme outlier to time series 0 at timestamp 3 and a base shift\n",
    "    #           to time series 3 between timestamps 4 and 10    \n",
    "    \n",
    "    shift_start_index =  np.random.randint(time_steps - int(time_steps/4), size=1)[0]\n",
    "    shift_end_index =  np.random.randint(shift_start_index+1, shift_start_index + int(int(time_steps/4)/2), size=1)[0]\n",
    "    \n",
    "    int_to_ano_type = {1: 'extreme',\n",
    "                       2: 'shift',\n",
    "                       3: 'variance',\n",
    "                       4: 'trend'}\n",
    "\n",
    "    ano_type_key = np.random.randint(1 , 4, size=1)[0]\n",
    "    \n",
    "    ano_data = baseline.add_outliers(\n",
    "                        {\n",
    "                        int_to_ano_type[ano_type_key]:   [{'n': 0, 'timestamps': [(shift_start_index,shift_end_index)]}]}\n",
    "                )\n",
    "\n",
    "                    \n",
    "                            \n",
    "    normal_baseline = MultivariateDataGenerator(stream_length=time_steps, n=sample_size, k=0)    \n",
    "    normal_data = normal_baseline.generate_baseline()\n",
    "    return normal_data.transpose().values, ano_data.transpose().values\n",
    "#    return normal_data.transpose().values.reshape([-1,time_steps,1]), ano_data.transpose().values.reshape([-1,time_steps,1])\n",
    "\n",
    "def generate_time_series2(time_steps,normal_sample_size,ano_sample_size):\n",
    "    x, _ = multi_var_ben_ano_gen(time_steps = time_steps, sample_size = 1)\n",
    "    y = [0]\n",
    "    \n",
    "    for i in range(normal_sample_size-1):\n",
    "        x2, _ = multi_var_ben_ano_gen(time_steps = time_steps, sample_size = 1)\n",
    "        x = np.concatenate((x,x2),axis = 0)\n",
    "        y = np.concatenate((y,[0]),axis = 0)\n",
    "\n",
    "    for i in range(ano_sample_size):\n",
    "        _, x2 = multi_var_ben_ano_gen(time_steps = time_steps, sample_size = 1)\n",
    "        x = np.concatenate((x,x2),axis = 0)\n",
    "        y = np.concatenate((y,[1]),axis = 0)\n",
    "        \n",
    "    return x.astype(np.float32), y.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started copying local path test.csv to hdfs path hdfs://rpc.namenode.service.consul:8020/Projects/amldemo/Logs//test.csv\n",
      "\n",
      "Finished copying\n",
      "\n",
      "Started copying local path labels.csv to hdfs path hdfs://rpc.namenode.service.consul:8020/Projects/amldemo/Logs//labels.csv\n",
      "\n",
      "Finished copying"
     ]
    }
   ],
   "source": [
    "from hops import hdfs\n",
    "x,y = generate_time_series2(time_steps=365,normal_sample_size=1,ano_sample_size=1)\n",
    "x.shape\n",
    "\n",
    "np.savetxt(\"test.csv\", x, delimiter=\",\") #Keras can't save to HDFS in current version so save to local first\n",
    "hdfs.copy_to_hdfs(\"test.csv\", hdfs.project_path() + \"Logs/\", overwrite=True) #copy from local fs to hdfs\n",
    "\n",
    "np.savetxt(\"labels.csv\", y, delimiter=\",\") #Keras can't save to HDFS in current version so save to local first\n",
    "hdfs.copy_to_hdfs(\"labels.csv\", hdfs.project_path() + \"Logs/\", overwrite=True) #copy from local fs to hdfs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%local\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "from hops import hdfs\n",
    "from numpy import genfromtxt\n",
    "path = hdfs.project_path() + \"Logs/test.csv\"\n",
    "local_dir = hdfs.localize(path)\n",
    "#ts_all_plot = pd.read_csv(local_dir + 'test.csv')\n",
    "my_data = genfromtxt('test.csv', delimiter=',')\n",
    "my_data = my_data.transpose()\n",
    "print(my_data.shape)\n",
    "\n",
    "l_path = hdfs.project_path() + \"Logs/labels.csv\"\n",
    "l_local_dir = hdfs.localize(l_path)\n",
    "labels = genfromtxt('labels.csv', delimiter=',')\n",
    "\n",
    "cdict = {1.0: 'red', 0.0: 'blue'}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(50,20))\n",
    "for g in np.unique(labels):\n",
    "    ix = np.where(labels == g)\n",
    "    ax.plot(my_data[:, ix[0]], c = cdict[g]) #, label = g\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_var_ano_gen(time_steps, sample_size):\n",
    "    time_steps = time_steps + 1\n",
    "    baseline = MultivariateDataGenerator(stream_length=time_steps, n=sample_size, k=0)    \n",
    "    baseline.generate_baseline()\n",
    "    # would add an extreme outlier to time series 0 at timestamp 3 and a base shift\n",
    "    #           to time series 3 between timestamps 4 and 10    \n",
    "    \n",
    "    shift_start_index =  np.random.randint(time_steps - int(time_steps/4), size=1)[0]\n",
    "    shift_end_index =  np.random.randint(shift_start_index+1, shift_start_index + int(int(time_steps/4)/2), size=1)[0]\n",
    "    \n",
    "    int_to_ano_type = {1: 'extreme',\n",
    "                       2: 'shift',\n",
    "                       3: 'variance',\n",
    "                       4: 'trend'}\n",
    "\n",
    "    ano_type_key = np.random.randint(1 , 4, size=1)[0]\n",
    "    \n",
    "    ano_ts = np.random.randint(low=0, high=sample_size,size=1)[0]\n",
    "    \n",
    "    ano_data = baseline.add_outliers(\n",
    "                        {\n",
    "                            int_to_ano_type[ano_type_key]:   [{'n': ano_ts, 'timestamps': [(shift_start_index,shift_end_index)]}]\n",
    "                        }\n",
    "                )\n",
    "\n",
    "                    \n",
    "    return [float(val) for val in list(ano_data.transpose().values[0])][1:time_steps] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_var_ben_gen(time_steps, sample_size):\n",
    "    time_steps = time_steps + 1\n",
    "    baseline = MultivariateDataGenerator(stream_length=time_steps, n=sample_size, k=0)    \n",
    "    ben_data = baseline.generate_baseline()\n",
    "                    \n",
    "    return [float(val) for val in list(ben_data.transpose().values[0])][1:time_steps] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24"
     ]
    }
   ],
   "source": [
    "x = multi_var_ano_gen(time_steps=24, sample_size=1)\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "ano_parallel_instances = sc.parallelize(range(1000))\n",
    "ano_rdd = ano_parallel_instances.map(lambda row: multi_var_ano_gen(time_steps=365, sample_size=1))\n",
    "ano_data = ano_rdd.toDF().withColumn(\"target\",lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ben_parallel_instances = sc.parallelize(range(59000))\n",
    "ben_rdd = ben_parallel_instances.map(lambda row: multi_var_ben_gen(time_steps=365, sample_size=1))\n",
    "ben_data = ben_rdd.toDF().withColumn(\"target\",lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ano_data.union(ben_data)\n",
    "feature_names = data.columns\n",
    "feature_names = [\"f\" + s  for s in feature_names]\n",
    "data = data.toDF(*feature_names)\n",
    "data = data.withColumnRenamed(\"ftarget\",\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.feature_group.FeatureGroup object at 0x7f8f901198d0>"
     ]
    }
   ],
   "source": [
    "simts_fg_meta = fs.create_feature_group(name=\"simts_features\",\n",
    "                                       version=1,\n",
    "                                       time_travel_format=None,                                        \n",
    "                                       statistics_config=False)\n",
    "simts_fg_meta.save(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
