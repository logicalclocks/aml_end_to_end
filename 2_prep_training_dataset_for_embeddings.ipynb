{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training dataset\n",
    "In this notebook We are going to create training datasets and register to Hopsworks Feature Store. This training dataset will be later used to train for graph emmbedings model.\n",
    "![Training Dataset](./images/create_training_dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>11</td><td>application_1631881319280_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://resourcemanager.service.consul:8089/proxy/application_1631881319280_0003/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://ip-10-0-0-216.us-west-2.compute.internal:8044/node/containerlogs/container_e03_1631881319280_0003_01_000001/amlsim__meb10180\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "<pyspark.sql.session.SparkSession object at 0x7f8b01323e10>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "import hsfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate a connection and get the project feature store handler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get transactions feature group handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------+-------+--------+--------------+\n",
      "|  source|  target|tran_id|tx_type|base_amt|tran_timestamp|\n",
      "+--------+--------+-------+-------+--------+--------------+\n",
      "|35d9e14d|ea20f3ea| 328889|      4|  341.23|        Aug-18|\n",
      "|ecaba057|42e760b1| 328890|      4|  846.74|        Aug-18|\n",
      "|43e1ce2b|d85de509| 328891|      4|  269.45|        Aug-18|\n",
      "|2d76caaa|9dff007b| 328892|      4|  180.32|        Aug-18|\n",
      "|b2efe3c6|92cf8d33| 328893|      4|  507.09|        Aug-18|\n",
      "+--------+--------+-------+-------+--------+--------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "transactions_fg = fs.get_feature_group(\"transactions_fg\", 1)\n",
    "transactions_fg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438386"
     ]
    }
   ],
   "source": [
    "transactions_fg.read().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load alert transactions feature group  handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-------+--------------+\n",
      "|alert_id|is_sar|tran_id|    alert_type|\n",
      "+--------+------+-------+--------------+\n",
      "|      32|     1|1000145|scatter_gather|\n",
      "|      18|     1|1002861|scatter_gather|\n",
      "|      34|     1| 100431|scatter_gather|\n",
      "|      18|     1|1006056|scatter_gather|\n",
      "|      32|     1|1006362|scatter_gather|\n",
      "+--------+------+-------+--------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "alert_transactions_fg = fs.get_feature_group(\"alert_transactions_fg\", 1)\n",
    "alert_transactions_fg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "915"
     ]
    }
   ],
   "source": [
    "alert_transactions_fg.read().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load party feature group  handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+\n",
      "|      id|type|\n",
      "+--------+----+\n",
      "|0016359b|   0|\n",
      "|0019b8d0|   0|\n",
      "|001dcc27|   1|\n",
      "|00298665|   1|\n",
      "|003cd8f3|   0|\n",
      "|003e2533|   0|\n",
      "|00403fbd|   1|\n",
      "|00498ec2|   1|\n",
      "|0049ee5b|   0|\n",
      "|0054a022|   0|\n",
      "|00575ac9|   0|\n",
      "|005c0c19|   1|\n",
      "|006ac170|   1|\n",
      "|006cc052|   0|\n",
      "|0075d230|   1|\n",
      "|007749eb|   0|\n",
      "|00794932|   1|\n",
      "|007f2674|   0|\n",
      "|007f76dc|   1|\n",
      "|0081b086|   0|\n",
      "+--------+----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "party_fg = fs.get_feature_group(\"party_fg\", 1)\n",
    "party_fg.read().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training datasets\n",
    "To create training datasest we will use hsfs `Query` object. Training dataset's metadata, created from hsfs `Query` object, contains information such as: \n",
    "* which feature groups it was created from; \n",
    "* commit id of these feature froups;\n",
    "* the order of features. \n",
    "\n",
    "This will give us possibility to \n",
    "* track back and see which feature were used to create this training; \n",
    "* perform time-travel and see how features looked like when this training dataset was created ;\n",
    "* reconstruct feature order during model inferencing.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create graph edge training datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = transactions_fg.select([\"source\",\"target\",\"tran_id\",\"tx_type\",\"base_amt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------+-------+--------+\n",
      "|  source|  target|tran_id|tx_type|base_amt|\n",
      "+--------+--------+-------+-------+--------+\n",
      "|35d9e14d|ea20f3ea| 328889|      4|  341.23|\n",
      "|ecaba057|42e760b1| 328890|      4|  846.74|\n",
      "|43e1ce2b|d85de509| 328891|      4|  269.45|\n",
      "|2d76caaa|9dff007b| 328892|      4|  180.32|\n",
      "|b2efe3c6|92cf8d33| 328893|      4|  507.09|\n",
      "|94405fbd|5033b863| 328899|      4|   404.6|\n",
      "|ecc461e1|7b0a5b49| 328900|      4|  903.96|\n",
      "|0c535264|c316b5f7| 328902|      4|  805.33|\n",
      "|c32b2ef9|0ba25b05| 328903|      4|  263.29|\n",
      "|896eaf11|cd7dbc4f| 328905|      4|  220.48|\n",
      "|35ca5309|fafb6f18| 328907|      4|  981.35|\n",
      "|c6d05e8e|8dd827b1| 328909|      4|  141.77|\n",
      "|40c671aa|faa1e4dc| 328910|      4|  674.86|\n",
      "|d3adb450|fbc5ada0| 328919|      4|  351.92|\n",
      "|65ec7365|7b870ef2| 328925|      4|  463.31|\n",
      "|9405082c|d9768d01| 328927|      4|  647.38|\n",
      "|5a99160f|a87a051f| 328929|      4|  320.18|\n",
      "|ff26d5c9|3858d176| 328933|      4|  351.77|\n",
      "|7e90e454|29342e2b| 328935|      4|  830.61|\n",
      "|397d6734|d44a5b84| 328936|      4|  776.21|\n",
      "+--------+--------+-------+-------+--------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "edges.read().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438386"
     ]
    }
   ],
   "source": [
    "edges.read().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_td_meta = fs.create_training_dataset(name=\"edges_td\",\n",
    "                                           version=1,\n",
    "                                           data_format=\"csv\",\n",
    "                                           description=\"edges training dataset\",\n",
    "                                           coalesce=True,\n",
    "                                           statistics_config={\"enabled\": True, \"histograms\": True, \"correlations\": False}\n",
    "                                          )\n",
    "edges_td_meta.save(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create graph node training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+\n",
      "|      id|type|\n",
      "+--------+----+\n",
      "|0016359b|   0|\n",
      "|0019b8d0|   0|\n",
      "|001dcc27|   1|\n",
      "|00298665|   1|\n",
      "|003cd8f3|   0|\n",
      "|003e2533|   0|\n",
      "|00403fbd|   1|\n",
      "|00498ec2|   1|\n",
      "|0049ee5b|   0|\n",
      "|0054a022|   0|\n",
      "|00575ac9|   0|\n",
      "|005c0c19|   1|\n",
      "|006ac170|   1|\n",
      "|006cc052|   0|\n",
      "|0075d230|   1|\n",
      "|007749eb|   0|\n",
      "|00794932|   1|\n",
      "|007f2674|   0|\n",
      "|007f76dc|   1|\n",
      "|0081b086|   0|\n",
      "+--------+----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "nodes = party_fg.select_all()\n",
    "nodes.read().show()\n",
    "\n",
    "node_td_meta = fs.create_training_dataset(name=\"node_td\",\n",
    "                                          version=1,\n",
    "                                          data_format=\"csv\",   \n",
    "                                          description=\"node training dataset\",\n",
    "                                          statistics_config={\"enabled\": True, \"histograms\": True, \"correlations\": False},\n",
    "                                          coalesce=True)\n",
    "node_td_meta.save(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create derived feature group `alert_nodes_fg`, whether nodes were part of previously known money laundering scheme or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|      id|is_sar|\n",
      "+--------+------+\n",
      "|33a8ff5b|     1|\n",
      "|43e028ef|     1|\n",
      "|fcf3bbf3|     1|\n",
      "|8b9017b8|     1|\n",
      "|9c187eed|     1|\n",
      "|65636b63|     1|\n",
      "|68c0230d|     1|\n",
      "|550a25ff|     1|\n",
      "|d73e5230|     1|\n",
      "|c0be245b|     1|\n",
      "|cdbd2ed5|     1|\n",
      "|963b978f|     1|\n",
      "|84563a83|     1|\n",
      "|da77c74b|     1|\n",
      "|840701de|     1|\n",
      "|dc37f73b|     1|\n",
      "|b0f4351c|     1|\n",
      "|dd2ebcf1|     1|\n",
      "|c29d75dc|     1|\n",
      "|d7c99aa5|     1|\n",
      "+--------+------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "alert_edges = transactions_fg.select([\"source\",\"target\",\"tran_id\",\"tx_type\",\"base_amt\"]).join(alert_transactions_fg.select([\"is_sar\"]),[\"tran_id\"],\"left\")\n",
    "alert_edges = alert_edges.read().where(F.col(\"is_sar\")==1)\n",
    "alert_sources = alert_edges.select([\"source\"]).toDF(\"id\")\n",
    "alert_targets = alert_edges.select([\"target\"]).toDF(\"id\")\n",
    "alert_nodes = alert_sources.union(alert_targets).dropDuplicates(subset=[\"id\"])\n",
    "alert_nodes = alert_nodes.withColumn(\"is_sar\",F.lit(1))\n",
    "alert_nodes.cache()\n",
    "alert_nodes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|      id|is_sar|\n",
      "+--------+------+\n",
      "|33a8ff5b|     1|\n",
      "|43e028ef|     1|\n",
      "|fcf3bbf3|     1|\n",
      "|8b9017b8|     1|\n",
      "|9c187eed|     1|\n",
      "|65636b63|     1|\n",
      "|68c0230d|     1|\n",
      "|550a25ff|     1|\n",
      "|d73e5230|     1|\n",
      "|c0be245b|     1|\n",
      "|cdbd2ed5|     1|\n",
      "|963b978f|     1|\n",
      "|84563a83|     1|\n",
      "|da77c74b|     1|\n",
      "|840701de|     1|\n",
      "|dc37f73b|     1|\n",
      "|b0f4351c|     1|\n",
      "|dd2ebcf1|     1|\n",
      "|c29d75dc|     1|\n",
      "|d7c99aa5|     1|\n",
      "+--------+------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "alert_sources = alert_edges.select([\"source\"]).toDF(\"id\")\n",
    "alert_targets = alert_edges.select([\"target\"]).toDF(\"id\")\n",
    "alert_nodes = alert_sources.union(alert_targets).dropDuplicates(subset=[\"id\"])\n",
    "alert_nodes = alert_nodes.withColumn(\"is_sar\",F.lit(1))\n",
    "alert_nodes.cache()\n",
    "alert_nodes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+------+\n",
      "|      id|type|is_sar|\n",
      "+--------+----+------+\n",
      "|0016359b|   0|     0|\n",
      "|0019b8d0|   0|     0|\n",
      "|001dcc27|   1|     0|\n",
      "|00298665|   1|     0|\n",
      "|003cd8f3|   0|     0|\n",
      "|003e2533|   0|     0|\n",
      "|00403fbd|   1|     0|\n",
      "|00498ec2|   1|     0|\n",
      "|0049ee5b|   0|     0|\n",
      "|0054a022|   0|     0|\n",
      "|00575ac9|   0|     0|\n",
      "|005c0c19|   1|     0|\n",
      "|006ac170|   1|     0|\n",
      "|006cc052|   0|     0|\n",
      "|0075d230|   1|     0|\n",
      "|007749eb|   0|     1|\n",
      "|00794932|   1|     0|\n",
      "|007f2674|   0|     1|\n",
      "|007f76dc|   1|     0|\n",
      "|0081b086|   0|     0|\n",
      "+--------+----+------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "alert_nodes_df = nodes.read().join(alert_nodes,[\"id\"], \"left\").withColumn(\"is_sar\",F.when(F.col(\"is_sar\") == 1, F.col(\"is_sar\")).otherwise(0))\n",
    "alert_nodes_df.cache()\n",
    "alert_nodes_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "816"
     ]
    }
   ],
   "source": [
    "alert_nodes_df.where(F.col(\"is_sar\") == 1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6531"
     ]
    }
   ],
   "source": [
    "alert_nodes_df.where(F.col(\"is_sar\") == 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to use hudi options that is suited to the size of our dataset \n",
    "extra_hudi_options = {\n",
    "    \"hoodie.bulkinsert.shuffle.parallelism\":\"1\", \n",
    "    \"hoodie.insert.shuffle.parallelism\":\"1\", \n",
    "    \"hoodie.upsert.shuffle.parallelism\":\"1\",\n",
    "    \"hoodie.parquet.compression.ratio\":\"0.5\"\n",
    "}\n",
    "\n",
    "alert_nodes_fg = fs.create_feature_group(name=\"alert_nodes_fg\",\n",
    "                                       version=1,\n",
    "                                       primary_key=[\"id\"],\n",
    "                                       description=\"node embeddings from transactions, derived fg\",\n",
    "                                       time_travel_format=\"HUDI\",     \n",
    "                                       online_enabled=True,                                                \n",
    "                                       statistics_config=False)\n",
    "\n",
    "alert_nodes_fg.save(alert_nodes_df, extra_hudi_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training datasets exploration from the user interface\n",
    "\n",
    "##### Hopsworks provides user interface that enables to discover and explore avaibale Training datasets and related features. Bellow screen shot demonstrates how one can preview list of available features in `edges_td` and get basic information such us identify feature types and which one is as a label.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Incremental Feature Engineering](./images/td_features.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One of the important steps of training dataset exploration is discover distribution of ist features. Since we enabled statistics to be computed durring training dataset creation we can easily preview descriptive statitsics. If training dataset has splits we can also preview statitics for each split separately and campare distributions to make sure that train and test splits have similar distibutions.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Incremental Feature Engineering](./images/td_stats.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hopsworks UI also give access to training dataset activity timeline metadata.  \n",
    "![Incremental Feature Engineering](./images/td_activity.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}