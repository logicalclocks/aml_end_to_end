{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create node embeddings feature groups.\n",
    "\n",
    "Up until now we use feature engineering, feature store and model training to create node embedding. We will now materialise this as node embeddings feature group. This feature group will be used to train anomaly detection model.\n",
    "\n",
    "![Feature Stores](./images/online_offline_fs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**NOTE**: \n",
    "\n",
    "In real life scenarios financial transaction are dynamically evolving graphs. If live Transaction Monitoring System is based on graph or node embeddings then this will require 1st to update the graph and node representations after new transactions arrive. Recomputing entire graph for every newly arrived transaction will lead to unaxeptable delayes and even monitoring system failures. This problem  will be more sever if large amount of updates happen in a short time window.\n",
    "\n",
    "Contact us at Logical Clocks and we will help you to setup end to end graph based deep anomaly detection live Transaction Monitoring Systems. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Model Repository for best node embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>8</td><td>application_1630916144621_0009</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://sp.h.p:8089/proxy/application_1630916144621_0009/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://sp.h.p:8044/node/containerlogs/container_1630916144621_0009_01_000001/amldemo__meb10000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from hops import model\n",
    "from hops.model import Metric\n",
    "MODEL_NAME=\"NodeEmbeddings\"\n",
    "EVALUATION_METRIC=\"accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model.get_best_model(MODEL_NAME, EVALUATION_METRIC, Metric.MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'application_1630916144621_0008_2'"
     ]
    }
   ],
   "source": [
    "best_model['experimentId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model and load wights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# tensorflow \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras  \n",
    "\n",
    "# pandas and numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# stellargraph library\n",
    "from stellargraph import StellarDiGraph\n",
    "from stellargraph.mapper import Node2VecLinkGenerator, Node2VecNodeGenerator\n",
    "from stellargraph.data import UnsupervisedSampler, BiasedRandomWalk\n",
    "from stellargraph.layer import Node2Vec\n",
    "\n",
    "# pyspark functions\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import array, coalesce, concat,  col\n",
    "\n",
    "# hops utility library for accessing files in HopsFS\n",
    "from hops import hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## connect hsfs library and get fs handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get node and edge traininhg dataset objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_td = fs.get_training_dataset(\"node_td\", 1)\n",
    "edge_td = fs.get_training_dataset(\"edges_td\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read training datasets as pandas df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fg as pandas\n",
    "node_pdf = node_td.read().toPandas()\n",
    "edge_pdf = edge_td.read().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read hyperparamenter for graph embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparams_path = \"Resources/embeddings_best_hp.json\"\n",
    "best_hyperparams = json.loads(hdfs.load(best_hyperparams_path))\n",
    "args_dict = {}\n",
    "for key in best_hyperparams.keys():\n",
    "    args_dict[key] = [best_hyperparams[key]]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct stellargraph Graph object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining StellarDiGraph"
     ]
    }
   ],
   "source": [
    "node_data = pd.DataFrame(node_pdf[['type']], index=node_pdf['id'])\n",
    "print('Defining StellarDiGraph')\n",
    "G =StellarDiGraph(node_data,\n",
    "                      edges=edge_pdf, \n",
    "                      edge_type_column=\"tx_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### infer node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_number = args_dict['walk_number']\n",
    "walk_length = args_dict['walk_length']\n",
    "batch_size = 1\n",
    "emb_size = args_dict['emb_size'][0]\n",
    "# Extracting node embeddings\n",
    "walker = BiasedRandomWalk(\n",
    "        G,\n",
    "        n=walk_number,\n",
    "        length=walk_length,\n",
    "        p=0.5,  # defines probability, 1/p, of returning to source node\n",
    "        q=2.0,  # defines probability, 1/q, for moving to a node away from the source node\n",
    "    )\n",
    "unsupervised_samples = UnsupervisedSampler(G, nodes=list(G.nodes()), walker=walker)\n",
    "generator = Node2VecLinkGenerator(G, batch_size)\n",
    "\n",
    "node2vec = Node2Vec(emb_size, generator=generator)\n",
    "x_inp, x_out = node2vec.in_out_tensors()\n",
    "\n",
    "x_inp_src = x_inp[0]\n",
    "x_out_src = x_out[0]\n",
    "embedding_model = keras.Model(inputs=x_inp_src, outputs=x_out_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(G.nodes())\n",
    "node_gen = Node2VecNodeGenerator(G, batch_size).flow(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.DataFrame(embedding_model.predict(node_gen), index=G.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_feature_names = [\"em_\" + str(c)  for c in pdf.columns]\n",
    "pdf.columns = emb_feature_names\n",
    "pdf['id'] = pdf.index\n",
    "node_embeddings_df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------------+------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-------------------+------------------+-------------------+--------------------+--------------------+-------------------+--------------------+-------------------+--------------------+------------------+------------------+--------------------+--------------------+--------------------+--------------------+-------------------+------------------+--------+\n",
      "|              em_0|               em_1|                em_2|               em_3|               em_4|               em_5|               em_6|              em_7|              em_8|                em_9|               em_10|               em_11|               em_12|              em_13|              em_14|              em_15|             em_16|              em_17|               em_18|               em_19|              em_20|               em_21|              em_22|               em_23|             em_24|             em_25|               em_26|               em_27|               em_28|               em_29|              em_30|             em_31|      id|\n",
      "+------------------+-------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------------+------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-------------------+------------------+-------------------+--------------------+--------------------+-------------------+--------------------+-------------------+--------------------+------------------+------------------+--------------------+--------------------+--------------------+--------------------+-------------------+------------------+--------+\n",
      "|  0.91471266746521| 0.5738992691040039| -0.5318446159362793|-0.8261938095092773| -0.637232780456543|0.38968586921691895|-0.8669264316558838|0.5759057998657227|0.6858961582183838| -0.4520151615142822|-0.25370097160339355| -0.8251006603240967|-0.02848887443542...|-0.1511833667755127|-0.9922478199005127| 0.2619180679321289|0.6308295726776123| 0.5930097103118896| 0.37354588508605957|-0.04880571365356445| 0.5698261260986328|  0.8568906784057617| 0.7701013088226318|-0.07309842109680176|0.3864095211029053|0.5565693378448486| -0.8627619743347168|-0.13106036186218262|-0.21767663955688477|  0.9527761936187744|-0.2758457660675049|0.4883086681365967|0016359b|\n",
      "|0.5313665866851807|-0.5588550567626953|-0.15098285675048828|0.42961812019348145|-0.8179001808166504|-0.8064382076263428| 0.9970095157623291|0.1922013759613037| 0.634667158126831|-0.41524481773376465| -0.7939519882202148|-0.46732354164123535|-0.04243683815002...|-0.7484605312347412| 0.4580545425415039|0.40073394775390625| -0.15753173828125|0.32444334030151367|-0.23216009140014648|  0.8669109344482422|0.48666834831237793|-0.36054491996765137|0.12740802764892578|  0.7555127143859863|0.9747557640075684|  0.97102952003479|-0.16083598136901855|-0.39045095443725586| -0.6131150722503662|-0.03789377212524414|  0.770148754119873|0.4423346519470215|0019b8d0|\n",
      "+------------------+-------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------------+------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-------------------+------------------+-------------------+--------------------+--------------------+-------------------+--------------------+-------------------+--------------------+------------------+------------------+--------------------+--------------------+--------------------+--------------------+-------------------+------------------+--------+\n",
      "only showing top 2 rows"
     ]
    }
   ],
   "source": [
    "node_embeddings_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|      id|           embedding|\n",
      "+--------+--------------------+\n",
      "|0016359b|[0.91471266746521...|\n",
      "|0019b8d0|[0.53136658668518...|\n",
      "+--------+--------------------+\n",
      "only showing top 2 rows"
     ]
    }
   ],
   "source": [
    "node_embeddings_df = node_embeddings_df.withColumn(\"embedding\", array(emb_feature_names)).select(\"id\",\"embedding\")\n",
    "node_embeddings_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a connection to hsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "from hops import hdfs\n",
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsfs import engine\n",
    "features = engine.get_instance().parse_schema_feature_group(node_embeddings_df)\n",
    "for f in features:\n",
    "    if f.type == \"array<double>\":\n",
    "        f.online_type = \"VARBINARY(200)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_hudi_options = {\n",
    "    \"hoodie.bulkinsert.shuffle.parallelism\":\"1\", \n",
    "    \"hoodie.insert.shuffle.parallelism\":\"1\", \n",
    "    \"hoodie.upsert.shuffle.parallelism\":\"1\",\n",
    "    \"hoodie.parquet.compression.ratio\":\"0.5\"\n",
    "}\n",
    "\n",
    "node_embeddings_fg = fs.create_feature_group(name=\"node_embeddings_fg\",\n",
    "                                       version=1,\n",
    "                                       primary_key=[\"id\"],\n",
    "                                       description=\"node embeddings from transactions\",\n",
    "                                       time_travel_format=\"HUDI\",     \n",
    "                                       online_enabled=True,                                                \n",
    "                                       statistics_config=False,\n",
    "                                       features=features)\n",
    "\n",
    "node_embeddings_fg.save(node_embeddings_df, extra_hudi_options)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
