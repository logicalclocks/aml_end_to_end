{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"><img src=\"images/icon102.png\" width=\"38px\"></img> **Hopsworks Feature Store** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 03: Model training & UI Exploration</span>\n",
    "\n",
    "<span style=\"font-width:bold; font-size: 1.4rem;\">In this last notebook, we will train a model on the dataset we created in the previous tutorial. We will train our model using standard Python and Scikit-learn, although it could just as well be trained with other machine learning frameworks such as PySpark, TensorFlow, and PyTorch. We will also show some of the exploration that can be done in Hopsworks, notably the search functions and the lineage. </span>\n",
    "\n",
    "## **üóíÔ∏è This notebook is divided in 3 main sections:** \n",
    "1. **Loading the training data**\n",
    "2. **Train the model**\n",
    "3. **Register and deploy model** \n",
    "\n",
    "![tutorial-flow](images/03_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "\n",
    "import hsfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to hsfs and retrieve datasets for training and evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get feature view objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view = fs.get_feature_view('aml_feature_view', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> ü™ù Training Dataset retreival </span>\n",
    "\n",
    "To retrieve training data from storage (already materialised) or from feature groups direcly we can use `get_training_dataset_splits` or `get_training_dataset` methods. If version is not provided or provided version has not already existed, it creates a new version of training data according to given arguments and returns a dataframe. If version is provided and has already existed, it reads training data from storage or feature groups and returns a dataframe. If split is provided, it reads the specific split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-19 16:10:10,615 INFO: USE `aml_demo_featurestore`\n",
      "2022-06-19 16:10:11,430 INFO: WITH right_fg0 AS (SELECT *\n",
      "FROM (SELECT `fg2`.`type` `type`, `fg2`.`is_sar` `is_sar`, `fg2`.`id` `join_pk_id`, `fg2`.`tran_timestamp` `join_evt_tran_timestamp`, `fg0`.`monthly_in_count` `monthly_in_count`, `fg0`.`monthly_in_total_amount` `monthly_in_total_amount`, `fg0`.`monthly_in_mean_amount` `monthly_in_mean_amount`, `fg0`.`monthly_in_std_amount` `monthly_in_std_amount`, `fg0`.`monthly_out_count` `monthly_out_count`, `fg0`.`monthly_out_total_amount` `monthly_out_total_amount`, `fg0`.`monthly_out_mean_amount` `monthly_out_mean_amount`, `fg0`.`monthly_out_std_amount` `monthly_out_std_amount`, RANK() OVER (PARTITION BY `fg2`.`id`, `fg2`.`tran_timestamp` ORDER BY `fg0`.`tran_timestamp` DESC) pit_rank_hopsworks\n",
      "FROM `aml_demo_featurestore`.`party_fg_1` `fg2`\n",
      "INNER JOIN `aml_demo_featurestore`.`transactions_monthly_fg_1` `fg0` ON `fg2`.`id` = `fg0`.`id` AND `fg2`.`tran_timestamp` >= `fg0`.`tran_timestamp`) NA\n",
      "WHERE `pit_rank_hopsworks` = 1), right_fg1 AS (SELECT *\n",
      "FROM (SELECT `fg2`.`type` `type`, `fg2`.`is_sar` `is_sar`, `fg2`.`id` `join_pk_id`, `fg2`.`tran_timestamp` `join_evt_tran_timestamp`, `fg1`.`graph_embeddings` `graph_embeddings`, RANK() OVER (PARTITION BY `fg2`.`id`, `fg2`.`tran_timestamp` ORDER BY `fg1`.`tran_timestamp` DESC) pit_rank_hopsworks\n",
      "FROM `aml_demo_featurestore`.`party_fg_1` `fg2`\n",
      "INNER JOIN `aml_demo_featurestore`.`graph_embeddings_fg_1` `fg1` ON `fg2`.`id` = `fg1`.`id` AND `fg2`.`tran_timestamp` >= `fg1`.`tran_timestamp`) NA\n",
      "WHERE `pit_rank_hopsworks` = 1) (SELECT `right_fg0`.`type` `type`, `right_fg0`.`is_sar` `is_sar`, `right_fg0`.`monthly_in_count` `monthly_in_count`, `right_fg0`.`monthly_in_total_amount` `monthly_in_total_amount`, `right_fg0`.`monthly_in_mean_amount` `monthly_in_mean_amount`, `right_fg0`.`monthly_in_std_amount` `monthly_in_std_amount`, `right_fg0`.`monthly_out_count` `monthly_out_count`, `right_fg0`.`monthly_out_total_amount` `monthly_out_total_amount`, `right_fg0`.`monthly_out_mean_amount` `monthly_out_mean_amount`, `right_fg0`.`monthly_out_std_amount` `monthly_out_std_amount`, `right_fg1`.`graph_embeddings` `graph_embeddings`\n",
      "FROM right_fg0\n",
      "INNER JOIN right_fg1 ON `right_fg0`.`join_pk_id` = `right_fg1`.`join_pk_id` AND `right_fg0`.`join_evt_tran_timestamp` = `right_fg1`.`join_evt_tran_timestamp`)\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y  = feature_view.get_training_data(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode embeddings\n",
    "train_x['graph_embeddings'] = train_x['graph_embeddings'].apply(literal_eval)\n",
    "train_x = train_x.merge( pd.DataFrame(train_x['graph_embeddings'].to_list()).add_prefix('emb_'), left_index=True, right_index=True).drop('graph_embeddings', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train [gan for anomaly detection](https://arxiv.org/pdf/1905.11034.pdf). Durring training step  we will provide only features of accounts that have never been reported for suspicios activity.  We will disclose previously reported accounts to the model only in evaluation step.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_sar_transactions = train_x[train_y.values == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_sar_transactions = non_sar_transactions.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now lets define Tensorflow Dataset as we are going to train keras tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (16, None, 41), types: tf.float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def windowed_dataset(dataset, window_size, batch_size):\n",
    "    ds = dataset.window(window_size, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda x: x.batch(window_size))\n",
    "    return ds.batch(batch_size,True).prefetch(1)\n",
    "\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices(tf.cast(non_sar_transactions.values, tf.float32))\n",
    "training_dataset = windowed_dataset(training_dataset, window_size=2, batch_size=16)\n",
    "training_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üèÉ Train Model</span>\n",
    "\n",
    "Next we'll train a model. Here, we set the class weight of the positive class to be twice as big as the negative class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture\n",
    "![tutorial-flow](images/model_architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GanEncAnomalyDetector(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, input_dim):\n",
    "        super(GanEncAnomalyDetector, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = [1, input_dim[1]] \n",
    "        self.d_steps = 3\n",
    "        self.gp_weight = 10 \n",
    "        \n",
    "        self.encoder = self.make_encoder_model(self.input_dim)\n",
    "        self.generator = self.make_generator(self.input_dim, self.latent_dim)\n",
    "        self.discriminator = self.make_discriminator_model(self.input_dim)\n",
    "\n",
    "        self.mse = tf.keras.losses.MeanSquaredError()\n",
    "        \n",
    "        self.epoch_e_loss_avg = tf.keras.metrics.Mean(name=\"epoch_e_loss_avg\")\n",
    "        self.epoch_d_loss_avg = tf.keras.metrics.Mean(name=\"epoch_d_loss_avg\")\n",
    "        self.epoch_g_loss_avg = tf.keras.metrics.Mean(name=\"epoch_g_loss_avg\")\n",
    "        self.epoch_a_score_avg = tf.keras.metrics.Mean(name=\"epoch_a_score_avg\")\n",
    "\n",
    "        @property\n",
    "        def metrics(self):\n",
    "            return [\n",
    "                self.epoch_e_loss_avg,\n",
    "                self.epoch_d_loss_avg,\n",
    "                self.epoch_g_loss_avg,\n",
    "                self.epoch_a_score_avg,\n",
    "            ]\n",
    "\n",
    "    # define model architectures\n",
    "    def make_encoder_model(self, input_dim):\n",
    "        inputs = tf.keras.layers.Input(shape=(input_dim[0],input_dim[1]))\n",
    "        x = tf.keras.layers.Conv1D(filters = 64, kernel_size= 1,padding='same', kernel_initializer=\"uniform\")(inputs)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "        x = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(x)\n",
    "        x = tf.keras.layers.Conv1D(filters = input_dim[1], kernel_size= 1,padding='same',  kernel_initializer=\"uniform\")(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "        x = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(x)    \n",
    "        encoder = tf.keras.Model(inputs=inputs, outputs=x, name=\"encoder_model\")\n",
    "        return encoder\n",
    "\n",
    "    def make_generator(self, input_dim, latent_dim):\n",
    "        latent_inputs = tf.keras.layers.Input(shape=(latent_dim[0],latent_dim[1]))\n",
    "        x = tf.keras.layers.Conv1D(filters = 8, kernel_size= 1,padding='same', kernel_initializer=\"uniform\")(latent_inputs) \n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "        x = tf.keras.layers.UpSampling1D(2)(x) \n",
    "        x = tf.keras.layers.Conv1D(filters = 16, kernel_size= 1,padding='same', kernel_initializer=\"uniform\")(x) \n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "        #x = tf.keras.layers.UpSampling1D(2)(x) \n",
    "        x = tf.keras.layers.Conv1D(filters = input_dim[1], kernel_size= 1,padding='same', kernel_initializer=\"uniform\")(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "        generator = tf.keras.Model(inputs=latent_inputs, outputs=x, name=\"generator_model\")        \n",
    "        return generator\n",
    "\n",
    "    def make_discriminator_model(self, input_dim):\n",
    "        inputs = tf.keras.layers.Input(shape=(input_dim[0],input_dim[1]))\n",
    "        x = tf.keras.layers.Conv1D(filters = 128, kernel_size= 1,padding='same', kernel_initializer=\"uniform\")(inputs)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "        x = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(x)\n",
    "        x = tf.keras.layers.Conv1D(filters = 64, kernel_size= 1,padding='same', kernel_initializer=\"uniform\")(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "\n",
    "        # dense output layer\n",
    "        x = tf.keras.layers.Flatten()(x)    \n",
    "        x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "        x = tf.keras.layers.Dense(128)(x)\n",
    "        x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "        prediction = tf.keras.layers.Dense(1)(x)\n",
    "        discriminator = tf.keras.Model(inputs=inputs, outputs=prediction, name=\"discriminator_model\" )               \n",
    "        return discriminator\n",
    "        \n",
    "    # Training function\n",
    "    @tf.function\n",
    "    def train_step(self, real_data):\n",
    "        if isinstance(real_data, tuple):\n",
    "            real_data = real_data[0]\n",
    "\n",
    "        # Get the batch size\n",
    "        batch_size = tf.shape(real_data)[0]\n",
    "\n",
    "        # For each batch, we are going to perform the\n",
    "        # following steps as laid out in the original paper:\n",
    "        # 1. Train the generator and get the generator loss\n",
    "        # 1a. Train the encoder and get the encoder loss\n",
    "        # 2. Train the discriminator and get the discriminator loss\n",
    "        # 3. Calculate the gradient penalty\n",
    "        # 4. Multiply this gradient penalty with a constant weight factor\n",
    "        # 5. Add the gradient penalty to the discriminator loss\n",
    "        # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "        # Train the discriminator first. The original paper recommends training\n",
    "        # the discriminator for `x` more steps (typically 5) as compared to\n",
    "        # one step of the generator. Here we will train it for 3 extra steps\n",
    "        # as compared to 5 to reduce the training time.\n",
    "        for i in range(self.d_steps):\n",
    "            # Get the latent vector\n",
    "            random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim[0], self.latent_dim[1])), \n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generate fake data from the latent vector\n",
    "                fake_data = self.generator(random_latent_vectors, training=True)\n",
    "\n",
    "                #(somewhere here step forward?)\n",
    "                # Get the logits for the fake data\n",
    "                fake_logits = self.discriminator(fake_data, training=True)\n",
    "                # Get the logits for the real data\n",
    "                real_logits = self.discriminator(real_data, training=True)\n",
    "\n",
    "                # Calculate the discriminator loss using the fake and real sample logits\n",
    "                d_cost = self.discriminator_loss(real_sample=real_logits, fake_sample=fake_logits)\n",
    "                # Calculate the gradient penalty\n",
    "                gp = self.gradient_penalty(real_data, fake_data)\n",
    "                # Add the gradient penalty to the original discriminator loss\n",
    "                d_loss = d_cost + gp * self.gp_weight\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(d_gradient, self.discriminator.trainable_variables)\n",
    "            )\n",
    "\n",
    "        # Train the generator\n",
    "        # Get the latent vector\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim[0], self.latent_dim[1]))\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake data using the generator\n",
    "            generated_data = self.generator(random_latent_vectors, training=True)\n",
    "            # Get the discriminator logits for fake data\n",
    "            gen_sample_logits = self.discriminator(generated_data, training=True)\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.generator_loss(gen_sample_logits)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "\n",
    "        # Train the encoder\n",
    "        with tf.GradientTape() as tape:\n",
    "            generated_data = self.generator(random_latent_vectors, training=True)\n",
    "            # Compress generate fake data from the latent vector\n",
    "            encoded_fake_data = self.encoder(generated_data, training=True)\n",
    "            # Reconstruct encoded generate fake data\n",
    "            generator_reconstructed_encoded_fake_data = self.generator(encoded_fake_data, training=True)\n",
    "            # Encode the latent vector\n",
    "            encoded_random_latent_vectors = self.encoder(tf.random.normal(shape=(batch_size, self.input_dim[0], self.input_dim[1])), \n",
    "                                                         training=True)\n",
    "            # Calculate encoder loss\n",
    "            e_loss = self.encoder_loss(generated_data, generator_reconstructed_encoded_fake_data)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        enc_gradient = tape.gradient(e_loss, self.encoder.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.e_optimizer.apply_gradients(\n",
    "            zip(enc_gradient, self.encoder.trainable_variables)\n",
    "        )\n",
    "\n",
    "        anomaly_score = self.compute_anomaly_score(real_data)\n",
    "\n",
    "        self.epoch_d_loss_avg.update_state(d_loss)\n",
    "        self.epoch_g_loss_avg.update_state(g_loss)\n",
    "        self.epoch_e_loss_avg.update_state(e_loss)\n",
    "        self.epoch_a_score_avg.update_state(anomaly_score[\"anomaly_score\"])\n",
    "\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss, \"e_loss\": e_loss, \"anomaly_score\": anomaly_score[\"anomaly_score\"]}\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, input):\n",
    "        if isinstance(input, tuple):\n",
    "            input = input[0]\n",
    "        \n",
    "        batch_size = tf.shape(input)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim[0], self.latent_dim[1]))\n",
    "        # Generate fake data using the generator\n",
    "        generated_data = self.generator(random_latent_vectors, training=False)\n",
    "        # Get the discriminator logits for fake data\n",
    "        gen_sample_logits = self.discriminator(generated_data, training=False)\n",
    "        # Calculate the generator loss\n",
    "        g_loss = self.generator_loss(gen_sample_logits)\n",
    "\n",
    "        \n",
    "        # Compress generate fake data from the latent vector\n",
    "        encoded_fake_data = self.encoder(generated_data, training=False)\n",
    "        # Reconstruct encoded generate fake data\n",
    "        generator_reconstructed_encoded_fake_data = self.generator(encoded_fake_data, training=False)\n",
    "\n",
    "        # Calculate encoder loss\n",
    "        e_loss = self.encoder_loss(generated_data, generator_reconstructed_encoded_fake_data)\n",
    "        \n",
    "        anomaly_score = self.compute_anomaly_score(input)\n",
    "        return {\n",
    "            \"g_loss\": g_loss,\n",
    "            \"e_loss\": e_loss,\n",
    "            \"anomaly_score\": anomaly_score[\"anomaly_score\"]\n",
    "        }\n",
    "    \n",
    "    # define custom server function\n",
    "    @tf.function\n",
    "    def serve_function(self, input):\n",
    "        return self.compute_anomaly_score(input)\n",
    "\n",
    "    def call(self, input):\n",
    "        if isinstance(input, tuple):\n",
    "            input = input[0]\n",
    "        \n",
    "        encoded = self.encoder(input)\n",
    "        decoded = self.generator(encoded)\n",
    "        anomaly_score = self.compute_anomaly_score(input)\n",
    "        return anomaly_score[\"anomaly_score\"], decoded\n",
    "\n",
    "    def compile(self):\n",
    "        super(GanEncAnomalyDetector, self).compile()     \n",
    "        # Define optimizers\n",
    "        self.e_optimizer = tf.keras.optimizers.SGD(lr=0.00001, clipnorm=0.01)        \n",
    "        self.d_optimizer = tf.keras.optimizers.SGD(lr=0.00001, clipnorm=0.01)\n",
    "        self.g_optimizer = tf.keras.optimizers.SGD(lr=0.00001, clipnorm=0.01)\n",
    "\n",
    "    def gradient_penalty(self, real_data, fake_data):\n",
    "        \"\"\" Calculates the gradient penalty.\n",
    "        This loss is calculated on an interpolated sample\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # Get the interpolated sample\n",
    "        real_data_shape = tf.shape(real_data)\n",
    "        alpha = tf.random.normal(shape=[real_data_shape[0], real_data_shape[1], real_data_shape[2]], mean=0.0, stddev=2.0, dtype=tf.dtypes.float32)\n",
    "        #alpha = tf.random_uniform([self.batch_size, 1], minval=-2, maxval=2, dtype=tf.dtypes.float32)\n",
    "        interpolated = (alpha * real_data) + ((1 - alpha) * fake_data)\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated sample.\n",
    "            pred = self.discriminator(interpolated, training=True)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated sample.\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        # 3. Calculate the norm of the gradients.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[-2, -1]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp    \n",
    "        \n",
    "    def encoder_loss(self,generated_fake_data, generator_reconstructed_encoded_fake_data):\n",
    "        generator_reconstracted_data = tf.cast(generator_reconstructed_encoded_fake_data, tf.float32)\n",
    "        loss = self.mse(generated_fake_data, generator_reconstracted_data)\n",
    "        beta_cycle_gen = 10.0\n",
    "        loss = loss * beta_cycle_gen\n",
    "        return loss\n",
    "\n",
    "    # Define the loss functions for the discriminator,\n",
    "    # which should be (fake_loss - real_loss).\n",
    "    # We will add the gradient penalty later to this loss function.\n",
    "    def discriminator_loss(self, real_sample, fake_sample):\n",
    "        real_loss = tf.reduce_mean(real_sample)\n",
    "        fake_loss = tf.reduce_mean(fake_sample)\n",
    "        return fake_loss - real_loss\n",
    "\n",
    "    # Define the loss functions for the generator.\n",
    "    def generator_loss(self, fake_sample):\n",
    "        return -tf.reduce_mean(fake_sample)\n",
    "    \n",
    "    def compute_anomaly_score(self, input):\n",
    "        \"\"\"anomaly score.\n",
    "          See https://arxiv.org/pdf/1905.11034.pdf for more details\n",
    "        \"\"\"\n",
    "        # Encode the real data\n",
    "        encoded_real_data = self.encoder(input, training=False)\n",
    "        # Reconstruct encoded real data\n",
    "        generator_reconstructed_encoded_real_data = self.generator(encoded_real_data, training=False)\n",
    "        # Calculate distance between real and reconstructed data (Here may be step forward?)\n",
    "        gen_rec_loss_predict = self.mse(input,generator_reconstructed_encoded_real_data)\n",
    "\n",
    "        # # Compute anomaly score\n",
    "        # real_to_orig_dist_predict = tf.math.reduce_sum(tf.math.pow(encoded_random_latent - encoded_real_data, 2), axis=[-1])\n",
    "        # anomaly_score = (gen_rec_loss_predict * self.anomaly_alpha) + ((1 - self.anomaly_alpha) * real_to_orig_dist_predict)\n",
    "        anomaly_score = gen_rec_loss_predict\n",
    "        return {'anomaly_score': anomaly_score} \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GanEncAnomalyDetector([2, 41])\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_model (None, 1, 41)\n",
      "generator_model (None, 2, 41)\n",
      "discriminator_model (None, 1)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(training_dataset, epochs=2, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics={'loss': history.history[\"g_loss\"][0]} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">  Register anomaly detection model to hopsworks model registry </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting trained model to: /srv/hops/staging/private_dirs/8deec0164f68faee054f68bf17951435dcd209946539a0f4f9cd0e48f91fd3d7/model-0f743f48-5af4-48c1-a7b5-d4b9babc78a5\n",
      "2022-06-19 16:12:35,934 WARNING: Skipping full serialization of Keras layer <__main__.GanEncAnomalyDetector object at 0x7f41db9c95e0>, because it is not built.\n",
      "2022-06-19 16:12:51,427 WARNING: Found untraced functions such as test_step while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "2022-06-19 16:12:51,773 WARNING: Found untraced functions such as test_step while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "2022-06-19 16:12:53,962 INFO: Assets written to: /srv/hops/staging/private_dirs/8deec0164f68faee054f68bf17951435dcd209946539a0f4f9cd0e48f91fd3d7/model-0f743f48-5af4-48c1-a7b5-d4b9babc78a5/assets\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c70b231cb5425c8311aaf64d8d97d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created, explore it at https://hopsworks0.logicalclocks.com/p/119/models/aml_model/1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(name: 'aml_model', version: 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "\n",
    "# hops model registry library\n",
    "import hsml\n",
    "    \n",
    "# save to the model registry\n",
    "export_path = os.getcwd() + '/model-' + str(uuid.uuid4())\n",
    "print('Exporting trained model to: {}'.format(export_path))\n",
    "    \n",
    "call = model.serve_function.get_concrete_function(tf.TensorSpec([None,None,None], tf.float32))\n",
    "tf.saved_model.save(model, export_path, signatures=call)\n",
    "\n",
    "conn_hsml = hsml.connection()\n",
    "mr = conn_hsml.get_model_registry()\n",
    "\n",
    "# define sample input \n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(tf.cast(non_sar_transactions.values, tf.float32))\n",
    "test_dataset = windowed_dataset(test_dataset, window_size=2, batch_size=1)\n",
    "input_example = next(iter(test_dataset)).numpy()\n",
    "\n",
    "mr_model = mr.tensorflow.create_model(\n",
    "    name=\"aml_model\",\n",
    "    metrics=metrics,\n",
    "    description=\"Adversarial anomaly detection model.\",\n",
    "    input_example=input_example\n",
    ")\n",
    "\n",
    "mr_model.save(export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> Deploy model </span>\n",
    "\n",
    "## About Model Serving\n",
    "Models can be served via KFServing or \"default\" serving, which means a Docker container exposing a Flask server. For KFServing models, or models written in Tensorflow, you do not need to write a prediction file (see the section below). However, for sklearn models using default serving, you do need to proceed to write a prediction file.\n",
    "\n",
    "In order to use KFServing, you must have Kubernetes installed and enabled on your cluster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model name from the previous notebook.\n",
    "model = mr.get_model(\"aml_model\", version=1)\n",
    "\n",
    "# Give it any name you want\n",
    "deployment = model.deploy(\n",
    "    name=\"amldeployment\",\n",
    "    model_server=\"TENSORFLOW_SERVING\", \n",
    "    serving_tool=\"KSERVE\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment: amldeployment\n",
      "{\n",
      "    \"artifact_version\": 0,\n",
      "    \"batching_configuration\": {\n",
      "        \"batching_enabled\": false\n",
      "    },\n",
      "    \"created\": \"2022-06-19T16:15:14.24Z\",\n",
      "    \"creator\": \"Admin Admin\",\n",
      "    \"id\": 2,\n",
      "    \"inference_logging\": \"NONE\",\n",
      "    \"model_name\": \"aml_model\",\n",
      "    \"model_path\": \"/Projects/aml_demo/Models/aml_model\",\n",
      "    \"model_server\": \"TENSORFLOW_SERVING\",\n",
      "    \"model_version\": 1,\n",
      "    \"name\": \"amldeployment\",\n",
      "    \"predictor\": null,\n",
      "    \"predictor_resources\": {\n",
      "        \"limits\": {\n",
      "            \"cores\": 1,\n",
      "            \"gpus\": 0,\n",
      "            \"memory\": 1024\n",
      "        },\n",
      "        \"requests\": {\n",
      "            \"cores\": 1,\n",
      "            \"gpus\": 0,\n",
      "            \"memory\": 1024\n",
      "        }\n",
      "    },\n",
      "    \"requested_instances\": 1,\n",
      "    \"serving_tool\": \"KSERVE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"Deployment: \" + deployment.name)\n",
    "deployment.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deployment has now been registered. However, to start it you need to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b6ff90ff7644ef9eaf009c142c1352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start making predictions by using `.predict()`\n"
     ]
    }
   ],
   "source": [
    "deployment.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For trouble shooting one can use `get_logs` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explore all the logs and filters in the Kibana logs at https://hopsworks0.logicalclocks.com/p/119/deployments/2\n",
      "\n",
      "Instance name: amldeployment-predictor-default-00001-deployment-55cfb55d5rj8zb\n",
      "2022-06-19 16:15:41.935199: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.\n",
      "2022-06-19 16:15:41.969706: I external/org_tensorflow/tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2199985000 Hz\n",
      "2022-06-19 16:15:42.183170: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:190] Running initialization op on SavedModel bundle at path: /mnt/models/0\n",
      "2022-06-19 16:15:42.337756: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: success: OK. Took 782650 microseconds.\n",
      "2022-06-19 16:15:42.377744: I tensorflow_serving/servables/tensorflow/saved_model_warmup_util.cc:59] No warmup data file found at /mnt/models/0/assets.extra/tf_serving_warmup_requests\n",
      "2022-06-19 16:15:42.381680: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: amldeployment version: 0}\n",
      "2022-06-19 16:15:42.396467: I tensorflow_serving/model_servers/server.cc:371] Running gRPC ModelServer at 0.0.0.0:9000 ...\n",
      "[warn] getaddrinfo: address family for nodename not supported\n",
      "2022-06-19 16:15:42.400007: I tensorflow_serving/model_servers/server.cc:391] Exporting HTTP/REST API at:localhost:8080 ...\n",
      "[evhttp_server.cc : 238] NET_LOG: Entering the event loop ...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deployment.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the deployment\n",
    "Let's use the input example that we registered together with the model to query the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'outputs': 0.75004369}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    \"inputs\": model.input_example\n",
    "}\n",
    "\n",
    "deployment.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explore all the logs and filters in the Kibana logs at https://hopsworks0.logicalclocks.com/p/119/deployments/2\n",
      "\n",
      "Instance name: amldeployment-predictor-default-00001-deployment-55cfb55d5rj8zb\n",
      "2022-06-19 16:15:41.969706: I external/org_tensorflow/tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2199985000 Hz\n",
      "2022-06-19 16:15:42.183170: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:190] Running initialization op on SavedModel bundle at path: /mnt/models/0\n",
      "2022-06-19 16:15:42.337756: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: success: OK. Took 782650 microseconds.\n",
      "2022-06-19 16:15:42.377744: I tensorflow_serving/servables/tensorflow/saved_model_warmup_util.cc:59] No warmup data file found at /mnt/models/0/assets.extra/tf_serving_warmup_requests\n",
      "2022-06-19 16:15:42.381680: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: amldeployment version: 0}\n",
      "2022-06-19 16:15:42.396467: I tensorflow_serving/model_servers/server.cc:371] Running gRPC ModelServer at 0.0.0.0:9000 ...\n",
      "[warn] getaddrinfo: address family for nodename not supported\n",
      "2022-06-19 16:15:42.400007: I tensorflow_serving/model_servers/server.cc:391] Exporting HTTP/REST API at:localhost:8080 ...\n",
      "[evhttp_server.cc : 238] NET_LOG: Entering the event loop ...\n",
      "2022-06-19 16:16:58.672139: W external/org_tensorflow/tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at conv_ops_fused_impl.h:768 : Invalid argument: input must be 4-dimensional[1,1,1,2,41]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For trouble shooting one can use get_logs methods\n",
    "deployment.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use REST endpoint\n",
    "You can also use a REST endpoint for your model. To do this you need to create an API key with 'serving' enabled, and retrieve the endpoint URL from the Model Serving UI.\n",
    "\n",
    "Go to the Model Serving UI and click on the eye icon next to a model to retrieve the endpoint URL. The shorter URL is an internal endpoint that you can only reach from within Hopsworks. If you want to call it from outside, you need one of the longer URLs. \n",
    "\n",
    "![serving-endpoints](images/serving_endpoints.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "mr = project.get_model_registry()\n",
    "\n",
    "# Use the model name from the previous notebook.\n",
    "model = mr.get_model(\"fraud_tutorial_model\", version=1)\n",
    "\n",
    "test_inputs = [model.input_example]\n",
    "\n",
    "API_KEY = \"...\"  # Put your API key here.\n",
    "MODEL_SERVING_URL = \"...\" # Put model serving endppoint here.\n",
    "HOST_NAME = \"...\" # Put your hopsworks model serving hostname here \n",
    "\n",
    "data = {\"inputs\": test_inputs}\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\", \"Accept\": \"application/json\",\n",
    "    \"Authorization\": f\"ApiKey {API_KEY}\",\n",
    "    \"Host\": HOST_NAME}\n",
    "\n",
    "response = requests.post(MODEL_SERVING_URL, verify=False, headers=headers, json=data)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets test feature vectors from online store\n",
    "ids_to_score = [\"0016359b\", \n",
    "                \"001dcc27\", \n",
    "                \"0054a022\", \n",
    "                \"00d6b609\", \n",
    "                \"00e14860\", \n",
    "                \"00e39a1b\", \n",
    "                \"014ed5cb\", \n",
    "                \"01ce3306\", \n",
    "                \"01fa19ae\", \n",
    "                \"01fa1d01\", \n",
    "                \"036dce03\", \n",
    "                \"03e09be4\", \n",
    "                \"04b23f4b\"]\n",
    "\n",
    "def flat2gen(alist):\n",
    "    for item in alist:\n",
    "        if isinstance(item, list):\n",
    "            for subitem in item: yield subitem\n",
    "        else:\n",
    "            yield item\n",
    "\n",
    "def model_server(input):\n",
    "    data = {\"inputs\": [input]}\n",
    "    return deployment.predict(data)\n",
    "\n",
    "for node_id in ids_to_score:\n",
    "    serving_vector = np.array(list(flat2gen(feature_view.get_feature_vector({'id': node_id})))).reshape(1,41).tolist()\n",
    "    print(\" anomaly score for node_id \", node_id, \" : \",   model_server(serving_vector)[\"outputs\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Deployment\n",
    "To stop the deployment we simply run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üéÅ Wrapping things up </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module we perforemed feature engineering, created feature view and traning dataset, trained advance anomaly detection model and depoyed it in production. You can try this out on hopsworks.ai and contuct us for any futher details.\n",
    "\n",
    "<img src=\"images/contuct_us.png\" width=\"400px\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}