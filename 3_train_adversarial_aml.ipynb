{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"><img src=\"images/icon102.png\" width=\"38px\"></img> **Hopsworks Feature Store** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 03: Model training & UI Exploration</span>\n",
    "\n",
    "<span style=\"font-width:bold; font-size: 1.4rem;\">In this last notebook, we will train a model on the dataset we created in the previous tutorial. We will train our model using standard Python and Scikit-learn, although it could just as well be trained with other machine learning frameworks such as PySpark, TensorFlow, and PyTorch. We will also show some of the exploration that can be done in Hopsworks, notably the search functions and the lineage. </span>\n",
    "\n",
    "## **üóíÔ∏è This notebook is divided in 3 main sections:** \n",
    "1. **Loading the training data**\n",
    "2. **Train the model**\n",
    "3. **Explore feature groups and views** via the UI.\n",
    "\n",
    "![tutorial-flow](images/03_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "\n",
    "import hsfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to hsfs and retrieve datasets for training and evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get feature view objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view_non_sar = fs.get_feature_view('non_sar_transactions_view', 1)\n",
    "feature_view_sar = fs.get_feature_view('sar_transactions_view', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> ü™ù Training Dataset retreival </span>\n",
    "\n",
    "To retrieve training data from storage (already materialised) or from feature groups direcly we can use `get_training_dataset_splits` or `get_training_dataset` methods. If version is not provided or provided version has not already existed, it creates a new version of training data according to given arguments and returns a dataframe. If version is provided and has already existed, it reads training data from storage or feature groups and returns a dataframe. If split is provided, it reads the specific split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-11 23:43:27,974 INFO: USE `aml_demo_featurestore`\n",
      "2022-06-11 23:43:28,806 INFO: WITH right_fg0 AS (SELECT *\n",
      "FROM (SELECT `fg2`.`type` `type`, `fg2`.`is_sar` `is_sar`, `fg2`.`id` `join_pk_id`, `fg2`.`tran_timestamp` `join_evt_tran_timestamp`, `fg0`.`monthly_in_count` `monthly_in_count`, `fg0`.`monthly_in_total_amount` `monthly_in_total_amount`, `fg0`.`monthly_in_mean_amount` `monthly_in_mean_amount`, `fg0`.`monthly_in_std_amount` `monthly_in_std_amount`, `fg0`.`monthly_out_count` `monthly_out_count`, `fg0`.`monthly_out_total_amount` `monthly_out_total_amount`, `fg0`.`monthly_out_mean_amount` `monthly_out_mean_amount`, `fg0`.`monthly_out_std_amount` `monthly_out_std_amount`, RANK() OVER (PARTITION BY `fg2`.`id`, `fg2`.`tran_timestamp` ORDER BY `fg0`.`tran_timestamp` DESC) pit_rank_hopsworks\n",
      "FROM `aml_demo_featurestore`.`party_fg_1` `fg2`\n",
      "INNER JOIN `aml_demo_featurestore`.`transactions_monthly_fg_1` `fg0` ON `fg2`.`id` = `fg0`.`id` AND `fg2`.`tran_timestamp` >= `fg0`.`tran_timestamp`\n",
      "WHERE `fg2`.`is_sar` = 0) NA\n",
      "WHERE `pit_rank_hopsworks` = 1), right_fg1 AS (SELECT *\n",
      "FROM (SELECT `fg2`.`type` `type`, `fg2`.`is_sar` `is_sar`, `fg2`.`id` `join_pk_id`, `fg2`.`tran_timestamp` `join_evt_tran_timestamp`, `fg1`.`graph_embeddings` `graph_embeddings`, RANK() OVER (PARTITION BY `fg2`.`id`, `fg2`.`tran_timestamp` ORDER BY `fg1`.`tran_timestamp` DESC) pit_rank_hopsworks\n",
      "FROM `aml_demo_featurestore`.`party_fg_1` `fg2`\n",
      "INNER JOIN `aml_demo_featurestore`.`graph_embeddings_fg_1` `fg1` ON `fg2`.`id` = `fg1`.`id` AND `fg2`.`tran_timestamp` >= `fg1`.`tran_timestamp`\n",
      "WHERE `fg2`.`is_sar` = 0) NA\n",
      "WHERE `pit_rank_hopsworks` = 1) (SELECT `right_fg0`.`type` `type`, `right_fg0`.`is_sar` `is_sar`, `right_fg0`.`monthly_in_count` `monthly_in_count`, `right_fg0`.`monthly_in_total_amount` `monthly_in_total_amount`, `right_fg0`.`monthly_in_mean_amount` `monthly_in_mean_amount`, `right_fg0`.`monthly_in_std_amount` `monthly_in_std_amount`, `right_fg0`.`monthly_out_count` `monthly_out_count`, `right_fg0`.`monthly_out_total_amount` `monthly_out_total_amount`, `right_fg0`.`monthly_out_mean_amount` `monthly_out_mean_amount`, `right_fg0`.`monthly_out_std_amount` `monthly_out_std_amount`, `right_fg1`.`graph_embeddings` `graph_embeddings`\n",
      "FROM right_fg0\n",
      "INNER JOIN right_fg1 ON `right_fg0`.`join_pk_id` = `right_fg1`.`join_pk_id` AND `right_fg0`.`join_evt_tran_timestamp` = `right_fg1`.`join_evt_tran_timestamp`)\n"
     ]
    }
   ],
   "source": [
    "td_non_sar_bundle = feature_view_non_sar.get_training_dataset(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-11 23:44:55,861 INFO: USE `aml_demo_featurestore`\n",
      "2022-06-11 23:44:56,784 INFO: WITH right_fg0 AS (SELECT *\n",
      "FROM (SELECT `fg2`.`type` `type`, `fg2`.`is_sar` `is_sar`, `fg2`.`id` `join_pk_id`, `fg2`.`tran_timestamp` `join_evt_tran_timestamp`, `fg0`.`monthly_in_count` `monthly_in_count`, `fg0`.`monthly_in_total_amount` `monthly_in_total_amount`, `fg0`.`monthly_in_mean_amount` `monthly_in_mean_amount`, `fg0`.`monthly_in_std_amount` `monthly_in_std_amount`, `fg0`.`monthly_out_count` `monthly_out_count`, `fg0`.`monthly_out_total_amount` `monthly_out_total_amount`, `fg0`.`monthly_out_mean_amount` `monthly_out_mean_amount`, `fg0`.`monthly_out_std_amount` `monthly_out_std_amount`, RANK() OVER (PARTITION BY `fg2`.`id`, `fg2`.`tran_timestamp` ORDER BY `fg0`.`tran_timestamp` DESC) pit_rank_hopsworks\n",
      "FROM `aml_demo_featurestore`.`party_fg_1` `fg2`\n",
      "INNER JOIN `aml_demo_featurestore`.`transactions_monthly_fg_1` `fg0` ON `fg2`.`id` = `fg0`.`id` AND `fg2`.`tran_timestamp` >= `fg0`.`tran_timestamp`\n",
      "WHERE `fg2`.`is_sar` = 0) NA\n",
      "WHERE `pit_rank_hopsworks` = 1), right_fg1 AS (SELECT *\n",
      "FROM (SELECT `fg2`.`type` `type`, `fg2`.`is_sar` `is_sar`, `fg2`.`id` `join_pk_id`, `fg2`.`tran_timestamp` `join_evt_tran_timestamp`, `fg1`.`graph_embeddings` `graph_embeddings`, RANK() OVER (PARTITION BY `fg2`.`id`, `fg2`.`tran_timestamp` ORDER BY `fg1`.`tran_timestamp` DESC) pit_rank_hopsworks\n",
      "FROM `aml_demo_featurestore`.`party_fg_1` `fg2`\n",
      "INNER JOIN `aml_demo_featurestore`.`graph_embeddings_fg_1` `fg1` ON `fg2`.`id` = `fg1`.`id` AND `fg2`.`tran_timestamp` >= `fg1`.`tran_timestamp`\n",
      "WHERE `fg2`.`is_sar` = 0) NA\n",
      "WHERE `pit_rank_hopsworks` = 1) (SELECT `right_fg0`.`type` `type`, `right_fg0`.`is_sar` `is_sar`, `right_fg0`.`monthly_in_count` `monthly_in_count`, `right_fg0`.`monthly_in_total_amount` `monthly_in_total_amount`, `right_fg0`.`monthly_in_mean_amount` `monthly_in_mean_amount`, `right_fg0`.`monthly_in_std_amount` `monthly_in_std_amount`, `right_fg0`.`monthly_out_count` `monthly_out_count`, `right_fg0`.`monthly_out_total_amount` `monthly_out_total_amount`, `right_fg0`.`monthly_out_mean_amount` `monthly_out_mean_amount`, `right_fg0`.`monthly_out_std_amount` `monthly_out_std_amount`, `right_fg1`.`graph_embeddings` `graph_embeddings`\n",
      "FROM right_fg0\n",
      "INNER JOIN right_fg1 ON `right_fg0`.`join_pk_id` = `right_fg1`.`join_pk_id` AND `right_fg0`.`join_evt_tran_timestamp` = `right_fg1`.`join_evt_tran_timestamp`)\n"
     ]
    }
   ],
   "source": [
    "td_sar_bundle = feature_view_non_sar.get_training_dataset(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_sar_df = td_non_sar_bundle.get_dataset()\n",
    "sar_df = td_sar_bundle.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_sar_df['graph_embeddings'] = non_sar_df['graph_embeddings'].apply(literal_eval)\n",
    "non_sar_df = non_sar_df.join( pd.DataFrame(non_sar_df['graph_embeddings'].to_list()).add_prefix('emb_') ).drop('graph_embeddings', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "sar_df['graph_embeddings'] = sar_df['graph_embeddings'].apply(literal_eval)\n",
    "sar_df = sar_df.join( pd.DataFrame(sar_df['graph_embeddings'].to_list()).add_prefix('emb_') ).drop('graph_embeddings', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now lets define Tensorflow Dataset as we are going to train keras tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (16, None, 41), types: tf.float32>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def windowed_dataset(dataset, window_size, batch_size):\n",
    "    ds = dataset.window(window_size, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda x: x.batch(window_size))\n",
    "    return ds.batch(batch_size,True).prefetch(1)\n",
    "\n",
    "targets = feature_view_non_sar.labels\n",
    "features = [col for col in non_sar_df.columns if col not in targets]\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices(tf.cast(non_sar_df[features].values, tf.float32)) #, tf.cast(non_sar_df[targets].values, tf.int32)\n",
    "    \n",
    "\n",
    "training_dataset = windowed_dataset(training_dataset, window_size=2, batch_size=16)\n",
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (1, None, 41), types: tf.float32>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = feature_view_sar.labels\n",
    "features = [col for col in sar_df.columns if col not in targets]\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(tf.cast(sar_df[features].values, tf.float32)) #tf.cast(sar_df[targets].values, tf.int32)\n",
    "val_dataset = windowed_dataset(val_dataset, window_size=2, batch_size=1)\n",
    "val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üèÉ Train Model</span>\n",
    "\n",
    "Next we'll train a model. Here, we set the class weight of the positive class to be twice as big as the negative class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GanEncAnomalyDetector(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, input_dim):\n",
    "        super(GanEncAnomalyDetector, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = [1, input_dim[1]] \n",
    "        self.d_steps = 3\n",
    "        self.gp_weight = 10 \n",
    "        \n",
    "        self.encoder = self.make_encoder_model(self.input_dim)\n",
    "        self.generator = self.make_generator(self.input_dim, self.latent_dim)\n",
    "        self.discriminator = self.make_discriminator_model(self.input_dim)\n",
    "\n",
    "        self.mse = tf.keras.losses.MeanSquaredError()\n",
    "        \n",
    "        self.epoch_e_loss_avg = tf.keras.metrics.Mean(name=\"epoch_e_loss_avg\")\n",
    "        self.epoch_d_loss_avg = tf.keras.metrics.Mean(name=\"epoch_d_loss_avg\")\n",
    "        self.epoch_g_loss_avg = tf.keras.metrics.Mean(name=\"epoch_g_loss_avg\")\n",
    "        self.epoch_a_score_avg = tf.keras.metrics.Mean(name=\"epoch_a_score_avg\")\n",
    "\n",
    "        @property\n",
    "        def metrics(self):\n",
    "            return [\n",
    "                self.epoch_e_loss_avg,\n",
    "                self.epoch_d_loss_avg,\n",
    "                self.epoch_g_loss_avg,\n",
    "                self.epoch_a_score_avg,\n",
    "            ]\n",
    "\n",
    "    # define model architectures\n",
    "    def make_encoder_model(self, input_dim):\n",
    "        inputs = tf.keras.layers.Input(shape=(input_dim[0],input_dim[1]))\n",
    "        x = tf.keras.layers.Conv1D(filters = 64, kernel_size= 1,padding='same', kernel_initializer=\"uniform\")(inputs)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "        x = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(x)\n",
    "        x = tf.keras.layers.Conv1D(filters = input_dim[1], kernel_size= 1,padding='same',  kernel_initializer=\"uniform\")(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "        x = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(x)    \n",
    "        encoder = tf.keras.Model(inputs=inputs, outputs=x, name=\"encoder_model\")\n",
    "        return encoder\n",
    "\n",
    "    def make_generator(self, input_dim, latent_dim):\n",
    "        latent_inputs = tf.keras.layers.Input(shape=(latent_dim[0],latent_dim[1]))\n",
    "        x = tf.keras.layers.Conv1D(filters = 8, kernel_size= 1,padding='same', kernel_initializer=\"uniform\")(latent_inputs) \n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "        x = tf.keras.layers.UpSampling1D(2)(x) \n",
    "        x = tf.keras.layers.Conv1D(filters = 16, kernel_size= 1,padding='same', kernel_initializer=\"uniform\")(x) \n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "        #x = tf.keras.layers.UpSampling1D(2)(x) \n",
    "        x = tf.keras.layers.Conv1D(filters = input_dim[1], kernel_size= 1,padding='same', kernel_initializer=\"uniform\")(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "        generator = tf.keras.Model(inputs=latent_inputs, outputs=x, name=\"generator_model\")        \n",
    "        return generator\n",
    "\n",
    "    def make_discriminator_model(self, input_dim):\n",
    "        inputs = tf.keras.layers.Input(shape=(input_dim[0],input_dim[1]))\n",
    "        x = tf.keras.layers.Conv1D(filters = 128, kernel_size= 1,padding='same', kernel_initializer=\"uniform\")(inputs)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "        x = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(x)\n",
    "        x = tf.keras.layers.Conv1D(filters = 64, kernel_size= 1,padding='same', kernel_initializer=\"uniform\")(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)    \n",
    "\n",
    "        # dense output layer\n",
    "        x = tf.keras.layers.Flatten()(x)    \n",
    "        x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "        x = tf.keras.layers.Dense(128)(x)\n",
    "        x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "        prediction = tf.keras.layers.Dense(1)(x)\n",
    "        discriminator = tf.keras.Model(inputs=inputs, outputs=prediction, name=\"discriminator_model\" )               \n",
    "        return discriminator\n",
    "        \n",
    "    # Training function\n",
    "    @tf.function\n",
    "    def train_step(self, real_data):\n",
    "        if isinstance(real_data, tuple):\n",
    "            real_data = real_data[0]\n",
    "\n",
    "        # Get the batch size\n",
    "        batch_size = tf.shape(real_data)[0]\n",
    "\n",
    "        # For each batch, we are going to perform the\n",
    "        # following steps as laid out in the original paper:\n",
    "        # 1. Train the generator and get the generator loss\n",
    "        # 1a. Train the encoder and get the encoder loss\n",
    "        # 2. Train the discriminator and get the discriminator loss\n",
    "        # 3. Calculate the gradient penalty\n",
    "        # 4. Multiply this gradient penalty with a constant weight factor\n",
    "        # 5. Add the gradient penalty to the discriminator loss\n",
    "        # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "        # Train the discriminator first. The original paper recommends training\n",
    "        # the discriminator for `x` more steps (typically 5) as compared to\n",
    "        # one step of the generator. Here we will train it for 3 extra steps\n",
    "        # as compared to 5 to reduce the training time.\n",
    "        for i in range(self.d_steps):\n",
    "            # Get the latent vector\n",
    "            random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim[0], self.latent_dim[1])), \n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generate fake data from the latent vector\n",
    "                fake_data = self.generator(random_latent_vectors, training=True)\n",
    "\n",
    "                #(somewhere here step forward?)\n",
    "                # Get the logits for the fake data\n",
    "                fake_logits = self.discriminator(fake_data, training=True)\n",
    "                # Get the logits for the real data\n",
    "                real_logits = self.discriminator(real_data, training=True)\n",
    "\n",
    "                # Calculate the discriminator loss using the fake and real sample logits\n",
    "                d_cost = self.discriminator_loss(real_sample=real_logits, fake_sample=fake_logits)\n",
    "                # Calculate the gradient penalty\n",
    "                gp = self.gradient_penalty(real_data, fake_data)\n",
    "                # Add the gradient penalty to the original discriminator loss\n",
    "                d_loss = d_cost + gp * self.gp_weight\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(d_gradient, self.discriminator.trainable_variables)\n",
    "            )\n",
    "\n",
    "        # Train the generator\n",
    "        # Get the latent vector\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim[0], self.latent_dim[1]))\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake data using the generator\n",
    "            generated_data = self.generator(random_latent_vectors, training=True)\n",
    "            # Get the discriminator logits for fake data\n",
    "            gen_sample_logits = self.discriminator(generated_data, training=True)\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.generator_loss(gen_sample_logits)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "\n",
    "        # Train the encoder\n",
    "        with tf.GradientTape() as tape:\n",
    "            generated_data = self.generator(random_latent_vectors, training=True)\n",
    "            # Compress generate fake data from the latent vector\n",
    "            encoded_fake_data = self.encoder(generated_data, training=True)\n",
    "            # Reconstruct encoded generate fake data\n",
    "            generator_reconstructed_encoded_fake_data = self.generator(encoded_fake_data, training=True)\n",
    "            # Encode the latent vector\n",
    "            encoded_random_latent_vectors = self.encoder(tf.random.normal(shape=(batch_size, self.input_dim[0], self.input_dim[1])), \n",
    "                                                         training=True)\n",
    "            # Calculate encoder loss\n",
    "            e_loss = self.encoder_loss(generated_data, generator_reconstructed_encoded_fake_data)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        enc_gradient = tape.gradient(e_loss, self.encoder.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.e_optimizer.apply_gradients(\n",
    "            zip(enc_gradient, self.encoder.trainable_variables)\n",
    "        )\n",
    "\n",
    "        anomaly_score = self.compute_anomaly_score(real_data)\n",
    "\n",
    "        self.epoch_d_loss_avg.update_state(d_loss)\n",
    "        self.epoch_g_loss_avg.update_state(g_loss)\n",
    "        self.epoch_e_loss_avg.update_state(e_loss)\n",
    "        self.epoch_a_score_avg.update_state(anomaly_score[\"anomaly_score\"])\n",
    "\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss, \"e_loss\": e_loss, \"anomaly_score\": anomaly_score[\"anomaly_score\"]}\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, input):\n",
    "        if isinstance(input, tuple):\n",
    "            input = input[0]\n",
    "        \n",
    "        batch_size = tf.shape(input)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim[0], self.latent_dim[1]))\n",
    "        # Generate fake data using the generator\n",
    "        generated_data = self.generator(random_latent_vectors, training=False)\n",
    "        # Get the discriminator logits for fake data\n",
    "        gen_sample_logits = self.discriminator(generated_data, training=False)\n",
    "        # Calculate the generator loss\n",
    "        g_loss = self.generator_loss(gen_sample_logits)\n",
    "\n",
    "        \n",
    "        # Compress generate fake data from the latent vector\n",
    "        encoded_fake_data = self.encoder(generated_data, training=False)\n",
    "        # Reconstruct encoded generate fake data\n",
    "        generator_reconstructed_encoded_fake_data = self.generator(encoded_fake_data, training=False)\n",
    "\n",
    "        # Calculate encoder loss\n",
    "        e_loss = self.encoder_loss(generated_data, generator_reconstructed_encoded_fake_data)\n",
    "        \n",
    "        anomaly_score = self.compute_anomaly_score(input)\n",
    "        return {\n",
    "            \"g_loss\": g_loss,\n",
    "            \"e_loss\": e_loss,\n",
    "            \"anomaly_score\": anomaly_score[\"anomaly_score\"]\n",
    "        }\n",
    "    \n",
    "    # define custom server function\n",
    "    @tf.function\n",
    "    def serve_function(self, input):\n",
    "        return self.compute_anomaly_score(input)\n",
    "\n",
    "    def call(self, input):\n",
    "        if isinstance(input, tuple):\n",
    "            input = input[0]\n",
    "        \n",
    "        encoded = self.encoder(input)\n",
    "        decoded = self.generator(encoded)\n",
    "        anomaly_score = self.compute_anomaly_score(input)\n",
    "        return anomaly_score[\"anomaly_score\"], decoded\n",
    "\n",
    "    def compile(self):\n",
    "        super(GanEncAnomalyDetector, self).compile()     \n",
    "        # Define optimizers\n",
    "        self.e_optimizer = tf.keras.optimizers.Adam(lr=0.0001)        \n",
    "        self.d_optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "        self.g_optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "\n",
    "    def gradient_penalty(self, real_data, fake_data):\n",
    "        \"\"\" Calculates the gradient penalty.\n",
    "        This loss is calculated on an interpolated sample\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # Get the interpolated sample\n",
    "        real_data_shape = tf.shape(real_data)\n",
    "        alpha = tf.random.normal(shape=[real_data_shape[0], real_data_shape[1], real_data_shape[2]], mean=0.0, stddev=2.0, dtype=tf.dtypes.float32)\n",
    "        #alpha = tf.random_uniform([self.batch_size, 1], minval=-2, maxval=2, dtype=tf.dtypes.float32)\n",
    "        interpolated = (alpha * real_data) + ((1 - alpha) * fake_data)\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated sample.\n",
    "            pred = self.discriminator(interpolated, training=True)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated sample.\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        # 3. Calculate the norm of the gradients.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[-2, -1]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp    \n",
    "        \n",
    "    def encoder_loss(self,generated_fake_data, generator_reconstructed_encoded_fake_data):\n",
    "        generator_reconstracted_data = tf.cast(generator_reconstructed_encoded_fake_data, tf.float32)\n",
    "        loss = self.mse(generated_fake_data, generator_reconstracted_data)\n",
    "        beta_cycle_gen = 10.0\n",
    "        loss = loss * beta_cycle_gen\n",
    "        return loss\n",
    "\n",
    "    # Define the loss functions for the discriminator,\n",
    "    # which should be (fake_loss - real_loss).\n",
    "    # We will add the gradient penalty later to this loss function.\n",
    "    def discriminator_loss(self, real_sample, fake_sample):\n",
    "        real_loss = tf.reduce_mean(real_sample)\n",
    "        fake_loss = tf.reduce_mean(fake_sample)\n",
    "        return fake_loss - real_loss\n",
    "\n",
    "    # Define the loss functions for the generator.\n",
    "    def generator_loss(self, fake_sample):\n",
    "        return -tf.reduce_mean(fake_sample)\n",
    "    \n",
    "    def compute_anomaly_score(self, input):\n",
    "        \"\"\"anomaly score.\n",
    "          See https://arxiv.org/pdf/1905.11034.pdf for more details\n",
    "        \"\"\"\n",
    "        # Encode the real data\n",
    "        encoded_real_data = self.encoder(input, training=False)\n",
    "        # Reconstruct encoded real data\n",
    "        generator_reconstructed_encoded_real_data = self.generator(encoded_real_data, training=False)\n",
    "        # Calculate distance between real and reconstructed data (Here may be step forward?)\n",
    "        gen_rec_loss_predict = self.mse(input,generator_reconstructed_encoded_real_data)\n",
    "\n",
    "        # # Compute anomaly score\n",
    "        # real_to_orig_dist_predict = tf.math.reduce_sum(tf.math.pow(encoded_random_latent - encoded_real_data, 2), axis=[-1])\n",
    "        # anomaly_score = (gen_rec_loss_predict * self.anomaly_alpha) + ((1 - self.anomaly_alpha) * real_to_orig_dist_predict)\n",
    "        anomaly_score = gen_rec_loss_predict\n",
    "        return {'anomaly_score': anomaly_score} \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GanEncAnomalyDetector([2, 41])\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_model (None, 1, 41)\n",
      "generator_model (None, 2, 41)\n",
      "discriminator_model (None, 1)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.96573831999558\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d_loss': [-205.0683135986328],\n",
       " 'g_loss': [96.07129669189453],\n",
       " 'e_loss': [0.695880115032196],\n",
       " 'anomaly_score': [1053.383056640625],\n",
       " 'val_g_loss': [-33.028106689453125],\n",
       " 'val_e_loss': [0.2668232321739197],\n",
       " 'val_anomaly_score': [1191.53515625]}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## from timeit import default_timer as timer\n",
    "start = timer()\n",
    "history = model.fit(training_dataset,\n",
    "                    epochs=1,\n",
    "                    verbose=0,\n",
    "#                    steps_per_epoch=5,\n",
    "                    validation_data=val_dataset,\n",
    "                    validation_steps=1,                    \n",
    "                   )\n",
    "end = timer()\n",
    "print(end - start)\n",
    "\n",
    "history_dict = history.history\n",
    "history_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">  Use the model to score transactions </span>\n",
    "We trained model based on January - February data. Now lets retrieve March data and score whether transactions are fraudulend or not   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}