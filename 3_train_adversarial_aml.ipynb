{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"><img src=\"images/icon102.png\" width=\"38px\"></img> **Hopsworks Feature Store** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 03: Model training & UI Exploration</span>\n",
    "\n",
    "<span style=\"font-width:bold; font-size: 1.4rem;\">In this last notebook, we will train a model on the dataset we created in the previous tutorial. We will train our model using standard Python and Scikit-learn, although it could just as well be trained with other machine learning frameworks such as PySpark, TensorFlow, and PyTorch. We will also show some of the exploration that can be done in Hopsworks, notably the search functions and the lineage. </span>\n",
    "\n",
    "## **üóíÔ∏è This notebook is divided in 3 main sections:** \n",
    "1. **Loading the training data**\n",
    "2. **Train the model**\n",
    "3. **Explore feature groups and views** via the UI.\n",
    "\n",
    "![tutorial-flow](images/03_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to hsfs and retrieve datasets for training and evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n"
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get feature view objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view_non_sar = fs.get_feature_view('non_sar_transactions_view', 1)\n",
    "feature_view_sar = fs.get_feature_view('sar_transactions_view', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> ‚ú® Load Training Data </span>\n",
    "\n",
    "First, we'll need to fetch the training dataset that we created in the previous notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-07 17:27:30,047 INFO: USE `aml_demo_featurestore`\n",
      "2022-06-07 17:27:30,862 INFO: WITH right_fg0 AS (SELECT *\n",
      "FROM (SELECT `fg2`.`type` `type`, `fg2`.`is_sar` `is_sar`, `fg2`.`id` `join_pk_id`, `fg2`.`tran_timestamp` `join_evt_tran_timestamp`, `fg0`.`monthly_in_count` `monthly_in_count`, `fg0`.`monthly_in_total_amount` `monthly_in_total_amount`, `fg0`.`monthly_in_mean_amount` `monthly_in_mean_amount`, `fg0`.`monthly_in_std_amount` `monthly_in_std_amount`, `fg0`.`monthly_out_count` `monthly_out_count`, `fg0`.`monthly_out_total_amount` `monthly_out_total_amount`, `fg0`.`monthly_out_mean_amount` `monthly_out_mean_amount`, `fg0`.`monthly_out_std_amount` `monthly_out_std_amount`, RANK() OVER (PARTITION BY `fg2`.`id`, `fg2`.`tran_timestamp` ORDER BY `fg0`.`tran_timestamp` DESC) pit_rank_hopsworks\n",
      "FROM `aml_demo_featurestore`.`party_fg_1` `fg2`\n",
      "INNER JOIN `aml_demo_featurestore`.`transactions_monthly_fg_1` `fg0` ON `fg2`.`id` = `fg0`.`id` AND `fg2`.`tran_timestamp` >= `fg0`.`tran_timestamp`\n",
      "WHERE `fg2`.`is_sar` = 0) NA\n",
      "WHERE `pit_rank_hopsworks` = 1), right_fg1 AS (SELECT *\n",
      "FROM (SELECT `fg2`.`type` `type`, `fg2`.`is_sar` `is_sar`, `fg2`.`id` `join_pk_id`, `fg2`.`tran_timestamp` `join_evt_tran_timestamp`, `fg1`.`graph_embeddings` `graph_embeddings`, RANK() OVER (PARTITION BY `fg2`.`id`, `fg2`.`tran_timestamp` ORDER BY `fg1`.`tran_timestamp` DESC) pit_rank_hopsworks\n",
      "FROM `aml_demo_featurestore`.`party_fg_1` `fg2`\n",
      "INNER JOIN `aml_demo_featurestore`.`graph_embeddings_fg_1` `fg1` ON `fg2`.`id` = `fg1`.`id` AND `fg2`.`tran_timestamp` >= `fg1`.`tran_timestamp`\n",
      "WHERE `fg2`.`is_sar` = 0) NA\n",
      "WHERE `pit_rank_hopsworks` = 1) (SELECT `right_fg0`.`type` `type`, `right_fg0`.`is_sar` `is_sar`, `right_fg0`.`monthly_in_count` `monthly_in_count`, `right_fg0`.`monthly_in_total_amount` `monthly_in_total_amount`, `right_fg0`.`monthly_in_mean_amount` `monthly_in_mean_amount`, `right_fg0`.`monthly_in_std_amount` `monthly_in_std_amount`, `right_fg0`.`monthly_out_count` `monthly_out_count`, `right_fg0`.`monthly_out_total_amount` `monthly_out_total_amount`, `right_fg0`.`monthly_out_mean_amount` `monthly_out_mean_amount`, `right_fg0`.`monthly_out_std_amount` `monthly_out_std_amount`, `right_fg1`.`graph_embeddings` `graph_embeddings`\n",
      "FROM right_fg0\n",
      "INNER JOIN right_fg1 ON `right_fg0`.`join_pk_id` = `right_fg1`.`join_pk_id` AND `right_fg0`.`join_evt_tran_timestamp` = `right_fg1`.`join_evt_tran_timestamp`)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VersionWarning: No version provided for creating training dataset, incremented version to `4`.\n"
     ]
    }
   ],
   "source": [
    "version_non_sar_td, non_sar_td = feature_view_non_sar.get_training_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-07 17:32:09,009 INFO: USE `aml_demo_featurestore`\n",
      "2022-06-07 17:32:09,844 INFO: WITH right_fg0 AS (SELECT *\n",
      "FROM (SELECT `fg2`.`type` `type`, `fg2`.`is_sar` `is_sar`, `fg2`.`id` `join_pk_id`, `fg2`.`tran_timestamp` `join_evt_tran_timestamp`, `fg0`.`monthly_in_count` `monthly_in_count`, `fg0`.`monthly_in_total_amount` `monthly_in_total_amount`, `fg0`.`monthly_in_mean_amount` `monthly_in_mean_amount`, `fg0`.`monthly_in_std_amount` `monthly_in_std_amount`, `fg0`.`monthly_out_count` `monthly_out_count`, `fg0`.`monthly_out_total_amount` `monthly_out_total_amount`, `fg0`.`monthly_out_mean_amount` `monthly_out_mean_amount`, `fg0`.`monthly_out_std_amount` `monthly_out_std_amount`, RANK() OVER (PARTITION BY `fg2`.`id`, `fg2`.`tran_timestamp` ORDER BY `fg0`.`tran_timestamp` DESC) pit_rank_hopsworks\n",
      "FROM `aml_demo_featurestore`.`party_fg_1` `fg2`\n",
      "INNER JOIN `aml_demo_featurestore`.`transactions_monthly_fg_1` `fg0` ON `fg2`.`id` = `fg0`.`id` AND `fg2`.`tran_timestamp` >= `fg0`.`tran_timestamp`\n",
      "WHERE `fg2`.`is_sar` = 0) NA\n",
      "WHERE `pit_rank_hopsworks` = 1), right_fg1 AS (SELECT *\n",
      "FROM (SELECT `fg2`.`type` `type`, `fg2`.`is_sar` `is_sar`, `fg2`.`id` `join_pk_id`, `fg2`.`tran_timestamp` `join_evt_tran_timestamp`, `fg1`.`graph_embeddings` `graph_embeddings`, RANK() OVER (PARTITION BY `fg2`.`id`, `fg2`.`tran_timestamp` ORDER BY `fg1`.`tran_timestamp` DESC) pit_rank_hopsworks\n",
      "FROM `aml_demo_featurestore`.`party_fg_1` `fg2`\n",
      "INNER JOIN `aml_demo_featurestore`.`graph_embeddings_fg_1` `fg1` ON `fg2`.`id` = `fg1`.`id` AND `fg2`.`tran_timestamp` >= `fg1`.`tran_timestamp`\n",
      "WHERE `fg2`.`is_sar` = 0) NA\n",
      "WHERE `pit_rank_hopsworks` = 1) (SELECT `right_fg0`.`type` `type`, `right_fg0`.`is_sar` `is_sar`, `right_fg0`.`monthly_in_count` `monthly_in_count`, `right_fg0`.`monthly_in_total_amount` `monthly_in_total_amount`, `right_fg0`.`monthly_in_mean_amount` `monthly_in_mean_amount`, `right_fg0`.`monthly_in_std_amount` `monthly_in_std_amount`, `right_fg0`.`monthly_out_count` `monthly_out_count`, `right_fg0`.`monthly_out_total_amount` `monthly_out_total_amount`, `right_fg0`.`monthly_out_mean_amount` `monthly_out_mean_amount`, `right_fg0`.`monthly_out_std_amount` `monthly_out_std_amount`, `right_fg1`.`graph_embeddings` `graph_embeddings`\n",
      "FROM right_fg0\n",
      "INNER JOIN right_fg1 ON `right_fg0`.`join_pk_id` = `right_fg1`.`join_pk_id` AND `right_fg0`.`join_evt_tran_timestamp` = `right_fg1`.`join_evt_tran_timestamp`)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VersionWarning: No version provided for creating training dataset, incremented version to `5`.\n"
     ]
    }
   ],
   "source": [
    "version_sar_td, sar_td = feature_view_non_sar.get_training_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üèÉ Train Model</span>\n",
    "\n",
    "Next we'll train a model. Here, we set the class weight of the positive class to be twice as big as the negative class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def make_discriminator_model_cnn(input_dim):\n",
    "    inputs = tf.keras.layers.Input(shape=(input_dim[0], input_dim[1]))\n",
    "    x = tf.keras.layers.Conv1D(filters=128, kernel_size=1, padding='same', kernel_initializer=\"uniform\")(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=64, kernel_size=1, padding='same', kernel_initializer=\"uniform\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    # dense output layer\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "    x = tf.keras.layers.Dense(128)(x)\n",
    "    x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "    prediction = tf.keras.layers.Dense(1)(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=prediction)\n",
    "\n",
    "\n",
    "def make_generator_model_cnn(input_dim, latent_dim):\n",
    "    latent_inputs = tf.keras.layers.Input(shape=(latent_dim[0], latent_dim[1]))\n",
    "    x = tf.keras.layers.Conv1D(filters=4, kernel_size=1, padding='same', kernel_initializer=\"uniform\")(latent_inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = tf.keras.layers.UpSampling1D(2)(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=8, kernel_size=1, padding='same', kernel_initializer=\"uniform\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = tf.keras.layers.UpSampling1D(2)(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=16, kernel_size=1, padding='same', kernel_initializer=\"uniform\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = tf.keras.layers.UpSampling1D(2)(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=input_dim[1], kernel_size=1, padding='same', kernel_initializer=\"uniform\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    return tf.keras.Model(inputs=latent_inputs, outputs=x)\n",
    "\n",
    "\n",
    "def make_encoder_model_cnn(input_dim):\n",
    "    inputs = tf.keras.layers.Input(shape=(input_dim[0], input_dim[1]))\n",
    "    x = tf.keras.layers.Conv1D(filters=16, kernel_size=1, padding='same', kernel_initializer=\"uniform\")(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=8, kernel_size=1, padding='same', kernel_initializer=\"uniform\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=4, kernel_size=1, padding='same', kernel_initializer=\"uniform\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2, padding='same')(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "\n",
    "def encoder_loss(generated_fake_data, generator_reconstructed_encoded_fake_data, global_batch_size):\n",
    "    generator_reconstracted_data = tf.cast(generator_reconstructed_encoded_fake_data, tf.float32)\n",
    "    mse = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    per_batch_loss = mse(generated_fake_data, generator_reconstracted_data)\n",
    "    # per_batch_loss = tf.math.reduce_sum(tf.math.pow(generated_fake_data - generator_reconstracted_data, 2), axis=[-1])\n",
    "    beta_cycle_gen = 10.0\n",
    "    per_batch_loss = per_batch_loss * beta_cycle_gen\n",
    "    # loss = tf.nn.compute_average_loss(per_batch_loss, global_batch_size=BATCH_SIZE)\n",
    "    return tf.reduce_sum(per_batch_loss) * (1. / global_batch_size)\n",
    "\n",
    "# Define the loss functions for the discriminator,\n",
    "# which should be (fake_loss - real_loss).\n",
    "# We will add the gradient penalty later to this loss function.\n",
    "def discriminator_loss(real_sample, fake_sample, beta_cycle_gen, global_batch_size):\n",
    "    real_loss = tf.reduce_mean(real_sample)\n",
    "    fake_loss = tf.reduce_mean(fake_sample)\n",
    "    per_batch_loss = fake_loss - real_loss\n",
    "    per_batch_loss = per_batch_loss * beta_cycle_gen\n",
    "    return tf.reduce_sum(per_batch_loss) * (1. / global_batch_size)\n",
    "\n",
    "\n",
    "# Define the loss functions for the generator.\n",
    "def generator_loss(fake_sample, global_batch_size):\n",
    "    per_batch_loss = -tf.reduce_mean(fake_sample)\n",
    "    return tf.reduce_sum(per_batch_loss) * (1. / global_batch_size)\n",
    "\n",
    "\n",
    "def experiment_wrapper():\n",
    "    \n",
    "    import os\n",
    "    import sys\n",
    "    import uuid\n",
    "    import random  \n",
    "    import numpy as np\n",
    "        \n",
    "    from pydoop import hdfs as pydoop_hdfs\n",
    "    from hops import hdfs\n",
    "    from hops import tensorboard\n",
    "    from hops import model as hops_model\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    \n",
    "    from adversarialaml import gan_enc_anomaly_model\n",
    "    from adversarialaml import gan_enc_anomaly_trainer\n",
    "    from adversarialaml import orbit\n",
    "    \n",
    "    ######################################\n",
    "    NCCL_SOCKET_NTHREADS = '16'\n",
    "    NCCL_NSOCKS_PERTHREAD = '16'\n",
    "    os.environ['NCCL_IB_DISABLE'] = '1'\n",
    "    os.environ['NCCL_DEBUG'] = 'INFO'\n",
    "    os.environ['NCCL_SOCKET_NTHREADS'] = NCCL_SOCKET_NTHREADS\n",
    "    os.environ['NCCL_NSOCKS_PERTHREAD'] = NCCL_NSOCKS_PERTHREAD\n",
    "    \n",
    "    \"\"\"\n",
    "    from tensorflow.keras import mixed_precision\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    mixed_precision.set_global_policy(policy)\n",
    "    \"\"\"\n",
    "    \n",
    "    discriminator_bytes_per_pack= 4 * 1024 * 1024\n",
    "    generator_bytes_per_pack= 4 * 1024 * 1024\n",
    "    encoder_bytes_per_pack= 1 * 1024 * 1024\n",
    "    \n",
    "    # Define distribution strategy\n",
    "    options = tf.distribute.experimental.CommunicationOptions(\n",
    "        #bytes_per_pack=1 * 1024 * 1024,\n",
    "        #timeout_seconds=120.0,\n",
    "        implementation=tf.distribute.experimental.CommunicationImplementation.NCCL\n",
    "    )\n",
    "    # Define distribution strategy\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy(communication_options=options)\n",
    "    #strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "\n",
    "    data_options = tf.data.Options()\n",
    "    data_options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "        \n",
    "    ######################################\n",
    "    BATCH_SIZE_PER_REPLICA = 4096\n",
    "    WINDOW_SIZE = 32\n",
    "    EPOCHS = 10000000 \n",
    "    \n",
    "    # Define global batch size\n",
    "    BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "    TOTAL_SAMPLES = 100000\n",
    "    STEPS_PER_EPOCH=5000000 #TOTAL_SAMPLES//BATCH_SIZE\n",
    "    VALIDATION_STEPS=2000          \n",
    "\n",
    "    INPUT_DIM = [32, 1] \n",
    "    LATENT_DIM = [4, 4] # TODO (davit): this is hard coded \n",
    "    D_STEPS = 5\n",
    "    GP_WEIGHT = 10 \n",
    "    \n",
    "    def windowed_dataset(dataset, window_size):\n",
    "        ds = dataset.window(window_size, shift=1, drop_remainder=True)\n",
    "        ds = ds.flat_map(lambda w: w.batch(window_size))\n",
    "        return ds\n",
    "\n",
    "    def input_fn(batch_size, window_size, epochs, steps_per_epoch):\n",
    "      x = np.random.random((100001, 1))\n",
    "      x = tf.cast(x, tf.float32)\n",
    "      dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "      dataset = windowed_dataset(dataset, window_size)\n",
    "      dataset = dataset.repeat(epochs*steps_per_epoch)\n",
    "      dataset = dataset.cache()    \n",
    "      dataset = dataset.batch(batch_size)\n",
    "      dataset = dataset.prefetch(50000000) #tf.data.experimental.AUTOTUNE\n",
    "        \n",
    "      options = tf.data.Options()\n",
    "      options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "      return dataset.with_options(options)\n",
    "\n",
    "    train_dataset = input_fn(BATCH_SIZE, WINDOW_SIZE, EPOCHS, STEPS_PER_EPOCH)\n",
    "    \n",
    "    #####################################################\n",
    "    # construct model under distribution strategy scope\n",
    "    with strategy.scope(): \n",
    "        discriminator_model = gan_enc_anomaly_model.make_discriminator_model_cnn(INPUT_DIM)\n",
    "        generator_model = gan_enc_anomaly_model.make_generator_model_cnn(INPUT_DIM, LATENT_DIM)\n",
    "        encoder_model = gan_enc_anomaly_model.make_encoder_model_cnn(INPUT_DIM)\n",
    "        #train_dataset = strategy.distribute_datasets_from_function(\n",
    "        #  lambda input_context: data_input(train_dataset_files, input_context, BATCH_SIZE, WINDOW_SIZE, EPOCHS))        \n",
    "        \n",
    "    # Define optimizers\n",
    "    with strategy.scope():\n",
    "        discriminator_optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "        generator_optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "        encoder_optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "        \n",
    "    # construct model under distribution strategy scope\n",
    "    with strategy.scope(): \n",
    "        discriminator_model = make_discriminator_model_ff()\n",
    "        generator_model = make_generator_model_ff()\n",
    "        encoder_model = make_encoder_model_ff()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">  Use the model to score transactions </span>\n",
    "We trained model based on January - February data. Now lets retrieve March data and score whether transactions are fraudulend or not   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}