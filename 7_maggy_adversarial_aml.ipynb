{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly detection model achitecture and hyperparameter tuning \n",
    "No we are ready to define GAN based anomaly detection model architecture and perform hyperparameter tuning using maggy. For more details about this model refer to https://arxiv.org/pdf/1905.11034.pdf.\n",
    "![Training Dataset](./images/maggy_hp.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>49</td><td>application_1651406915314_0007</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://ip-172-31-31-70.eu-north-1.compute.internal:8089/proxy/application_1651406915314_0007/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://ip-172-31-26-54.eu-north-1.compute.internal:8044/node/containerlogs/container_e02_1651406915314_0007_01_000001/aml_demo__davit000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "<pyspark.sql.session.SparkSession object at 0x7f179e574850>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import experiment\n",
    "from hops import hdfs\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparams_path = \"Resources/embeddings_best_hp.json\"\n",
    "best_hyperparams = json.loads(hdfs.load(best_hyperparams_path))\n",
    "args_dict = {}\n",
    "for key in best_hyperparams.keys():\n",
    "    args_dict[key] = [best_hyperparams[key]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define hopsworks experiments wrapper function and put all the training logic there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_wrapper(\n",
    "    latent_dim,\n",
    "    discriminator_n_layers,\n",
    "    discriminator_activation_fn,\n",
    "    discriminator_middle_layer_activation_fn,    \n",
    "    \n",
    "    discriminator_batch_norm,\n",
    "    discriminator_dropout_rate, \n",
    "    discriminator_learning_rate,\n",
    "    discriminator_extra_steps,\n",
    "\n",
    "    generator_start_n_units,\n",
    "    generator_n_layers,\n",
    "    generator_activation_fn,\n",
    "    generator_middle_layer_activation_fn,        \n",
    "    generator_batch_norm,\n",
    "    generator_dropout_rate, \n",
    "    generator_learning_rate,\n",
    "\n",
    "    encoder_start_n_units,\n",
    "    encoder_n_layers,\n",
    "    encoder_activation_fn,\n",
    "    encoder_middle_layer_activation_fn,        \n",
    "    encoder_batch_norm,\n",
    "    encoder_dropout_rate, \n",
    "    encoder_learning_rate):\n",
    "        \n",
    "    import tensorflow as tf\n",
    "    from adversarialaml.gan_enc_ano import GanAnomalyDetector,  GanAnomalyMonitor \n",
    "    from hops import tensorboard\n",
    "\n",
    "    ## Connect to hsfs and retrieve datasets for training and evaluation \n",
    "    import hsfs\n",
    "    # Create a connection\n",
    "    connection = hsfs.connection(engine = \"training\")\n",
    "    # Get the feature store handle for the project's feature store\n",
    "    fs = connection.get_feature_store()\n",
    "\n",
    "    ben_td = fs.get_training_dataset(\"gan_non_sar_training_df\", 1)\n",
    "    eval_td = fs.get_training_dataset(\"gan_eval_df\", 1)\n",
    "    \n",
    "    int_to_act_fn = {\n",
    "        1: 'linear',        \n",
    "        2: 'relu',\n",
    "        3: 'leaky_relu',\n",
    "        4: 'selu',\n",
    "        5: 'tanh'\n",
    "    }\n",
    "    \n",
    "    # Set the number of epochs for trainining.\n",
    "    EPOCHS = 2\n",
    "    BATCH_SIZE = 32\n",
    "    TOTAL_SAMPLES = 6366\n",
    "    STEPS_PER_EPOCH=TOTAL_SAMPLES//BATCH_SIZE\n",
    "\n",
    "    train_input = ben_td.tf_data(target_name='is_sar', is_training=True)\n",
    "    train_input_processed = train_input.tf_record_dataset(process=True, batch_size=BATCH_SIZE, num_epochs=EPOCHS)\n",
    "    eval_input = eval_td.tf_data(target_name='is_sar', is_training=True)\n",
    "    eval_input_processed = eval_input.tf_record_dataset(process=True, batch_size=1, num_epochs=EPOCHS)    \n",
    "        \n",
    "    discriminator_activation_fn=int_to_act_fn[discriminator_activation_fn]\n",
    "    discriminator_middle_layer_activation_fn=int_to_act_fn[discriminator_middle_layer_activation_fn]\n",
    "    \n",
    "    if discriminator_dropout_rate > 0.0:\n",
    "        discriminator_batch_dropout = True\n",
    "    else:\n",
    "        discriminator_batch_dropout = False\n",
    "    \n",
    "    if discriminator_dropout_rate > 0.0:\n",
    "        generator_batch_dropout=True\n",
    "    else:\n",
    "        generator_batch_dropout=False\n",
    "\n",
    "    if encoder_dropout_rate > 0.0:\n",
    "        encoder_batch_dropout=True\n",
    "    else:\n",
    "        encoder_batch_dropout=False   \n",
    "\n",
    "\n",
    "    if discriminator_batch_norm==0:\n",
    "        discriminator_batch_norm = False\n",
    "    else:\n",
    "        discriminator_batch_norm = True\n",
    "\n",
    "    generator_activation_fn=int_to_act_fn[generator_activation_fn]\n",
    "    generator_middle_layer_activation_fn=int_to_act_fn[generator_middle_layer_activation_fn]\n",
    "\n",
    "    if generator_batch_norm==0:\n",
    "        generator_batch_norm = False\n",
    "    else:\n",
    "        generator_batch_norm = True\n",
    "\n",
    "    encoder_activation_fn=int_to_act_fn[encoder_activation_fn]\n",
    "    encoder_middle_layer_activation_fn=int_to_act_fn[encoder_middle_layer_activation_fn]\n",
    "                \n",
    "    if encoder_batch_norm==0:\n",
    "        encoder_batch_norm=False\n",
    "    else:\n",
    "        encoder_batch_norm=True        \n",
    "        \n",
    "\n",
    "    discriminator_double_neurons=False\n",
    "    discriminator_bottleneck_neurons=True\n",
    "    generator_double_neurons=True\n",
    "    generator_bottleneck_neurons=False\n",
    "        \n",
    "    # Instantiate the GanAnomalyDetector model.\n",
    "    gan_anomaly_detector = GanAnomalyDetector(\n",
    "                input_dim=args_dict['emb_size'][0],\n",
    "                latent_dim=latent_dim,\n",
    "\n",
    "                discriminator_start_n_units=args_dict['emb_size'][0],\n",
    "                discriminator_n_layers=discriminator_n_layers,\n",
    "                discriminator_activation_fn=discriminator_activation_fn,\n",
    "                discriminator_middle_layer_activation_fn=discriminator_middle_layer_activation_fn,\n",
    "                discriminator_double_neurons=discriminator_double_neurons,\n",
    "                discriminator_bottleneck_neurons=discriminator_bottleneck_neurons,\n",
    "                discriminator_batch_norm=discriminator_batch_norm,\n",
    "                discriminator_batch_dropout=discriminator_batch_dropout,\n",
    "                discriminator_dropout_rate=discriminator_dropout_rate,\n",
    "                discriminator_learning_rate=discriminator_learning_rate,\n",
    "                discriminator_extra_steps=discriminator_extra_steps,\n",
    "\n",
    "                generator_start_n_units=generator_start_n_units,\n",
    "                generator_n_layers=generator_n_layers,\n",
    "                generator_activation_fn=generator_activation_fn,\n",
    "                generator_middle_layer_activation_fn=generator_middle_layer_activation_fn,        \n",
    "                generator_double_neurons=generator_double_neurons,\n",
    "                generator_bottleneck_neurons=generator_bottleneck_neurons,\n",
    "                generator_batch_norm=generator_batch_norm,\n",
    "                generator_batch_dropout=generator_batch_dropout,\n",
    "                generator_dropout_rate=generator_dropout_rate,\n",
    "                generator_learning_rate=generator_learning_rate,\n",
    "\n",
    "                encoder_start_n_units=encoder_start_n_units,\n",
    "                encoder_n_layers=encoder_n_layers,\n",
    "                encoder_activation_fn=encoder_activation_fn,\n",
    "                encoder_middle_layer_activation_fn=encoder_middle_layer_activation_fn,        \n",
    "                encoder_batch_norm=encoder_batch_norm,\n",
    "                encoder_batch_dropout=encoder_batch_dropout,\n",
    "                encoder_dropout_rate=encoder_dropout_rate,\n",
    "                encoder_learning_rate=encoder_learning_rate,\n",
    "    )\n",
    "    \n",
    "    # Compile the WGAN model.\n",
    "    gan_anomaly_detector.compile()\n",
    "    \n",
    "    # Start training the model.\n",
    "    history = gan_anomaly_detector.fit(train_input_processed, epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH)\n",
    "\n",
    "    metrics={'metric': history.history[\"g_loss\"][0]} \n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The searchspace can be instantiated with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter added: latent_dim\n",
      "Hyperparameter added: discriminator_n_layers\n",
      "Hyperparameter added: discriminator_activation_fn\n",
      "Hyperparameter added: discriminator_middle_layer_activation_fn\n",
      "Hyperparameter added: discriminator_batch_norm\n",
      "Hyperparameter added: discriminator_dropout_rate\n",
      "Hyperparameter added: discriminator_learning_rate\n",
      "Hyperparameter added: discriminator_extra_steps\n",
      "Hyperparameter added: generator_start_n_units\n",
      "Hyperparameter added: generator_n_layers\n",
      "Hyperparameter added: generator_activation_fn\n",
      "Hyperparameter added: generator_middle_layer_activation_fn\n",
      "Hyperparameter added: generator_batch_norm\n",
      "Hyperparameter added: generator_dropout_rate\n",
      "Hyperparameter added: generator_learning_rate\n",
      "Hyperparameter added: encoder_start_n_units\n",
      "Hyperparameter added: encoder_n_layers\n",
      "Hyperparameter added: encoder_activation_fn\n",
      "Hyperparameter added: encoder_middle_layer_activation_fn\n",
      "Hyperparameter added: encoder_batch_norm\n",
      "Hyperparameter added: encoder_dropout_rate\n",
      "Hyperparameter added: encoder_learning_rate"
     ]
    }
   ],
   "source": [
    "from maggy import Searchspace\n",
    "sp = Searchspace(\n",
    "\n",
    "    latent_dim=('DISCRETE', [16, 32]),\n",
    "    discriminator_n_layers=('INTEGER', [2, 3]),\n",
    "    discriminator_activation_fn=('INTEGER', [1, 4]),\n",
    "    discriminator_middle_layer_activation_fn=('INTEGER', [1, 4]),    \n",
    "    discriminator_batch_norm=('INTEGER', [0, 1]), \n",
    "    discriminator_dropout_rate=('DOUBLE', [0.0, 0.1]), \n",
    "    discriminator_learning_rate=('DOUBLE', [0.0001, 0.0002]),\n",
    "    discriminator_extra_steps=('INTEGER', [2, 3]),\n",
    "\n",
    "    generator_start_n_units=('DISCRETE', [16, 32]),\n",
    "    generator_n_layers=('INTEGER', [2, 3]),\n",
    "    generator_activation_fn=('INTEGER', [1, 5]),\n",
    "    generator_middle_layer_activation_fn=('INTEGER', [1, 4]),    \n",
    "    generator_batch_norm=('INTEGER', [0, 1]),\n",
    "    generator_dropout_rate=('DISCRETE', [0.0, 0.1]), \n",
    "    generator_learning_rate=('DISCRETE', [0.0001, 0.0002]),\n",
    "\n",
    "    encoder_start_n_units=('DISCRETE', [16, 32]),\n",
    "    encoder_n_layers=('INTEGER', [2, 3]),\n",
    "    encoder_activation_fn=('INTEGER', [2, 4]),\n",
    "    encoder_middle_layer_activation_fn=('INTEGER', [1, 4]),        \n",
    "    encoder_batch_norm=('INTEGER', [0, 1]),\n",
    "    encoder_dropout_rate=('DOUBLE', [0.0, 0.1]), \n",
    "    encoder_learning_rate=('DOUBLE', [0.0001, 0.0002]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use above experiments wrapper function to conduct hops training experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3993f46c3702453aadc153086ef9eecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Maggy experiment', max=2.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Connected. Call `.close()` to terminate connection gracefully.\n",
      "0: \n",
      "0: \n",
      "0: \n",
      "0: \n",
      "0: \n",
      "0: \n",
      "0: Epoch 1/2\n",
      "0: \n",
      "0: \n",
      "1: Connected. Call `.close()` to terminate connection gracefully.\n",
      "1: \n",
      "1: \n",
      "1: \n",
      "1: \n",
      "1: \n",
      "1: \n",
      "1: Epoch 1/2\n",
      "1: \n",
      "1: \n",
      "0: Epoch 2/2\n",
      "1: Epoch 2/2\n",
      "Started Maggy Experiment: ganaml, application_1651406915314_0007, run 1\n",
      "\n",
      "------ RandomSearch Results ------ direction(min) \n",
      "BEST combination {\"latent_dim\": 32, \"discriminator_n_layers\": 3, \"discriminator_activation_fn\": 1, \"discriminator_middle_layer_activation_fn\": 4, \"discriminator_batch_norm\": 1, \"discriminator_dropout_rate\": 0.0352678688936785, \"discriminator_learning_rate\": 0.00011325225509138715, \"discriminator_extra_steps\": 3, \"generator_start_n_units\": 16, \"generator_n_layers\": 3, \"generator_activation_fn\": 4, \"generator_middle_layer_activation_fn\": 3, \"generator_batch_norm\": 0, \"generator_dropout_rate\": 0.0, \"generator_learning_rate\": 0.0001, \"encoder_start_n_units\": 32, \"encoder_n_layers\": 3, \"encoder_activation_fn\": 2, \"encoder_middle_layer_activation_fn\": 3, \"encoder_batch_norm\": 0, \"encoder_dropout_rate\": 0.008476616865434272, \"encoder_learning_rate\": 0.00013585011007676072} -- metric 4.470348358154297e-08\n",
      "WORST combination {\"latent_dim\": 32, \"discriminator_n_layers\": 3, \"discriminator_activation_fn\": 1, \"discriminator_middle_layer_activation_fn\": 2, \"discriminator_batch_norm\": 0, \"discriminator_dropout_rate\": 0.010450312265667727, \"discriminator_learning_rate\": 0.0001684916717138383, \"discriminator_extra_steps\": 3, \"generator_start_n_units\": 16, \"generator_n_layers\": 3, \"generator_activation_fn\": 2, \"generator_middle_layer_activation_fn\": 1, \"generator_batch_norm\": 1, \"generator_dropout_rate\": 0.1, \"generator_learning_rate\": 0.0001, \"encoder_start_n_units\": 16, \"encoder_n_layers\": 2, \"encoder_activation_fn\": 3, \"encoder_middle_layer_activation_fn\": 1, \"encoder_batch_norm\": 1, \"encoder_dropout_rate\": 0.007501530290810499, \"encoder_learning_rate\": 0.00014188201104911694} -- metric 0.21065931022167206\n",
      "AVERAGE metric -- 0.10532967746257782\n",
      "EARLY STOPPED Trials -- 0\n",
      "Total job time 0 hours, 0 minutes, 32 seconds\n",
      "\n",
      "Finished Experiment\n"
     ]
    }
   ],
   "source": [
    "from maggy import experiment\n",
    "result = experiment.lagom(experiment_wrapper, \n",
    "                           searchspace=sp, \n",
    "                           optimizer='randomsearch', \n",
    "                           direction='min',\n",
    "                           num_trials=2, \n",
    "                           name='ganaml',\n",
    "                           hb_interval=5, \n",
    "                           es_interval=5,\n",
    "                           es_min=5\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from hops import hdfs\n",
    "EMBEDDINGS_HYPERPARAMS_FILE = 'gan_best_hp.json'\n",
    "hdfs.dump(json.dumps(result['best_hp']), \"Resources/\" + EMBEDDINGS_HYPERPARAMS_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing experiments\n",
    "Experiments service provides a unified view of all the experiments run using the `experiment` module.\n",
    "<br>\n",
    "As demonstrated in the gif it provides general information about the experiment and the resulting metric. Experiments can be visualized meanwhile or after training in a TensorBoard.\n",
    "<br>\n",
    "<br>\n",
    "![Image7-Monitor.png](./images/experiments.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}